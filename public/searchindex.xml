<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>Raft</title><url>/post/raft/</url><categories><category>Etcd</category></categories><tags/><content type="html"> etcd所使用的raft算法要点！
raft角色/状态 Leader: 领导者 Follower: 跟随者 Candidate: 候选人, 选举中的一个临时角色 选举 在一定时间内(ttl), follower没有收到来自leader的心跳(heartbear), 那么follower便会变为Candidate, 发送Request Vote到其他follower中, 进行leader选举和投票 其他follower收到Request Vote请求后, 便开始参与投票。投票完成之后，票数多的当选leader 选举出leader之后, leader会定期发送heartbeat消息给follower, 以此来告诉follower, leader还在线。 日志复制(Log Replication) leader收到client端请求, 每次操作请求都会产生日志条目(log entries)在节点(node)的日志中 leader产生log entries之后, 此时还不能提交, 还要将该条日志复制发到follower中 待follower收到新增日志, leader收到大多数follower的确认回复后, leader确认该条消息已经更新提交，最后leader再通知follower更新提交 注意: 如果client端是和follower进行交互, 那么follower会把消息重定向到leader上
日志一致性 在 Raft 算法中，领导人leader是通过强制跟随者follower直接复制自己的日志来处理不一致问题的。这意味着在跟随者follower中的冲突的日志条目会被领导人leader的日志覆盖
日志结构 日志由有序序号标记的条目组成。每个条目都包含创建时的任期号，和一个状态机需要执行的指令。一个条目当可以安全地被应用到状态机中去的时候，就认为是可以提交了。
日志匹配特性（Log Matching Property） 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。 过程 译文如下：
要使得跟随者的日志进入和自己一致的状态，领导人必须找到最后两者达成一致的地方，然后删除跟随者从那个点之后的所有日志条目，并发送自己在那个点之后的日志给跟随者。 所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导人针对每一个跟随者维护了一个 `nextIndex`，这表示下一个需要发送给跟随者的日志条目的索引地址。 当一个领导人刚获得权力的时候，他初始化所有的 `nextIndex` 值为自己的最后一条日志的 `index 加 1`。 如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的 `一致性检查` 就会失败。 在被跟随者拒绝之后，领导人就会减小 `nextIndex` 值并进行重试。最终 `nextIndex` 会在某个位置使得领导人和跟随者的日志达成一致。 当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。 简单来说就是leader维护一个 nextIndex ，然后通过发送附加日志RPCs给follower，follower检查 nextIndex 和自己日志的index是否一致，不一致便拒绝该消息，leader得知follower拒绝之后 就 nextIndex - 1, 继续该过程， 直到follwer接受，那么follwer就删除覆盖当前日志条目索引地址之后的日志，使得与leader的日志相同
压缩 Reference url: http://thesecretlivesofdata.com/raft/#home url: https://github.com/MrVWY/raft-zh_cn/blob/master/raft-zh_cn.md</content></entry><entry><title>go sync/atomic</title><url>/post/sync-atomic/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 定义 原子操作是一种可以在不加锁的情况下，对内存数据进行读写的操作, 使用sync/atomic提供的原子操作可以确保在任意时刻只有一个goroutine对变量进行操作，避免并发冲突
五种操作 增减: AddXXX 比较并交换: CompareAndSwapXXX 载入: LoadXXX 存储: StoreXXX 交换: SwapXXX 支持的数据类型: int32, int64, uint32, uint64, uintptr, unsafe.Pointer, 但是addXXX不支持unsafe.Pointer数据类型 支持Value类型: CompareAndSwap, Load, Store, Swap方法
使用 add // AddInt32 atomically adds delta to *addr and returns the new value. func AddInt32(addr *int32, delta int32) (new int32) // AddUint32 atomically adds delta to *addr and returns the new value. // To subtract a signed positive constant value c from x, do AddUint32(&amp;x, ^uint32(c-1)). // In particular, to decrement x, do AddUint32(&amp;x, ^uint32(0)). func AddUint32(addr *uint32, delta uint32) (new uint32) // AddInt64 atomically adds delta to *addr and returns the new value. func AddInt64(addr *int64, delta int64) (new int64) // AddUint64 atomically adds delta to *addr and returns the new value. // To subtract a signed positive constant value c from x, do AddUint64(&amp;x, ^uint64(c-1)). // In particular, to decrement x, do AddUint64(&amp;x, ^uint64(0)). func AddUint64(addr *uint64, delta uint64) (new uint64) // AddUintptr atomically adds delta to *addr and returns the new value. func AddUintptr(addr *uintptr, delta uintptr) (new uintptr) 功能: 把 addr 指针 指向的内存里的值 和 delta做加法，然后返回新值
CompareAndSwap // CompareAndSwapInt32 executes the compare-and-swap operation for an int32 value. func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) // CompareAndSwapInt64 executes the compare-and-swap operation for an int64 value. func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool) // CompareAndSwapUint32 executes the compare-and-swap operation for a uint32 value. func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool) // CompareAndSwapUint64 executes the compare-and-swap operation for a uint64 value. func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool) // CompareAndSwapUintptr executes the compare-and-swap operation for a uintptr value. func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool) // CompareAndSwapPointer executes the compare-and-swap operation for a unsafe.Pointer value. func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 功能: 比较 addr 指针指向的内存里的值是否为旧值old相等。如果相等，就把addr指针指向的内存里的值替换为新值new，并返回true。否则直接返回false
load // LoadInt32 atomically loads *addr. func LoadInt32(addr *int32) (val int32) // LoadInt64 atomically loads *addr. func LoadInt64(addr *int64) (val int64) // LoadUint32 atomically loads *addr. func LoadUint32(addr *uint32) (val uint32) // LoadUint64 atomically loads *addr. func LoadUint64(addr *uint64) (val uint64) // LoadUintptr atomically loads *addr. func LoadUintptr(addr *uintptr) (val uintptr) // LoadPointer atomically loads *addr. func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer) 功能: 返回 addr 指针指向的内存里的值
store // StoreInt32 atomically stores val into *addr. func StoreInt32(addr *int32, val int32) // StoreInt64 atomically stores val into *addr. func StoreInt64(addr *int64, val int64) // StoreUint32 atomically stores val into *addr. func StoreUint32(addr *uint32, val uint32) // StoreUint64 atomically stores val into *addr. func StoreUint64(addr *uint64, val uint64) // StoreUintptr atomically stores val into *addr. func StoreUintptr(addr *uintptr, val uintptr) // StorePointer atomically stores val into *addr. func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) 功能: 把 addr 指针指向的内存里的值修改为 val
swap // SwapInt32 atomically stores new into *addr and returns the previous *addr value. func SwapInt32(addr *int32, new int32) (old int32) // SwapInt64 atomically stores new into *addr and returns the previous *addr value. func SwapInt64(addr *int64, new int64) (old int64) // SwapUint32 atomically stores new into *addr and returns the previous *addr value. func SwapUint32(addr *uint32, new uint32) (old uint32) // SwapUint64 atomically stores new into *addr and returns the previous *addr value. func SwapUint64(addr *uint64, new uint64) (old uint64) // SwapUintptr atomically stores new into *addr and returns the previous *addr value. func SwapUintptr(addr *uintptr, new uintptr) (old uintptr) // SwapPointer atomically stores new into *addr and returns the previous *addr value. func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 功能: 把 addr 指针指向的内存里的值 替换为 新值new，然后返回 旧值old
例子 在日常中可能很少用到atomic，但还是得了解一下是如何在多个goroutine中操作使用的。经典的例子便是sync.Mutex
例子1 //多个goroutine对num2进行+1, 导致同一时间num2更新的值有误 var num2 int64 func main() { num2 = 0 ch := make(chan string) for i := 0; i &lt; 10000; i++ { go add2(ch, i) } for i := 0; i &lt; 10000; i++ { &lt;-ch } println(num2) } func add2(ch chan string, i int) { num2++ ch &lt;- ("add" + strconv.Itoa(i)) } //使用atomic对num进行操作, 使得同一时间num只有一个goroutine有操作权 var num int64 func main() { num = 0 ch := make(chan string) for i := 0; i &lt; 10000; i++ { go add(ch, i) } for i := 0; i &lt; 10000; i++ { &lt;-ch } println(num) } func add(ch chan string, i int) { atomic.AddInt64(&amp;num, 1) ch &lt;- ("add" + strconv.Itoa(i)) }</content></entry><entry><title>go runtime.SetFinalizer 用法</title><url>/post/runtime-setfinalizer/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> runtime.SetFinalizer可以在内存中对象被回收前(gc)，指定一些操作。
用法 document : https://pkg.go.dev/runtime#SetFinalizer //SetFinalizer sets the finalizer associated with obj to the provided finalizer function. When the garbage collector finds an unreachable block with an associated finalizer, it clears the //association and runs finalizer(obj) in a separate goroutine. This makes obj reachable again, but now without an associated finalizer. Assuming that SetFinalizer is not called again, the next //time the garbage collector sees that obj is unreachable, it will free obj. func SetFinalizer(obj any, finalizer any) 参数obj必须是指针类型 参数finalizer是一个函数，其参数可有可无，类型是obj的类型，并且没有返回值 当GC发现obj对象是unreachable(不可达)的时候，查看是否关联SetFinalizer函数
有则执行SetFinalizer函数，同时取消关联SetFinalizer函数。当下一次GC的时候，便可被回收 无则按正常GC流程进行回收 注意点 SetFinalizer函数只能在GC触发的时候运行，因此即使程序正常结束或者发生panic，obj没有被GC选中清除，那么SetFinalizer也不会运行 SetFinalizer函数按变量依赖顺序执行，如: A->B，只有A的SetFinalizer执行后，A被GC回收。B的SetFinalizer才能执行和被GC回收 例子 go-cache库 type Cache struct { *cache } type cache struct { defaultExpiration time.Duration items map[string]Item mu sync.RWMutex onEvicted func(string, interface{}) janitor *janitor } func New(defaultExpiration, cleanupInterval time.Duration) *Cache { items := make(map[string]Item) return newCacheWithJanitor(defaultExpiration, cleanupInterval, items) } func newCacheWithJanitor(de time.Duration, ci time.Duration, m map[string]Item) *Cache { c := newCache(de, m) C := &amp;Cache{c} if ci > 0 { runJanitor(c, ci) runtime.SetFinalizer(C, stopJanitor) } return C } func runJanitor(c *cache, ci time.Duration) { j := &amp;janitor{ Interval: ci, stop: make(chan bool), } c.janitor = j go j.Run(c) } func stopJanitor(c *Cache) { c.janitor.stop &lt;- true } func (j *janitor) Run(c *cache) { ticker := time.NewTicker(j.Interval) for { select { case &lt;-ticker.C: c.DeleteExpired() case &lt;-j.stop: ticker.Stop() return } } } os.newFile // newFile is like NewFile, but if called from OpenFile or Pipe // (as passed in the kind parameter) it tries to add the file to // the runtime poller. func newFile(fd uintptr, name string, kind newFileKind) *File { .......... runtime.SetFinalizer(f.file, (*file).close) //防止file没有close return f }</content></entry><entry><title>go http.transport</title><url>/post/go-http-transport/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> ​ 理解一下golang标准库里面的net/http包中的client类型，但是其实client类型也是通过实现RoundTripper接口来实现一个http客户端，因此如果说不希望默认的client类型帮你预处理一遍的话，那么你可以直接定义一个http.transport去直接获取返回来的resp，就不必经过默认的client类型处理了。
client客户端 c := http.Client{ Transport: nil, CheckRedirect: nil, Jar: nil, Timeout: 0, } c.Get() ​ 通过查看源码，可以发现client.get最终也是通过实现RoundTripper来实现http的发送。具体代码流程如下
// DefaultTransport is the default implementation of Transport and is // used by DefaultClient. It establishes network connections as needed // and caches them for reuse by subsequent calls. It uses HTTP proxies // as directed by the $HTTP_PROXY and $NO_PROXY (or $http_proxy and // $no_proxy) environment variables. var DefaultTransport RoundTripper = &amp;Transport{ Proxy: ProxyFromEnvironment, DialContext: defaultTransportDialContext(&amp;net.Dialer{ Timeout: 30 * time.Second, KeepAlive: 30 * time.Second, }), ForceAttemptHTTP2: true, MaxIdleConns: 100, IdleConnTimeout: 90 * time.Second, TLSHandshakeTimeout: 10 * time.Second, ExpectContinueTimeout: 1 * time.Second, } func (c *Client) transport() RoundTripper { if c.Transport != nil { return c.Transport } return DefaultTransport } func (c *Client) send(req *Request, deadline time.Time) (resp *Response, didTimeout func() bool, err error) { if c.Jar != nil { ... resp, didTimeout, err = send(req, c.transport(), deadline) ... return resp, nil, nil } // send issues an HTTP request. // Caller should close resp.Body when done reading from it. func send(ireq *Request, rt RoundTripper, deadline time.Time) (resp *Response, didTimeout func() bool, err error) { ... resp, err = rt.RoundTrip(req) ... } ​ 最后的话通过定义http.transport来实现RoundTripper接口（http/transport.go），最终client就能通过DefaultTransport去调用，因此我们可以直接定义一个http.transport去直接获取返回来的resp，就不必经过默认的client类型处理了。
client 类型 type Client struct { // Transport 指定执行独立、单次 HTTP 请求的机制。 // 如果 Transport 为 nil，则使用 DefaultTransport 。 Transport RoundTripper // CheckRedirect 指定处理重定向的策略。 // 如果 CheckRedirect 不为 nil，客户端会在执行重定向之前调用本函数字段。 // 参数 req 和 via 是将要执行的请求和已经执行的请求（切片，越新的请求越靠后）。 // 如果 CheckRedirect 返回一个错误，本类型的 Get 方法不会发送请求 req， // 而是返回之前得到的最后一个回复和该错误。（包装进 url.Error 类型里） // // 如果CheckRedirect为nil，会采用默认策略：连续10此请求后停止。 CheckRedirect func(req *Request, via []*Request) error // Jar 指定 cookie 管理器。 // 如果Jar为nil，请求中不会发送 cookie ，回复中的 cookie 会被忽略。 Jar CookieJar // Timeout 指定本类型的值执行请求的时间限制。 // 该超时限制包括连接时间、重定向和读取回复主体的时间。 // 计时器会在 Head 、 Get 、 Post 或 Do 方法返回后继续运作并在超时后中断回复主体的读取。 // // Timeout 为零值表示不设置超时。 // // Client 实例的 Transport 字段必须支持 CancelRequest 方法， // 否则 Client 会在试图用 Head 、 Get 、 Post 或 Do 方法执行请求时返回错误。 // 本类型的 Transport 字段默认值（ DefaultTransport ）支持 CancelRequest 方法。 Timeout time.Duration } RoundTripper interface type RoundTripper interface { // RoundTrip executes a single HTTP transaction, returning // a Response for the provided Request. // // RoundTrip should not attempt to interpret the response. In // particular, RoundTrip must return err == nil if it obtained // a response, regardless of the response's HTTP status code. // A non-nil err should be reserved for failure to obtain a // response. Similarly, RoundTrip should not attempt to // handle higher-level protocol details such as redirects, // authentication, or cookies. // // RoundTrip should not modify the request, except for // consuming and closing the Request's Body. RoundTrip may // read fields of the request in a separate goroutine. Callers // should not mutate or reuse the request until the Response's // Body has been closed. // // RoundTrip must always close the body, including on errors, // but depending on the implementation may do so in a separate // goroutine even after RoundTrip returns. This means that // callers wanting to reuse the body for subsequent requests // must arrange to wait for the Close call before doing so. // // The Request's URL and Header fields must be initialized. RoundTrip(*Request) (*Response, error) }</content></entry><entry><title>go slice</title><url>/post/go-slice/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 1、切片结构 一个切片由3部分组成：指针、长度和容量。指针指向底层数组，长度代表slice当前的长度，容量代表底层数组的长度，同时切片的长度可以在运行时修改，最小为 0 最大为相关数组的长度：切片是一个长度可变的数组。
type slice struct { array unsafe.Pointer len int cap int } s = [2]int{1, 2} //数组 s = []int{1, 2} //切片 2、nil和空切片 nil 切片：描述一个不存在的切片（无指针地址）
空切片：一个空的集合
区别：空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素
3、切片扩容 扩容代码
func growslice(et *_type, old slice, cap int) slice { if raceenabled { callerpc := getcallerpc() racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, funcPC(growslice)) } if msanenabled { msanread(old.array, uintptr(old.len*int(et.size))) } if cap &lt; old.cap { panic(errorString("growslice: cap out of range")) } if et.size == 0 { // append should not create a slice with nil pointer but non-zero len. // We assume that append doesn't need to preserve old.array in this case. return slice{unsafe.Pointer(&amp;zerobase), old.len, cap} } //扩容 newcap := old.cap doublecap := newcap + newcap if cap > doublecap { newcap = cap } else { if old.cap &lt; 1024 { newcap = doublecap } else { // Check 0 &lt; newcap to detect overflow // and prevent an infinite loop. for 0 &lt; newcap &amp;&amp; newcap &lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap &lt;= 0 { newcap = cap } } } // 计算新的切片的容量、长度 var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don't need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch { case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) > maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) > maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 { // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 } else { shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 } lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) > (maxAlloc >> shift) newcap = int(capmem >> shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) } if overflow || capmem > maxAlloc { panic(errorString("growslice: cap out of range")) } //确定 array unsafe.Pointer 地址 var p unsafe.Pointer if et.ptrdata == 0 { p = mallocgc(capmem, nil, false) // The append() that calls growslice is going to overwrite from old.len to cap (which will be the new length). // Only clear the part that will not be overwritten. memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) } else { // Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory. p = mallocgc(capmem, et, true) if lenmem > 0 &amp;&amp; writeBarrier.enabled { // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) } } //迁移数据 memmove(p, old.array, lenmem) return slice{p, old.len, newcap} } 切片扩容的策略： newcap := old.cap doublecap := newcap + newcap //2倍 if cap > doublecap { newcap = cap } else { if old.cap &lt; 1024 { newcap = doublecap } else { // Check 0 &lt; newcap to detect overflow // and prevent an infinite loop. for 0 &lt; newcap &amp;&amp; newcap &lt; cap { // 1/4 newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap &lt;= 0 { newcap = cap } } } 首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap） 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍，即（newcap=doublecap） 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的 1/4，直到最终容量（newcap）大于等于新申请的容量(cap)，即（newcap >= cap） 如果最终容量（cap）计算值溢出，则最终容量（cap）就是新申请容量（cap） 4、切片扩容的两种情况 情况一 func main() { array := [5]int{1, 2, 3, 4, 5} //数组 slice := array[0:2] //切片 len=2 cap=5 array = &amp;array newSlice := append(slice, 100) fmt.Printf("Before slice = %v, Pointer = %p, len = %d, cap = %d\n", slice, &amp;slice, len(slice), cap(slice)) fmt.Printf("Before newSlice = %v, Pointer = %p, len = %d, cap = %d\n", newSlice, &amp;newSlice, len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf("After slice = %v, Pointer = %p, len = %d, cap = %d\n", slice, &amp;slice, len(slice), cap(slice)) fmt.Printf("After newSlice = %v, Pointer = %p, len = %d, cap = %d\n", newSlice, &amp;newSlice, len(newSlice), cap(newSlice)) fmt.Printf("After array = %v\n", array) } //打印输出 Before slice = [1 2], Pointer = 0xc000004078, len = 2, cap = 5 Before newSlice = [1 2 100], Pointer = 0xc000004090, len = 3, cap = 5 After slice = [1 12], Pointer = 0xc000004078, len = 2, cap = 5 After newSlice = [1 12 100], Pointer = 0xc000004090, len = 3, cap = 5 After array = [1 12 100 4 5] 结合切片的结构可以看出，由于原数组还有容量cap可以使用，所以执行 append() 操作以后，会在原数组上直接操作，所以这种情况下，扩容以后的数组还是指向原来的数组。
因此多个slice指向相同的底层数组时，修改其中一个slice，可能会影响其他slice的值。
情况二 var p unsafe.Pointer if et.ptrdata == 0 { p = mallocgc(capmem, nil, false) // The append() that calls growslice is going to overwrite from old.len to cap (which will be the new length). // Only clear the part that will not be overwritten. memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) } else { // Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory. //重新申请capmem大小的内存地址，并初始化为zero p = mallocgc(capmem, et, true) if lenmem > 0 &amp;&amp; writeBarrier.enabled { // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. //执行写屏障，将p给打上颜色 bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) } } //迁移数据 memmove(p, old.array, lenmem) 数组的容量已经达到了最大值，Go 默认会先开一片新的内存区域，把原来的值拷贝过来，然后再执行 append() 操作。这时候新旧切片所指向的数组的内存地址不一样，因此在新切片上操作不会对旧切片造成影响。
5、切片copy // slicecopy is used to copy from a string or slice of pointerless elements into a slice. func slicecopy(toPtr unsafe.Pointer, toLen int, fromPtr unsafe.Pointer, fromLen int, width uintptr) int { if fromLen == 0 || toLen == 0 { return 0 } n := fromLen if toLen &lt; n { n = toLen } if width == 0 { return n } size := uintptr(n) * width if raceenabled { callerpc := getcallerpc() pc := funcPC(slicecopy) racereadrangepc(fromPtr, size, callerpc, pc) racewriterangepc(toPtr, size, callerpc, pc) } if msanenabled { msanread(fromPtr, size) msanwrite(toPtr, size) } if size == 1 { // common case worth about 2x to do here // TODO: is this still worth it with new memmove impl? *(*byte)(toPtr) = *(*byte)(fromPtr) // known to be a byte pointer } else { memmove(toPtr, fromPtr, size) } return n } 例
func main() { array := []int{10, 20, 30, 40} slice := make([]int, 6) n := copy(slice, array) fmt.Println(n, slice) } //output 4 [10 20 30 40 0 0] 注意
func main() { slice := []int{10, 20, 30, 40} for index, value := range slice { fmt.Printf("value = %d , value-addr = %x , slice-addr = %x\n", value, &amp;value, &amp;slice[index]) } } //output value = 10 , value-addr = c000018098 , slice-addr = c00000e200 value = 20 , value-addr = c000018098 , slice-addr = c00000e208 value = 30 , value-addr = c000018098 , slice-addr = c00000e210 value = 40 , value-addr = c000018098 , slice-addr = c00000e218 说明value的内存地址上的值的从slice复制过来的，直接操作value是不会对slice本身造成什么影响的。</content></entry><entry><title>linux tmpfs</title><url>/post/linux-tmpfs/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"> ​ tmpfs,临时文件系统，是一种基于内存的文件系统，它和虚拟磁盘ramdisk比较类似像，但不完全相同，和ramdisk一样，tmpfs可以使用RAM，但它也可以使用swap分区来存储，而且传统的ramdisk是个块设备 ，要用mkfs来格式化它，才能真正地使用它；而tmpfs是一个文件系统，并不是块设备，只是安装它，就可以使用了。tmpfs是最好的基于RAM的文件系统。
特点 动态文件系统大小
/mnt/tmpfs最初会只有很小的空间，但随着文件的复制和创建，tmpfs文件系统驱动程序会分配更多的 VM，并按照需求动态地增加文件系统的空间。而且，当 /mnt/tmpfs 中的文件被删除时，tmpfs 文件系统驱动程序会动态地减小文件系统并释放 VM 资源，这样做可以将 VM 返回到循环当中以供系统中其它部分按需要使用。因为 VM 是宝贵的资源，所以您一定不希望任何东西浪费超出它实际所需的 VM，tmpfs 的好处之一就在于这些都是自动处理的。
速度快（快速读写能力）
tmpfs 的另一个主要的好处是它闪电般的速度。因为典型的 tmpfs 文件系统会完全驻留在 RAM 中，读写几乎可以是瞬间的。即使用了一些交换分区 ，性能仍然是卓越的，当更多空闲的 VM 资源可以使用时，这部分 tmpfs 文件系统会被移动到 RAM 中去。让 VM 子系统自动地移动部分 tmpfs 文件系统到交换分区实际上对性能上是好的，因为这样做可以让 VM 子系统为需要 RAM 的进程释放空间。这一点连同它动态调整大小的能力，比选择使用传统的 RAM 磁盘可以让操作系统有好得多的整体性能和灵活性。
临时性
由于tmpfs是构建在内存中的，所以存放在tmpfs中的所有数据在卸载或断电后都会丢失
一些使用场景 ​ 例如在公司中（目前我们公司用的是mongodb）对于数据读写操作速度要求比较高的情况下，我会跑2个mongodb进程，一个数据挂载在硬盘上（这里称为mongdb-A），另一个挂载在内存中（/run、/dev/shm&hellip;&hellip;）（这里称为mongdb-B）。在操作时，我们将当天没有处理完的数据存到mongdb-B中，等到数据完全处理完之后，把已经处理好的数据迁移到mongdb-A中，同时删除mongdb-B这条已经处理好的数据。</content></entry><entry><title>go 如何实现线程安全（并发安全）的 map</title><url>/post/go-%E5%B9%B6%E5%8F%91map/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 在网上查了查如何实现线程安全（并发安全）的 map，发现了几种方法。
sync.RWMutex 加读写锁
//例子 type RWMap struct { // 一个读写锁保护的线程安全的map sync.RWMutex // 读写锁保护下面的map字段 m map[int]int } // 新建一个RWMap func NewRWMap(n int) *RWMap { return &amp;RWMap{ m: make(map[int]int, n), } } func (m *RWMap) Get(k int) (int, bool) { //从map中读取一个值 m.RLock() defer m.RUnlock() v, existed := m.m[k] // 在锁的保护下从map中读取 return v, existed } func (m *RWMap) Set(k int, v int) { // 设置一个键值对 m.Lock() // 锁保护 defer m.Unlock() m.m[k] = v } func (m *RWMap) Delete(k int) { //删除一个键 m.Lock() // 锁保护 defer m.Unlock() delete(m.m, k) } func (m *RWMap) Len() int { // map的长度 m.RLock() // 锁保护 defer m.RUnlock() return len(m.m) } func (m *RWMap) Each(f func(k, v int) bool) { // 遍历map m.RLock() //遍历期间一直持有读锁 defer m.RUnlock() for k, v := range m.m { if !f(k, v) { return } } } ​ 功能上满足，但是在高并发的场景下，性能跟不上，一旦加上锁，就必须等到锁释放其他协程才能使用
分片加锁 ​ 文档里说concurrent-map提供了一种高性能的解决方案:通过对内部map进行分片，降低锁粒度，从而达到最少的锁等待时间(锁冲突)。意思就是将 map 分成 n 块，每个块之间的读写操作都互不干扰，从而降低冲突的可能性。
​ 直接看源码：https://github.com/orcaman/concurrent-map/blob/master/concurrent_map.go
​ Github：https://github.com/orcaman/concurrent-map
​ 其实就是定义ConcurrentMapShared结构体（这便是map里面的某个块），里面含有一个读写锁。
​ 当对map进行相关操作时，就去计算hash值，算出落在哪一个块中，再去进行相关操作。
sync.map sync.Map在读多写少性能（读取 read 并不需要加锁，而读或写 dirty 都需要加锁）比较好，否则并发性能很差。关于sync.map的理解，在网上找了几张理解图，如下
第一张是sync.map的结构图
第二张是关于sync.map里面的dirty和read的转换图
关于从dirty迁移到read，当misses达到dirty长度时，才触发迁移，迁移完成之后misses置为0，dirty置为nil，read就拥有了dirty原来的数据。
总结：
通过 read 和 dirty 两个字段将读写分离，读的数据存在只读字段 read 上，将最新写入的数据则存在 dirty 字段上 读取时会先查询 read，不存在再查询 dirty，写入时则只写入 dirty 读取 read 并不需要加锁，而读或写 dirty 都需要加锁 另外有 misses 字段来统计 read 被穿透的次数（被穿透指需要读 dirty 的情况），超过一定次数则将 dirty 数据同步到 read 上 对于删除数据则直接通过标记来延迟删除 源码：https://github.com/golang/go/blob/master/src/sync/map.go</content></entry><entry><title>go map 存储方法</title><url>/post/go-map-save-and-use-func/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 背景 最近在公司重构个系统的时候，由于某一个流程需要按需来自定义功能来进行处理，于是我想着用一个map来做一个映射关系，然后在这个流程中利用一个标记来匹配map中的自定义功能函数，以此达到按需处理。但是一番敲击之后，发现效果不尽人意，最主要的还是如果要用map做映射关系，那么就必须要在开头import一堆自定义函数包。
例如下面这个例子
// server/ package main import ( "fmt" "server/a" "server/b" ....... ) func main() { functions := buildFunctions() f := functions["isInValid"] // f("hello") } func buildFunctions() map[string]func() bool { functions := map[string]func() bool{ "isInValid": a.IsInValid, "isAvailable": b.IsAvailable, ...... } return functions } // server/a/ func IsInValid(s string) bool { fmt.Println("Invalid ", s) return true } // server/B/ func IsAvailable(s string, s1 string) bool { return true } 思考 由于旧系统是用nodejs写的，其可以动态的将函数require存进map中，我们只需要定义好路径和函数名字，就可以以拼接的方式将函数require存进map中，如下所示
entry[mod] = load_module mod for mod in [ "a" "b" ...... //函数名 ] module.exports = load_module = (module_name, proj = argv.project) -> try obj_common = require "./#{module_name}" catch err debug "未找到 #{module_name}, #{err.stack}".bold.red try obj_proj = require "./proj/#{proj}/#{module_name}" # debug "./proj/#{proj}/#{module_name}" catch err # debug "#{err}" return _.assign obj_common, obj_proj if "function" != typeof obj_common obj_proj or obj_common //other js file module.exports = a = (str) -> console.log("a", str) //other js file module.exports = b = (str) -> console.log("b", str) //最终就可以生成一个map对象 entry[a] = a entry[b] = b 基于旧系统的这种处理方式，在go中利用map去一个一个在map中做映射就会导致开头会出现一大堆import包的语句。
解决 在与公司的大佬的建议下，他建议使用typescript编写这一功能，然后转成lua代码。最后嵌入进golang中。于是本人变浅尝一下。
//main.ts let A = new Map() function L(s:string) { let a = require("./ProjectA/" + s) A.set(s,a.My) } function Say(s:string) { L(s) let a = A.get(s) console.log("s", s) console.log("a", a) a() } //ProjectA/A export function My() { console.log("This is a function A ! ") } //tsconfig.json { "compilerOptions": { "target": "esnext", "lib": ["esnext","dom"], "moduleResolution": "node", "types": ["node"], "strict": true }, "tstl": { "luaTarget": "5.3", "noImplicitSelf":true, // 是否打包为单个lua文件 "luaBundle":"api.lua", "luaBundleEntry":"main.ts" } } cmd : npm run build 通过上述运行npm run build命令之后会得到一个api.lua文件，接着我们把它嵌入进go中，使其通过传一个参数A去加载ProjectA/A中的My函数。
package main import ( "fmt" lua "github.com/yuin/gopher-lua" ) func main() { L := lua.NewState() defer L.Close() if err := L.DoFile("api.lua"); err != nil { panic(err) } if err := L.CallByParam(lua.P{ Fn: L.GetGlobal("Say"), NRet: 1, // 指定返回值数量 Protect: true, // 如果出现异常，是panic还是返回err }, lua.LString("A")); err != nil { fmt.Println(err.Error()) } } //out s A a function: 0xc000219b80 This is a function A ! 总结 通过这种方式，我们可以很灵活的在golang去动态的加载某个自定义的方法和功能
Reference https://stackoverflow.com/questions/52298469/map-to-store-generic-type-functions-in-go https://github.com/yuin/gopher-lua https://typescripttolua.github.io/ https://gitee.com/zmwcodediy/lua-ts-demo</content></entry><entry><title>CommonJS, AMD and RequireJS</title><url>/post/commonjs-amd-and-requirejs/</url><categories><category>Js</category></categories><tags><tag>Js</tag></tags><content type="html"> 最近在使用typescript写一点小脚本，再用tsc编译成js文件后，使用node运行发现报错，再转去看一眼编译出来的js文件，发现不对劲。编译出来的js文件如下
define(["require", "exports", "./ProjectA/A", "./ProjectB/B"], function (require, exports, A_1, B_1) { ........ }); 这时问题就来了，经过一番寻找发现编译出来的这种js文件和AMD有关。
AMD Asynchronous Module Definition，其思想上其实也就只有一种目的，那就是浏览器的按需加载，也就是异步加载模块，从而不影响后面的代码运行。RequireJS 就是其中实现AMD思想的一个js。
CommonJs commonjs最好的体现那就是nodejs的模块功能。其给我的感觉与AMD相比，最大的一点那就是commonjs的模块加载是同步处理的，这样就是阻塞后面的代码运行。
// someModule.js exports.doSomething = function() { return "foo"; }; //otherModule.js var someModule = require('someModule'); // in the vein of node exports.doSomethingElse = function() { return someModule.doSomething() + "bar"; };</content></entry><entry><title>正则表达式 消耗字符</title><url>/post/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E6%B6%88%E8%80%97%E5%AD%97%E7%AC%A6/</url><categories><category>LeetCode</category></categories><tags><tag>LeetCode</tag></tags><content type="html"> 正则表达式匹配过程 主要的引擎有2种:
DFA(Deterministic Finite Automatons/确定性有限自动机—)，文本主导的正则引擎 NFA(Nondeterministic Finite Automatons/非确定性有限自动机)，表达式主导的正则引擎，核心特点——回溯 消耗字符 在正则表达式匹配的过程中，存在“消耗字符”的概念。即一个字符在匹配过程中被检索（消耗）过，后面的匹配表达式就不会再检索这一个/串字符了。
(?=exp) 捕获，但不消耗字符，且匹配exp，返回位置 (?!exp) 捕获，但不消耗字符，且不匹配exp，返回位置 (?&lt;=exp) 反向 (?&lt;!exp) 反向 例：
1、找出字符串中出现过1次以上的单词 第一条正则表达式\b(\w+)\b.*\b\1\b，在第一个(\w+)匹配到what，并且其后的\1也匹配到第二个what的时候，“Oh what a day, what”这一段子串都已经被正则表达式消耗了，所以之后的匹配，将直接从第二个what之后开始。也就是消耗了字符
\1：正则表达式中的小括号"()"。是代表分组的意思。 如果再其后面出现\1则是代表与第一个小括号中要匹配的内容相同。 注意：\1必须与小括号配合使用
2、(?:pattern)与(?=pattern) (?:pattern) 消耗字符，下一字符匹配会从已匹配后的位置开始。 (?=pattern) 不消耗字符，下一字符匹配会从预查之前的位置开始。</content></entry><entry><title>linux F-RTO</title><url>/post/linux-f-rto/</url><categories><category>TCP/IP</category></categories><tags><tag>TCP/IP</tag></tags><content type="html"> F-RTO 全称 Forward RTO-Recovery，一种检测TCP的虚假超时重传的算法。目的是判断RTO是否正常，从而决定是否执行拥塞避免算法
rfc5682: It has been pointed out that the retransmission timer can expire spuriously and cause unnecessary retransmissions when no segments have been lost [LK00, GL02, LM03]. After a spurious retransmission timeout, the late acknowledgments of the original segments arrive at the sender, usually triggering unnecessary retransmissions of a whole window of segments during the RTO recovery. Furthermore, after a spurious retransmission timeout, a conventional TCP sender increases the congestion window on each late acknowledgment in slow start. This injects a large number of data segments into the network within one round-trip time, thus violating the packet conservation principle [Jac88]. 造成虚假超时的原因，在rfc5682上指出了3点原因
突发的网络拥塞造成RTT增加，触发RTO机制
First, some mobile networking technologies involve sudden delay spikes on transmission because of actions taken during a hand- off. 从低延迟链路进入到高延迟链路，比如路由重收敛（网络的拓扑结构发生变化后，路由表重新建立到发送再到学习直至稳定，并通告网络中所有相关路由器都得知该变化的过程）
Second, a hand-off may take place from a low latency path to a high latency path, suddenly increasing the round-trip time beyond the current RTO value. 在低延迟链路来了一股竞争流量
Third, on a low-bandwidth link the arrival of competing traffic (possibly with higher priority), or some other change in available bandwidth, can cause a sudden increase of the round-trip time. 造成虚假RTO的后果
​ 虚假的RTO会触发RTO机制，利用拥塞避免、快速重传或者快速恢复算法去重传自以为丢失的数据包，但是最终都会向网络中注入重复而无效的数据包，造成网络拥塞。
当发生重传时，先重传还未被ACK的第一个包，如果重传后收到的第一个ACK不是重复ACK，则尝试发送一个之前从未发送过的数据包，等待发出后的第一个ACK，如果这个ACK也不是重复ACK，说明这个重传是虚假重传。
上述第一个不重复的ACK有两个可能，一种是重传出去的包被ACK了，这种情况下不是虚假重传，另一种是原始数据包被ACK了，这种情况是虚假重传，因此收到这个ACK后再发一个未发送过的数据包，如果又收到一个不重复的ACK，说明当前网络情况正常，刚才的重传是虚假重传。
Reference https://datatracker.ietf.org/doc/html/rfc5682</content></entry><entry><title>Go 基础知识点</title><url>/post/go-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 记录一下golang的基础要点，以方便后续复习
GC 标记清除 ​ 标记清除（Mark-Sweep）算法是最常见的垃圾收集算法，标记清除收集器是跟踪式垃圾收集器，其执行过程可以分成标记（Mark）和清除（Sweep）两个阶段：
标记阶段 — 从根对象出发查找并标记堆中所有存活的对象； 清除阶段 — 遍历堆中的全部对象，回收未被标记的垃圾对象并将回收的内存加入空闲链表； 标记阶段结束后会进入清除阶段，在该阶段中收集器会依次遍历堆中的所有对象，释放其中没有被标记的 B、E 和 F 三个对象并将新的空闲内存空间以链表的结构串联起来，方便内存分配器的使用。
​ 标记清除算法：垃圾收集器从垃圾收集的根对象出发，递归遍历这些对象指向的子对象并将所有可达的对象标记成存活；标记阶段结束后，垃圾收集器会依次遍历堆中的对象并清除其中的垃圾，整个过程需要标记对象的存活状态，用户程序在垃圾收集的过程中也不能执行（STW），我们需要用到更复杂的机制来解决这个 STW （Stop The World）的问题。
三色标记算法 ​ 为了解决原始标记清除算法带来的长时间 STW，多数现代的追踪式垃圾收集器都会实现三色标记算法的变种以缩短 STW 的时间。三色标记算法将程序中的对象分成白色、黑色和灰色三类4 ：
白色对象 — 潜在的垃圾，其内存可能会被垃圾收集器回收； 黑色对象 — 活跃的对象，包括不存在任何引用外部指针的对象以及从根对象可达的对象； 灰色对象 — 活跃的对象，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象； ​ 在垃圾收集器开始工作时，程序中不存在任何的黑色对象，垃圾收集的根对象会被标记成灰色，垃圾收集器只会从灰色对象集合中取出对象开始扫描，当灰色集合中不存在任何对象时，标记阶段就会结束。
​ 三色标记垃圾收集器的工作原理很简单，我们可以将其归纳成以下几个步骤：
从灰色对象的集合中选择一个灰色对象并将其标记成黑色； 将黑色对象指向的所有对象都标记成灰色，保证该对象和被该对象引用的对象都不会被回收； 重复上述两个步骤直到对象图中不存在灰色对象； ​ 当三色的标记清除的标记阶段结束之后，应用程序的堆中就不存在任何的灰色对象，只能有黑色的存活对象以及白色的垃圾对象，垃圾收集器可以回收这些白色的垃圾。
屏障机制 ​ 因为用户程序可能在标记执行的过程中修改对象的指针，所以三色标记清除算法本身是不可以并发或者增量执行的，它仍然需要 STW。
​ 本来不应该被回收的对象却被回收了，这在内存管理中是非常严重的错误，我们将这种错误称为悬挂指针，即指针没有指向特定类型的合法对象，影响了内存的安全性5 ，想要并发或者增量地标记对象还是需要使用屏障技术。
想要在并发或者增量的标记算法中保证正确性，我们需要达成以下两种三色不变性（Tri-color invariant）中的一种：
强三色不变性 — 黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象； 弱三色不变性 — 黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径 ​ 上图分别展示了遵循强三色不变性和弱三色不变性的堆内存，遵循上述两个不变性中的任意一个，我们都能保证垃圾收集算法的正确性，而屏障技术就是在并发或者增量标记过程中保证三色不变性的重要技术。
​ 垃圾收集中的屏障技术更像是一个钩子方法，它是在用户程序读取对象、创建新对象以及更新对象指针时执行的一段代码，根据操作类型的不同，我们可以将它们分成读屏障（Read barrier）和写屏障（Write barrier）两种，因为读屏障需要在读操作中加入代码片段，对用户程序的性能影响很大，所以编程语言往往都会采用写屏障保证三色不变性。
​ 造成引用对象丢失的条件：
一个黑色的节点A新增了指向白色节点C的引用 并且白色节点C没有除了A之外的其他灰色节点的引用，或者存在但是在GC过程中被删除了 ​ 因此要满足弱三色不变性和强三色不变性，即可保证对象不丢失
Dijistra 插入写屏障：满足强三色不变性：黑色节点不允许引用白色节点，当黑色节点新增了白色节点的引用时，将对应的白色节点改为灰色。
标记过程：
垃圾收集器将根对象指向 A 对象标记成黑色并将 A 对象指向的对象 B 标记成灰色； 用户程序修改 A 对象的指针，将原本指向 B 对象的指针指向 C 对象，这时触发写屏障将 C 对象标记成灰色； 垃圾收集器依次遍历程序中的其他灰色对象，将它们分别标记成黑色； 缺点：
​ Dijkstra 的插入写屏障是一种相对保守的屏障技术，它会将有存活可能的对象都标记成灰色以满足强三色不变性。在如上所示的垃圾收集过程中，实际上不再存活的 B 对象最后没有被回收；而如果我们在第二和第三步之间将指向 C 对象的指针改回指向 B，垃圾收集器仍然认为 C 对象是存活的，这些被错误标记的垃圾对象只有在下一个循环才会被回收。
​ 插入式的 Dijkstra 写屏障虽然实现非常简单并且也能保证强三色不变性，但是它也有明显的缺点。因为栈上的对象在垃圾收集中也会被认为是根对象，所以为了保证内存的安全，Dijkstra 必须为栈上的对象增加写屏障或者在标记阶段完成重新对栈上的对象进行扫描，这两种方法各有各的缺点，前者会大幅度增加写入指针的额外开销，后者重新扫描栈对象时需要暂停程序
Yuasa 删除写屏障：满足弱三色不变性：黑色节点允许引用白色节点，但是该白色节点有其他灰色节点间接的引用（确保不会被遗漏）当白色节点被删除了一个引用时，悲观地认为它一定会被一个黑色节点新增引用，所以将它置为灰色
混合写屏障 ​ 过程：
GC开始时将栈上全部对象标记为黑色（之后不再进行第二次重复扫描，无需STW）
GC期间，任何在栈上创建的对象，均为黑色。
被删的对象标记为灰色。
被添加的对象标记为灰色。
GC 的触发条件 主动触发(手动触发)，通过调用 runtime.GC 来触发 GC，此调用阻塞式地等待当前 GC 运行完毕。
被动触发，分为两种方式：
使用系统监控，当超过两分钟没有产生任何 GC 时，强制触发 GC。
使用步调（Pacing）算法，其核心思想是控制内存增长的比例,每次内存分配时检查当前内存分配量是否已达到阈值（环境变量 GOGC）：默认 100%，即当内存扩大一倍时启用 GC。
GPM 调度 和 CSP 模型 CSP 模型 CSP 模型是“以通信的方式来共享内存”，不同于传统的多线程通过共享内存来通信。用于描述两个独立的并发实体通过共享的通讯 channel (管道)进行通信的并发模型。
GPM G（Goroutine） ​ 即 Go 协程（Goroutine），每个 go 关键字都会创建一个协程。Goroutine 在 Go 语言运行时使用私有结构体 runtime.g 表示，结构体 runtime.g 的 atomicstatus 字段存储了当前 Goroutine 的状态。除了几个已经不被使用的以及与 GC 相关的状态之外，Goroutine 可能处于以下 9 种状态：
状态 描述 _Gidle 刚刚被分配并且还没有被初始化 _Grunnable 没有执行代码，没有栈的所有权，存储在运行队列中 _Grunning 可以执行代码，拥有栈的所有权，被赋予了内核线程 M 和处理器 P _Gsyscall 正在执行系统调用，拥有栈的所有权，没有执行用户代码，被赋予了内核线程 M 但是不在运行队列上 _Gwaiting 由于运行时而被阻塞，没有执行用户代码并且不在运行队列上，但是可能存在于 Channel 的等待队列上 _Gdead 没有被使用，没有执行代码，可能有分配的栈 _Gcopystack 栈正在被拷贝，没有执行代码，不在运行队列上 _Gpreempted 由于抢占而被阻塞，没有执行用户代码并且不在运行队列上，等待唤醒 _Gscan GC 正在扫描栈空间，没有执行代码，可以与其他状态同时存在 上述状态中比较常见是 _Grunnable、_Grunning、_Gsyscall、_Gwaiting 和 _Gpreempted 五个状态
​ 虽然 Goroutine 在运行时中定义的状态非常多而且复杂，但是可以将这些不同的状态聚合成三种：等待中、可运行、运行中，运行期间会在这三种状态来回切换：
等待中：Goroutine 正在等待某些条件满足，例如：系统调用结束等，包括 _Gwaiting、_Gsyscall 和 _Gpreempted 几个状态； 可运行：Goroutine 已经准备就绪，可以在线程运行，如果当前程序中有非常多的 Goroutine，每个 Goroutine 就可能会等待更多的时间，即 _Grunnable； 运行中：Goroutine 正在某个线程上运行，即 _Grunning； M（Machine） ​ M 是操作系统线程。调度器最多可以创建 10000 个线程，但是其中大多数的线程都不会执行用户代码（可能陷入系统调用），最多只会有 GOMAXPROCS 个活跃线程能够正常运行。
​ 在默认情况下，运行时会将 GOMAXPROCS 设置成当前机器的核数，我们也可以在程序中使用 runtime.GOMAXPROCS 来改变最大的活跃线程数。一个四核机器会创建四个活跃的操作系统线程，每一个线程都对应一个运行时中的 runtime.m 结构体。
​ 结构体 runtime.m 表示操作系统线程，这个结构体也包含了几十个字段，这里先来了解几个与 Goroutine 相关的字段：
type m struct { g0 *g curg *g ... } ​ 其中 g0 是持有调度栈的 Goroutine，curg 是在当前线程上运行的用户 Goroutine，这也是操作系统线程唯一关心的两个 Goroutine。g0 是一个运行时中比较特殊的 Goroutine，它会深度参与运行时的调度过程，包括 Goroutine 的创建、大内存分配和 CGO 函数的执行。
P（Processor） ​ 调度器中的处理器 P 是线程和 Goroutine 的中间层，它能提供线程需要的上下文环境，也会负责调度线程上的等待队列，通过处理器 P 的调度，每一个内核线程都能够执行多个 Goroutine，它能在 Goroutine 进行一些 I/O 操作时及时让出计算资源，提高线程的利用率。
​ 因为调度器在启动时就会创建 GOMAXPROCS 个处理器，所以 Go 语言程序的处理器数量一定会等于 GOMAXPROCS，这些处理器会绑定到不同的内核线程上。
runtime.p 是处理器的运行时表示，作为调度器的内部实现。runtime.p 结构体中的状态 status 字段会是以下五种中的一种：
状态 描述 _Pidle 处理器没有运行用户代码或者调度器，被空闲队列或者改变其状态的结构持有，运行队列为空 _Prunning 被线程 M 持有，并且正在执行用户代码或者调度器 _Psyscall 没有执行用户代码，当前线程陷入系统调用 _Pgcstop 被线程 M 持有，当前处理器由于垃圾回收被停止 _Pdead 当前处理器已经不被使用 M 必须拥有 P 才可以执行 G 中的代码，P 含有一个包含多个 G 的队列，P 可以调度 G 交由 M 执行。
Goroutine 调度策略 队列轮转：P 会周期性的将 G 调度到 M 中执行，执行一段时间后，保存上下文，将 G 放到队列尾部，然后从队列中再取出一个 G 进行调度。除此之外，P 还会周期性的查看全局队列是否有 G 等待调度到 M 中执行。 系统调用：当 G0 即将进入系统调用时，M0 将释放 P，进而某个空闲的 M1 获取 P，继续执行 P 队列中剩下的 G。M1 的来源有可能是 M 的缓存池，也可能是新建的。 当 G0 系统调用结束后，如果有空闲的 P，则获取一个 P，继续执行 G0。如果没有，则将 G0 放入全局队列，等待被其他的 P 调度。然后 M0 将进入缓存池睡眠。 Channel ​ 虽然在 Go 语言中也能使用共享内存加互斥锁进行通信，但是 Go 语言提供了一种不同的并发模型，即通信顺序进程（Communicating sequential processes，CSP）1 。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Goroutine 之间会通过 Channel 传递数据
​ Channel 在运行时使用 runtime.hchan 结构体表示。我们在 Go 语言中创建新的 Channel 时，实际上创建的都是如下所示的结构：
type hchan struct { qcount uint dataqsiz uint buf unsafe.Pointer elemsize uint16 closed uint32 elemtype *_type sendx uint recvx uint recvq waitq sendq waitq lock mutex } runtime.hchan 结构体中的五个字段 qcount、dataqsiz、buf、sendx、recv 构建底层的循环队列：
qcount — Channel 中的元素个数； dataqsiz — Channel 中的循环队列的长度； buf — Channel 的缓冲区数据指针； sendx — Channel 的发送操作处理到的位置； recvx — Channel 的接收操作处理到的位置； 除此之外，elemsize 和 elemtype 分别表示当前 Channel 能够收发的元素类型和大小；sendq 和 recvq 存储了当前 Channel 由于缓冲区空间不足而阻塞的 Goroutine 列表，这些等待队列使用双向链表 runtime.waitq 表示，链表中所有的元素都是 runtime.sudog 结构：
type waitq struct { first *sudog last *sudog } runtime.sudog 表示一个在等待列表中的 Goroutine，该结构中存储了两个分别指向前后 runtime.sudog 的指针以构成链表。
发送数据 在发送数据的逻辑（ runtime.chansend 函数实现）执行之前会先为当前 Channel 加锁，防止多个线程并发修改数据。
过程 当存在等待的接收者时，通过 runtime.send 直接将数据发送给阻塞的接收者； 当缓冲区存在空余空间时，将发送的数据写入 Channel 的缓冲区； 当不存在缓冲区或者缓冲区已满时，等待其他 Goroutine 从 Channel 接收数据； 直接发送 如果目标 Channel 没有被关闭并且已经有处于读等待的 Goroutine，那么 runtime.chansend 会从接收队列 recvq 中取出最先陷入等待的 Goroutine 并直接向它发送数据：
if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true } 发送数据时会调用 runtime.send ，该函数的执行可以分成两个部分：
调用 runtime.sendDirect 将发送的数据直接拷贝到 x = &lt;-c 表达式中变量 x 所在的内存地址上； 调用 runtime.goready 将等待接收数据的 Goroutine 标记成可运行状态 Grunnable 并把该 Goroutine 放到发送方所在的处理器的 runnext 上等待执行，该处理器在下一次调度时会立刻唤醒数据的接收方； 缓冲区buf func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if c.qcount &lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(&amp;c.lock) return true } ... } ​ 如果当前 Channel 的缓冲区未满，向 Channel 发送的数据会存储在 Channel 的 sendx 索引所在的位置并将 sendx 索引加一。因为这里的 buf 是一个循环数组，所以当 sendx 等于 dataqsiz 时会重新回到数组开始的位置。
阻塞发送 当 Channel 没有接收者能够处理数据时，向 Channel 发送数据会被下游阻塞，当然使用 select 关键字可以向 Channel 非阻塞地发送消息。向 Channel 阻塞地发送数据会执行下面的代码，我们可以简单梳理一下这段代码的逻辑：
func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if !block { unlock(&amp;c.lock) return false } // Block on the channel. Some receiver will complete our operation for us. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) //goparkunlock(&amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) // Ensure the value being sent is kept alive until the // receiver copies it out. The sudog has a pointer to the // stack object, but sudogs aren't considered as roots of the // stack tracer. KeepAlive(ep) // someone woke us up. if mysg != gp.waiting { throw("G waiting list is corrupted") } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime > 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) if closed { if c.closed == 0 { throw("chansend: spurious wakeup") } panic(plainError("send on closed channel")) } return true } 调用 runtime.getg 获取发送数据使用的 Goroutine； 执行 runtime.acquireSudog 获取 runtime.sudog 结构并设置这一次阻塞发送的相关信息，例如发送的 Channel、是否在 select 中和待发送数据的内存地址等； 将刚刚创建并初始化的 runtime.sudog 加入发送等待队列，并设置到当前 Goroutine 的 waiting 上，表示 Goroutine 正在等待该 sudog 准备就绪； 调用 runtime.gopark将当前的 Goroutine 陷入沉睡等待唤醒； 被调度器唤醒后会执行一些收尾工作，将一些属性置零并且释放 runtime.sudog 结构体； 接受数据 Go 语言中可以使用两种不同的方式去接收 Channel 中的数据：
i &lt;- ch i, ok &lt;- ch 这两种不同的方法经过编译器的处理会被转换成 runtime.chanrecv1 和 runtime.chanrecv2 两种不同函数的调用，但是这两个函数最终还是会调用 runtime.chanrecv 。
// entry points for &lt;- c from compiled code //go:nosplit func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } //go:nosplit func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } 当我们从一个空 Channel 接收数据时会直接调用 runtime.gopark 让出处理器的使用权。
func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw("unreachable") } lock(&amp;c.lock) if c.closed != 0 &amp;&amp; c.qcount == 0 { unlock(&amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } 如果当前 Channel 已经被关闭并且缓冲区中不存在任何数据，那么会清除 ep 指针中的数据并立刻返回。
除了上述两种特殊情况，使用 runtime.chanrecv 从 Channel 接收数据时还包含以下三种不同情况：
当存在等待的发送者时，通过 runtime.recv 从阻塞的发送者或者缓冲区中获取数据； 当缓冲区存在数据时，从 Channel 的缓冲区中接收数据； 当缓冲区中不存在数据时，等待其他 Goroutine 向 Channel 发送数据； 直接接收 ​ 当 Channel 的 sendq 队列中包含处于等待状态的 Goroutine 时，该函数会取出队列头等待的 Goroutine，处理的逻辑和发送时相差无几，只是发送数据时调用的是 runtime.send 函数，而接收数据时使用 runtime.recv ：
if sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). recv(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true, true } runtime.recv：
// recv processes a receive operation on a full channel c. // There are 2 parts: // 1) The value sent by the sender sg is put into the channel // and the sender is woken up to go on its merry way. // 2) The value received by the receiver (the current G) is // written to ep. // For synchronous channels, both values are the same. // For asynchronous channels, the receiver gets its data from // the channel buffer and the sender's data is put in the // channel buffer. // Channel c must be full and locked. recv unlocks c with unlockf. // sg must already be dequeued from c. // A non-nil ep must point to the heap or the caller's stack. func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if c.dataqsiz == 0 { if raceenabled { racesync(c, sg) } if ep != nil { // copy data from sender recvDirect(c.elemtype, sg, ep) } } else { // Queue is full. Take the item at the // head of the queue. Make the sender enqueue // its item at the tail of the queue. Since the // queue is full, those are both the same slot. qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) } // copy data from queue to receiver if ep != nil { typedmemmove(c.elemtype, ep, qp) } // copy data from sender to queue typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz } sg.elem = nil gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } recv函数会根据缓冲区的大小分别处理不同的情况：
如果 Channel 不存在缓冲区（dataqsiz = 0）； 调用 runtime.recvDirect 将 Channel 发送队列中 Goroutine 存储的 elem 数据拷贝到目标内存地址中； 如果 Channel 存在缓冲区（dataqsiz != 0）； 将队列中的数据拷贝到接收方的内存地址； 将发送队列头的数据拷贝到缓冲区中，释放一个阻塞的发送方； 无论发生哪种情况，运行时都会调用 runtime.goready 将当前处理器的 runnext 设置成发送数据的 Goroutine，在调度器下一次调度时将阻塞的发送方唤醒。
缓冲区 ​ 当 Channel 的缓冲区中已经包含数据时，从 Channel 中接收数据会直接从缓冲区中 recvx 的索引位置中取出数据进行处理：
if c.qcount > 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(&amp;c.lock) return true, true } 阻塞接收 ​ 当 Channel 的发送队列中不存在等待的 Goroutine 并且缓冲区中也不存在任何数据时，从管道中接收数据的操作会变成阻塞的，然而不是所有的接收操作都是阻塞的，与 select 语句结合使用时就可能会使用到非阻塞的接收操作：
func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { ... if !block { unlock(&amp;c.lock) return false, false } // no sender available: block on this channel. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) // someone woke us up if mysg != gp.waiting { throw("G waiting list is corrupted") } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime > 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) return true, success } Context ​ 上下文 context.Context Go 语言中用来设置截止日期、同步信号，传递请求相关值的结构体。Context 只定义了接口，凡是实现该接口的类都可称为是一种 context
​ context.Context 接口定义了四个需要实现的方法，其中包括：
Deadline — 返回 context.Context 被取消的时间，也就是完成工作的截止日期； Done — 返回一个 Channel，这个 Channel 会在当前工作完成或者上下文被取消后关闭，多次调用 Done 方法会返回同一个 Channel； Err — 返回 context.Context 结束的原因，它只会在 Done 方法对应的 Channel 关闭时返回非空的值； 如果 context.Context 被取消，会返回 Canceled 错误； 如果 context.Context 超时，会返回 DeadlineExceeded 错误； Value — 从 context.Context 中获取键对应的值，对于同一个上下文来说，多次调用 Value 并传入相同的 Key 会返回相同的结果，该方法可以用来传递请求特定的数据； type Context interface { Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct{} Err() error Value(key interface{}) interface{} } ​ 同时context 包中提供的 context.Background、context.TODO、context.WithDeadline 和 context.WithValue函数会返回实现该接口的私有结构体
默认上下文 ​ context包中最常用的方法还是 context.Background、context.TODO，这两个方法都会返回预先初始化好的私有变量 background 和 todo，它们会在同一个 Go 程序中被复用：
var ( background = new(emptyCtx) todo = new(emptyCtx) ) func Background() Context { return background } func TODO() Context { return todo } ​ 这两个私有变量都是通过 new(emptyCtx) 语句初始化的，它们是指向私有结构体 context.emptyCtx 的指针
​ 从源代码来看，context.Background和 context.TODO也只是互为别名，没有太大的差别，只是在使用和语义上稍有不同：
context.Background 是上下文的默认值，所有其他的上下文都应该从它衍生出来； context.TODO应该仅在不确定应该使用哪种上下文时使用； 在多数情况下，如果当前函数没有上下文作为入参，我们都会使用 context.Background作为起始的上下文向下传递。
官方的 context 类型 主要有四种，分别是 emptyCtx，cancelCtx，timerCtx，valueCtx：
emptyCtx：空的 context，实现了上面的 4 个接口，但都是直接 return 默认值，没有具体功能代码。
// An emptyCtx is never canceled, has no values, and has no deadline. It is not // struct{}, since vars of this type must have distinct addresses. type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() &lt;-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } func (e *emptyCtx) String() string { switch e { case background: return "context.Background" case todo: return "context.TODO" } return "unknown empty Context" } cancelCtx：用来取消通知用的 context
// A canceler is a context type that can be canceled directly. The // implementations are *cancelCtx and *timerCtx. type canceler interface { cancel(removeFromParent bool, err error) Done() &lt;-chan struct{} } // closedchan is a reusable closed channel. var closedchan = make(chan struct{}) func init() { close(closedchan) } // A cancelCtx can be canceled. When canceled, it also cancels any children // that implement canceler. type cancelCtx struct { Context mu sync.Mutex // protects following fields done chan struct{} // created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call } func (c *cancelCtx) Value(key interface{}) interface{} { if key == &amp;cancelCtxKey { return c } return c.Context.Value(key) } func (c *cancelCtx) Done() &lt;-chan struct{} { c.mu.Lock() if c.done == nil { c.done = make(chan struct{}) } d := c.done c.mu.Unlock() return d } func (c *cancelCtx) Err() error { c.mu.Lock() err := c.err c.mu.Unlock() return err } type stringer interface { String() string } func contextName(c Context) string { if s, ok := c.(stringer); ok { return s.String() } return reflectlite.TypeOf(c).String() } func (c *cancelCtx) String() string { return contextName(c.Context) + ".WithCancel" } // cancel closes c.done, cancels each of c's children, and, if // removeFromParent is true, removes c from its parent's children. func (c *cancelCtx) cancel(removeFromParent bool, err error) { if err == nil { panic("context: internal error: missing cancel error") } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // already canceled } c.err = err if c.done == nil { c.done = closedchan } else { close(c.done) } for child := range c.children { // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err) } c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } timerCtx：用来超时通知用的 context
/ A timerCtx carries a timer and a deadline. It embeds a cancelCtx to // implement Done and Err. It implements cancel by stopping its timer then // delegating to cancelCtx.cancel. type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } func (c *timerCtx) Deadline() (deadline time.Time, ok bool) { return c.deadline, true } func (c *timerCtx) String() string { return contextName(c.cancelCtx.Context) + ".WithDeadline(" + c.deadline.String() + " [" + time.Until(c.deadline).String() + "])" } func (c *timerCtx) cancel(removeFromParent bool, err error) { c.cancelCtx.cancel(false, err) if removeFromParent { // Remove this timerCtx from its parent cancelCtx's children. removeChild(c.cancelCtx.Context, c) } c.mu.Lock() if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } valueCtx：用来传值的 context
// A valueCtx carries a key-value pair. It implements Value for that key and // delegates all other calls to the embedded Context. type valueCtx struct { Context key, val interface{} } // stringify tries a bit to stringify v, without using fmt, since we don't // want context depending on the unicode tables. This is only used by // *valueCtx.String(). func stringify(v interface{}) string { switch s := v.(type) { case stringer: return s.String() case string: return s } return "&lt;not Stringer>" } func (c *valueCtx) String() string { return contextName(c.Context) + ".WithValue(type " + reflectlite.TypeOf(c.key).String() + ", val " + stringify(c.val) + ")" } func (c *valueCtx) Value(key interface{}) interface{} { if c.key == key { return c.val } return c.Context.Value(key) } 创建emptyCtx，cancelCtx，timerCtx，valueCtx方法 emptyCtx
// Background returns a non-nil, empty Context. It is never canceled, has no // values, and has no deadline. It is typically used by the main function, // initialization, and tests, and as the top-level Context for incoming // requests. func Background() Context { return background } // TODO returns a non-nil, empty Context. Code should use context.TODO when // it's unclear which Context to use or it is not yet available (because the // surrounding function has not yet been extended to accept a Context // parameter). func TODO() Context { return todo } cancelCtx
// WithCancel returns a copy of parent with a new Done channel. The returned // context's Done channel is closed when the returned cancel function is called // or when the parent context's Done channel is closed, whichever happens first. // // Canceling this context releases resources associated with it, so code should // call cancel as soon as the operations running in this Context complete. func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { if parent == nil { panic("cannot create context from nil parent") } c := newCancelCtx(parent) propagateCancel(parent, &amp;c) return &amp;c, func() { c.cancel(true, Canceled) } } timerCtx
// WithDeadline returns a copy of the parent context with the deadline adjusted // to be no later than d. If the parent's deadline is already earlier than d, // WithDeadline(parent, d) is semantically equivalent to parent. The returned // context's Done channel is closed when the deadline expires, when the returned // cancel function is called, or when the parent context's Done channel is // closed, whichever happens first. // // Canceling this context releases resources associated with it, so code should // call cancel as soon as the operations running in this Context complete. func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { if parent == nil { panic("cannot create context from nil parent") } if cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(d) { // The current deadline is already sooner than the new one. return WithCancel(parent) } c := &amp;timerCtx{ cancelCtx: newCancelCtx(parent), deadline: d, } propagateCancel(parent, c) dur := time.Until(d) if dur &lt;= 0 { c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() { c.cancel(false, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded) }) } return c, func() { c.cancel(true, Canceled) } } valueCtx
// WithValue returns a copy of parent in which the value associated with key is // val. // // Use context Values only for request-scoped data that transits processes and // APIs, not for passing optional parameters to functions. // // The provided key must be comparable and should not be of type // string or any other built-in type to avoid collisions between // packages using context. Users of WithValue should define their own // types for keys. To avoid allocating when assigning to an // interface{}, context keys often have concrete type // struct{}. Alternatively, exported context key variables' static // type should be a pointer or interface. func WithValue(parent Context, key, val interface{}) Context { if parent == nil { panic("cannot create context from nil parent") } if key == nil { panic("nil key") } if !reflectlite.TypeOf(key).Comparable() { panic("key is not comparable") } return &amp;valueCtx{parent, key, val} } make 和 new make 的作用是初始化内置的数据结构，也就是我们在前面提到的切片slice、哈希表map和 Channel new 的作用是根据传入的类型分配一片内存空间并返回指向这片内存空间的指针 值接收者和指针接收者的区别 ​ 方法能给用户自定义的类型添加新的行为。它和函数的区别在于方法有一个接收者，给一个函数添加一个接收者，那么它就变成了方法。接收者可以是值接收者，也可以是指针接收者。
​ 在调用方法的时候，值类型既可以调用值接收者的方法，也可以调用指针接收者的方法；指针类型既可以调用指针接收者的方法，也可以调用值接收者的方法。
type Person struct { age int } func (p Person) howOld() int { return p.age } func (p *Person) growUp() { p.age += 1 } func main() { // qcrao 是值类型 qcrao := Person{age: 18} // 值类型 调用接收者也是值类型的方法 fmt.Println(qcrao.howOld()) // 值类型 调用接收者是指针类型的方法 qcrao.growUp() fmt.Println(qcrao.howOld()) // ---------------------- // stefno 是指针类型 stefno := &amp;Person{age: 100} // 指针类型 调用接收者是值类型的方法 fmt.Println(stefno.howOld()) // 指针类型 调用接收者也是指针类型的方法 stefno.growUp() fmt.Println(stefno.howOld()) } //output 18 19 100 101 - 值接收者 指针接收者 值类型调用者 方法会使用调用者的一个副本，类似于“传值” 使用值的引用来调用方法，上例中，qcrao.growUp() 实际上是 (&amp;qcrao).growUp() 指针类型调用者 指针被解引用为值，上例中，stefno.howOld() 实际上是 (*stefno).howOld() 实际上也是“传值”，方法里的操作会影响到调用者，类似于指针传参，拷贝了一份指针 实现了接收者是值类型的方法，相当于自动实现了接收者是指针类型的方法；而实现了接收者是指针类型的方法，不会自动生成对应接收者是值类型的方法。
type coder interface { code() debug() } type Gopher struct { language string } func (p Gopher) code() { fmt.Printf("I am coding %s language\n", p.language) } func (p *Gopher) debug() { fmt.Printf("I am debuging %s language\n", p.language) } func main() { var c coder = &amp;Gopher{"Go"} c.code() c.debug() //报错 var c1 coder = Gopher{"Go"} c1.code() c1.debug() } 所以，当实现了一个接收者是值类型的方法，就可以自动生成一个接收者是对应指针类型的方法，因为两者都不会影响接收者。但是，当实现了一个接收者是指针类型的方法，如果此时自动生成一个接收者是值类型的方法，原本期望对接收者的改变（通过指针实现），现在无法实现，因为值类型会产生一个拷贝，不会真正影响调用者。
最后，只要记住下面这点就可以了：
如果实现了接收者是值类型的方法，会隐含地也实现了接收者是指针类型的方法。
两者分别在何时使用 如果方法的接收者是值类型，无论调用者是对象还是对象指针，修改的都是对象的副本，不影响调用者；如果方法的接收者是指针类型，则调用者修改的是指针指向的对象本身。
使用指针作为方法的接收者的理由：
方法能够修改接收者指向的值。 避免在每次调用方法时复制该值，在值的类型为大型结构体时，这样做会更加高效。 是使用值接收者还是指针接收者，不是由该方法是否修改了调用者（也就是接收者）来决定，而是应该基于该类型的本质。
如果类型具备“原始的本质”，也就是说它的成员都是由 Go 语言里内置的原始类型，如字符串，整型值等，那就定义值接收者类型的方法。像内置的引用类型，如 slice，map，interface，channel，这些类型比较特殊，声明他们的时候，实际上是创建了一个 header， 对于他们也是直接定义值接收者类型的方法。这样，调用函数时，是直接 copy 了这些类型的 header，而 header 本身就是为复制设计的。
如果类型具备非原始的本质，不能被安全地复制，这种类型总是应该被共享，那就定义指针接收者的方法。比如 go 源码里的文件结构体（struct File）就不应该被复制，应该只有一份实体。
接口的动态类型和动态值 接口值的零值是指动态类型和动态值都为 nil。当仅且当这两部分的值都为 nil 的情况下，这个接口值就才会被认为 接口值 == nil
反射 维基百科上反射的定义：
在计算机科学中，反射是指计算机程序在运行时（Run time）可以访问、检测和修改它本身状态或行为的一种能力。用比喻来说，反射就是程序在运行的时候能够“观察”并且修改自己的行为。
《Go 语言圣经》中是这样定义反射的：
Go 语言提供了一种机制在运行时更新变量和检查它们的值、调用它们的方法，但是在编译时并不知道这些变量的具体类型，这称为反射机制。
使用反射的常见场景有以下两种：
不能明确接口调用哪个函数，需要根据传入的参数在运行时决定。 不能明确传入函数的参数类型，需要在运行时处理任意对象。 如何实现 ​ interface，它是 Go 语言实现抽象的一个非常强大的工具。当向接口变量赋予一个实体类型的时候，接口会存储实体的类型信息，反射就是通过接口的类型信息实现的，反射建立在类型的基础上
​ Go 语言中，每个变量都有一个静态类型，在编译阶段就确定了的，比如 int, float64, []int 等等。注意，这个类型是声明时候的类型，不是底层数据类型。
type MyInt int var i int var j MyInt 尽管 i，j 的底层类型都是 int，他们是不同的静态类型，除非进行类型转换，否则，i 和 j 不能同时出现在等号两侧。j 的静态类型就是 MyInt 反射主要与 interface{} 类型相关。关于 interface 的底层结构（iface和eface），如下所示
type iface struct { tab *itab data unsafe.Pointer } type itab struct { inter *interfacetype _type *_type link *itab hash uint32 bad bool inhash bool unused [2]byte fun [1]uintptr } 其中 itab 由具体类型 _type 以及 interfacetype 组成。_type 表示具体类型，而 interfacetype 则表示具体类型实现的接口类型
实际上，iface 描述的是非空接口，它包含方法；与之相对的是 eface，描述的是空接口，不包含任何方法，Go 语言里所以类型都 “实现了” 空接口
type eface struct { _type *_type data unsafe.Pointer } 相比 iface，eface 就比较简单了。只维护了一个 _type 字段，表示空接口所承载的具体的实体类型。data 描述了具体的值
接口interface的静态类型和动态类型 type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } var r io.Reader tty, err := os.OpenFile("/Users/qcrao/Desktop/test", os.O_RDWR, 0) if err != nil { return nil, err } r = tty 首先声明 r 的类型是 io.Reader，注意，这是 r 的静态类型，此时它的动态类型为 nil，并且它的动态值也是 nil。
之后，r = tty 这一语句，将 r 的动态类型变成 *os.File，动态值则变成非空，表示打开的文件对象。这时，r 可以用&lt;value, type>对来表示为： &lt;tty, *os.File>
注意看上图，此时虽然 fun 所指向的函数只有一个 Read 函数，其实 *os.File 还包含 Write 函数，也就是说 *os.File 其实还实现了 io.Writer 接口。因此下面的断言语句可以执行：
var w io.Writer w = r.(io.Writer) 之所以用断言，而不能直接赋值，是因为 r 的静态类型是 io.Reader，并没有实现 io.Writer 接口。断言能否成功，看 r 的动态类型是否符合要求
这样，w 也可以表示成 &lt;tty, *os.File>，仅管它和 r 一样，但是 w 可调用的函数取决于它的静态类型 io.Writer，也就是说它只能有这样的调用形式： w.Write() 。w 的内存形式如下图：
和 r 相比，仅仅是 fun 对应的函数变了：Read -> Write
接着将w赋值给empty
var empty interface{} empty = w 反射的基本函数 reflect.ValueOf() 获取输入参数接口中的数据的值
源码：src/reflect/value.go
// ValueOf returns a new Value initialized to the concrete value // stored in the interface i. ValueOf(nil) returns the zero Value. func ValueOf(i interface{}) Value { if i == nil { return Value{} } // TODO: Maybe allow contents of a Value to live on the stack. // For now we make the contents always escape to the heap. It // makes life easier in a few places (see chanrecv/mapassign // comment below). escapes(i) return unpackEface(i) } 还有一堆关于value的类型方法....... reflect.TypeOf() 获取输入参数接口中值的类型
源码：src/reflect/type.go
func TypeOf(i interface{}) Type { eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ) } type Type interface { // 所有的类型都可以调用下面这些函数 // 此类型的变量对齐后所占用的字节数 Align() int // 如果是 struct 的字段，对齐后占用的字节数 FieldAlign() int // 返回类型方法集里的第 `i` (传入的参数)个方法 Method(int) Method // 通过名称获取方法 MethodByName(string) (Method, bool) // 获取类型方法集里导出的方法个数 NumMethod() int // 类型名称 Name() string // 返回类型所在的路径，如：encoding/base64 PkgPath() string // 返回类型的大小，和 unsafe.Sizeof 功能类似 Size() uintptr // 返回类型的字符串表示形式 String() string // 返回类型的类型值 Kind() Kind // 类型是否实现了接口 u Implements(u Type) bool // 是否可以赋值给 u AssignableTo(u Type) bool // 是否可以类型转换成 u ConvertibleTo(u Type) bool // 类型是否可以比较 Comparable() bool // 下面这些函数只有特定类型可以调用 // 如：Key, Elem 两个方法就只能是 Map 类型才能调用 // 类型所占据的位数 Bits() int // 返回通道的方向，只能是 chan 类型调用 ChanDir() ChanDir // 返回类型是否是可变参数，只能是 func 类型调用 // 比如 t 是类型 func(x int, y ... float64) // 那么 t.IsVariadic() == true IsVariadic() bool // 返回内部子元素类型，只能由类型 Array, Chan, Map, Ptr, or Slice 调用 Elem() Type // 返回结构体类型的第 i 个字段，只能是结构体类型调用 // 如果 i 超过了总字段数，就会 panic Field(i int) StructField // 返回嵌套的结构体的字段 FieldByIndex(index []int) StructField // 通过字段名称获取字段 FieldByName(name string) (StructField, bool) // FieldByNameFunc returns the struct field with a name // 返回名称符合 func 函数的字段 FieldByNameFunc(match func(string) bool) (StructField, bool) // 获取函数类型的第 i 个参数的类型 In(i int) Type // 返回 map 的 key 类型，只能由类型 map 调用 Key() Type // 返回 Array 的长度，只能由类型 Array 调用 Len() int // 返回类型字段的数量，只能由类型 Struct 调用 NumField() int // 返回函数类型的输入参数个数 NumIn() int // 返回函数类型的返回值个数 NumOut() int // 返回函数类型的第 i 个值的类型 Out(i int) Type // 返回类型结构体的相同部分 common() *rtype // 返回类型结构体的不同部分 uncommon() *uncommonType } reflect.ValueOf().Elem() 获取原始可操作的数据
反射的三大定律 反射可以将interface 类型变量转换成反射对象
func main(){ var x float64 = 3.4 t := reflect.TypeOf(x) fmt.Println("type:", t) v := reflect.ValueOf(x) fmt.Println("value", v) } 反射可以将反射对象还原成interface 对象
func main(){ var x float64 = 3.4 v := reflect.ValueOf(x) //v is reflext.Value var y float64 = v.Interface().(float64) fmt.Println("value", y) } 反射对象可修改，value值必须是可设置的
func main(){ var x float64 = 3.4 v := reflect.ValueOf(&amp;x) v.Elem().SetFloat(6.6) fmt.Println("x :", v.Elem().Interface()) } 断言 类型断言（Type Assertion）是一个使用在接口值上的操作，用于检查接口类型变量所持有的值是否实现了期望的接口或者具体的类型。
在Go语言中类型断言的语法格式如下：
value, ok := x.(T) 其中，x 表示一个接口的类型，T 表示一个具体的类型（也可为接口类型）。
该断言表达式会返回 x 的值（也就是 value）和一个布尔值（也就是 ok），可根据该布尔值判断 x 是否为 T 类型：
如果 T 是具体某个类型，类型断言会检查 x 的动态类型是否等于具体类型 T。如果检查成功，类型断言返回的结果是 x 的动态值，其类型是 T。 如果 T 是接口类型，类型断言会检查 x 的动态类型是否满足 T。如果检查成功，x 的动态值不会被提取，返回值是一个类型为 T 的接口值。 无论 T 是什么类型，如果 x 是 nil 接口值，类型断言都会失败。 内存模型 ​ 基本模型的相关概念和理解可以参考我另一篇文章（重点看图理解）：https://mrvwy.github.io/2020/08/02/%E8%81%8A%E8%81%8Athread-caching-malloc/
​ TCMalloc 是用来替代传统的malloc内存分配函数。它有减少内存碎片，适用于多核，更好的并行性支持等特性。
相关名词概念：
Page
操作系统对内存管理的单位，TCMalloc也是以页为单位管理内存，但是TCMalloc中Page大小是操作系统中页的倍数关系。2，4，8 &hellip;.
Span
Span 是PageHeap中管理内存页的单位，它是由一组连续的Page组成，比如2个Page组成的span，多个这样的span就用链表来管理。当然，还可以有4个Page组成的span等等。
ThreadCache
ThreadCache是每个线程各自独立拥有的cache，一个cache包含多个空闲内存链表（size classes），每一个链表（size-class）都有自己的object，每个object都是大小相同的。
CentralCache
CentralCache是当ThreadCache内存不足时，提供内存供其使用。它保持的是空闲块链表，链表数量和ThreadCache数量相同。ThreadCache中内存过多时，可以放回CentralCache中。
PageHeap
PageHeap保存的也是若干链表，不过链表保存的是Span（多个相同的page组成一个Span）。CentralCache内存不足时，可以从PageHeap获取Span，然后把Span切割成object。
map 为啥是无序的 //https://github.com/golang/go/blob/36f30ba289e31df033d100b2adb4eaf557f05a34/src/runtime/map.go func mapiterinit(t *maptype, h *hmap, it *hiter) { // decide where to start r := uintptr(fastrand()) if h.B > 31-bucketCntBits { r += uintptr(fastrand()) &lt;&lt; 31 } mapiternext(it) } 主要是因为 map 在扩容后，可能会将部分 key 移至新内存，那么这一部分实际上就已经是无序的了。而遍历的过程，其实就是按顺序遍历内存地址，同时按顺序遍历内存地址中的 key。但这时已经是无序的了。当然如果就一个 map，我保证不会对 map 进行修改删除等操作，那么按理说没有扩容就不会发生改变。但也是因为这样，GO 才在源码中加上随机的元素，将遍历 map 的顺序随机化，用来防止使用者用来顺序遍历，因为在某些情况下，可能会酿成大错。
原理 // A header for a Go map. type hmap struct { // 元素个数，调用 len(map) 时，直接返回此值 count int flags uint8 // buckets 的对数 log_2 B uint8 // overflow 的 bucket 近似数 noverflow uint16 // 计算 key 的哈希的时候会传入哈希函数 hash0 uint32 // 指向 buckets 数组，大小为 2^B // 如果元素个数为0，就为 nil buckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 oldbuckets unsafe.Pointer // 指示扩容进度，小于此地址的 buckets 迁移完成 nevacuate uintptr extra *mapextra // optional fields } B 是 buckets 数组的长度的对数，也就是说 buckets 数组的长度就是 2^B。bucket 里面存储了 key 和 value，后面会再讲。
buckets 是一个指针，最终它指向的是一个结构体：
// A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] &lt; minTopHash, // tophash[0] is a bucket evacuation state instead. tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer. } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。
整体图
当 map 的 key 和 value 都不是指针，并且 size 都小于 128 字节的情况下，会把 bmap 标记为不含指针，这样可以避免 gc 时扫描整个 hmap。但是，我们看 bmap 其实有一个 overflow 的字段，是指针类型的，破坏了 bmap 不含指针的设想，这时会把 overflow 移动到 extra 字段来。
// mapextra holds fields that are not present on all maps. type mapextra struct { // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap } bmap 是存放 k-v 的地方，bmap 的内部组成如下
上图就是 bucket 的内存模型，HOB Hash 指的就是 top hash。 注意到 key 和 value 是各自放在一起的，并不是 key/value/key/value/... 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。
例如，有这样一个类型的 map：map[int64]int8，如果按照 key/value/key/value/... 这样的模式存储，那在每一个 key/value 对之后都要额外 padding 7 个字节；而将所有的 key，value 分别绑定到一起，这种形式 key/key/.../value/value/...，则只需要在最后添加 padding。每个 bucket 设计成最多只能放 8 个 key-value 对，如果有第 9 个 key-value 落入当前的 bucket，那就需要再构建一个 bucket ，通过 overflow 指针连接起来。
扩容 符合下面这 2 个条件，就会触发扩容：
装载因子超过阈值，源码里定义的阈值是 6.5。当装载因子超过 6.5 时，表明很多 bucket 都快要装满了，查找效率和插入效率都变低了。在这个时候进行扩容是有必要的 overflow 的 bucket 数量过多：当 B 小于 15，也就是 bucket 总数 2^B 小于 2^15 时，如果 overflow 的 bucket 数量超过 2^B；当 B >= 15，也就是 bucket 总数 2^B 大于等于 2^15，如果 overflow 的 bucket 数量超过 2^15。 注意 直接将使用 map1 == map2 是错误的。这种写法只能比较 map 是否为 nil。 无法对 map 的 key 或 value 进行取址 map 并不是一个线程安全的数据结构。同时读写一个 map 是未定义的行为，如果被检测到，会直接 panic。可以通过读写锁来解决：sync.RWMutex map 不是线程安全的。在查找、赋值、遍历、删除的过程中都会检测写标志，一旦发现写标志置位（等于1），则直接 panic。赋值和删除函数在检测完写标志是复位之后，先将写标志位置位，才会进行之后的操作。 值传递和引用传递 形参和实参 形式参数：是在定义函数名和函数体的时候使用的参数,目的是用来接收调用该函数时传入的参数。 实际参数：在调用有参函数时，主调函数和被调函数之间有数据传递关系。在主调函数中调用一个函数时，函数名后面括号中的参数称为“实际参数” 值传递和引用传递 Go语言中所有的传参都是值传递（传值），都是一个副本，一个拷贝。因为拷贝的内容有时候是非引用类型（int、string、struct等这些），这样就在函数中就无法修改原内容数据；有的是引用类型（指针、map、slice、chan等这些），这样就可以修改原内容数据。
内存对齐 ​ 相关概念和理解可以参考我另一篇文章：https://mrvwy.github.io/2020/07/18/data-structure-alignment/
​ 例子：
变量 a、b 各占据 3 字节的空间，内存对齐后，a、b 占据 4 字节空间，CPU 读取 b 变量的值只需要进行一次内存访问。如果不进行内存对齐，CPU 读取 b 变量的值需要进行 2 次内存访问。第一次访问得到 b 变量的第 1 个字节，第二次访问得到 b 变量的后两个字节。
rune 类型 uint8 类型，或者叫 byte 型，代表了 ASCII 码的一个字符。 rune 类型，代表一个 UTF-8 字符，当需要处理中文、日文或者其他复合字符时，则需要用到 rune 类型。rune 类型等价于 int32 类型。 defer和return执行先后顺序 多个defer的执行顺序为“后进先出”； defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出。 ​ 如果函数的返回值是无名的（不带命名返回值），则go语言会在执行return的时候会执行一个类似创建一个临时变量作为保存return值的动作，而有名返回值的函数，由于返回值在函数定义的时候已经将该变量进行定义，在执行return的时候会先执行返回值保存操作，而后续的defer函数会改变这个返回值(虽然defer是在return之后执行的，但是由于使用的函数定义的变量，所以执行defer操作后对该变量的修改会影响到return的值
//例子1 func increaseA() int { // 返回的是ret var i int //1. i被初始化为0 defer func() { i++ // 3. i = 1 }() return i // 2. ret = i } func increaseB() (r int) { defer func() { r++ }() return r } func main() { fmt.Println(increaseA()) fmt.Println(increaseB()) } //output ： 0 1 //分析：1.发生return时，将赋值给返回值（可以理解为go自动创建了一个返回值ret，相当于执行了ret=value），此时defer修改该的是value而不是ret。 //2 .而在命名返回值函数中，由于返回值在方法定义时已经被定义，所以不会在创建retValue了，defer对value的修改也会被直接返回。 //例子2 func f1() (r int) { //有命名返回，返回是r defer func() { r++ //2. r = r + 1 = 1 }() return 0 // 1. r = 0 } func f2() (r int) { //有命名返回，返回是r t := 5 defer func() { t = t + 5 // 2. t = t + 5 = 10 }() return t // 1 . r = t = 5 } func f3() (r int) { //有命名返回，返回是r defer func(r int) { r = r + 5 //进入defer的函数之后r又是一个全新的指向不同内存地址的变量，不会对外层函数返回的r造成影响 }(r) return 1 //1. r = 1 } func f4() (r int) { //有命名返回，返回是r t := 5 defer func() { r = t + 5 // 2. r = t + 5 = 10 }() return t //1. r = t = 5 } //output ： 1, 5, 1, 10 //例子3 var a bool = true func main() { defer func(){ fmt.Println("1") }() if a == true { fmt.Println("2") return } defer func(){ fmt.Println("3") }() } //output ： 2, 1 //分析 ： 关键字后面的函数或者方法想要执行必须先注册，return 之后的 defer 是不能注册的， 也就不能执行后面的函数或方法。 //例子4 func calc(index string, a, b int) int { ret := a + b fmt.Println(index, a, b, ret) return ret } func main() { a := 1 b := 2 defer calc("1", a, calc("10", a, b)) a = 0 defer calc("2", a, calc("20", a, b)) b = 1 } //output : 10 1 2 3 // 20 0 2 2 // 2 0 2 2 // 1 1 3 4 //分析： 不管代码顺序如何，defer-calc-func中参数b必须先计算，故会在运行到第一个defer时，执行calc("10",a,b)输出：10 1 2 3得到值3，将cal("1",1,3)存放到延后执执行函数队列中。同理运行到第二个defer也是一样。 多重赋值 func main() { i := 1 s := []string{"A", "B", "C"} i, s[i-1] = 2, "Z" // 先计算 s[i-1] 最后变成 i, s[0] = 2, “Z” fmt.Printf("s: %v \n", s) } //output ： [Z,B,C] 多重赋值分为两个步骤，有先后顺序：
计算等号左边的索引表达式和取址表达式，接着计算等号右边的表达式 赋值 nil 可以用作 interface、function、pointer、map、slice 和 channel 的“空值” ​ nil 可以用作 interface、function、pointer、map、slice 和 channel 的“空值”
Reference https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-memory-allocator/ https://golang.design/go-questions/interface/receiver/ https://github.com/LeoYang90/Golang-Internal-Notes https://www.topgoer.cn/docs/gomianshiti</content></entry><entry><title>EVPN</title><url>/post/evpn/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> 最初的VXLAN方案（RFC7348）中没有定义控制平面，是手工配置VXLAN隧道，然后通过流量泛洪的方式进行主机地址的学习。这种方式实现上较为简单，但是会导致网络中存在很多泛洪流量、网络扩展起来困难。
为了解决上述问题，VXLAN引入了EVPN（Ethernet VPN）作为VXLAN的控制平面。EVPN参考了BGP/MPLS IP VPN的机制，通过扩展BGP协议新定义了几种BGP EVPN路由，通过在网络中发布路由来实现VTEP的自动发现、主机地址学习。
采用EVPN作为控制平面具有以下一些优势：
可实现VTEP自动发现、VXLAN隧道自动建立，从而降低网络部署、扩展的难度。
EVPN可以同时发布二层MAC和三层路由信息。
可以减少网络中泛洪流量。
EVPN网络模型 EVPN的典型网络模型中包括如下几部分 VTEP（VXLAN Tunnel End Point，VXLAN隧道端点）：EVPN的边缘设备。EVPN的相关处理都在VTEP上进行。VTEP可以是一台独立的物理设备，也可以是虚拟机所在的服务器。 VXLAN隧道：两个VTEP之间的点到点逻辑隧道。VTEP为数据帧封装VXLAN头、UDP头和IP头后，通过VXLAN隧道将封装后的报文转发给远端VTEP，远端VTEP对其进行解封装。 核心设备：IP核心网络中的设备。核心设备不参与EVPN处理，仅需要根据封装后报文的外层目的IP地址对报文进行三层转发。 VXLAN网络/EVPN实例：用户网络可能包括分布在不同地理位置的多个站点内的虚拟机。在骨干网上可以利用VXLAN隧道将这些站点连接起来，为用户提供一个逻辑的二层VPN。这个二层VPN称为一个VXLAN网络，也称为EVPN实例。VXLAN网络通过VXLAN ID来标识，VXLAN ID又称VNI（VXLAN Network Identifier，VXLAN网络标识符），其长度为24比特。不同VXLAN网络中的虚拟机不能二层互通。 VSI（Virtual Switch Instance，虚拟交换实例）：VTEP上为一个VXLAN提供二层交换服务的虚拟交换实例。VSI可以看做是VTEP上的一台基于VXLAN进行二层转发的虚拟交换机。VSI与VXLAN一一对应。 ES（Ethernet Segment，以太网段）：用户站点连接到VTEP的链路，通过ESI（Ethernet Segment Identifier，以太网段标识符）唯一标识。目前，一个用户站点只能通过一条链路连接一台VTEP，该ES的ESI为0。 多归属站点 DF选举 当一个站点连接到多台VTEP时，为了避免冗余备份组中的VTEP均发送泛洪流量给该站点，需要在冗余备份组中为每个AC选举一个VTEP作为DF（Designated Forwarder，指定转发者），负责将泛洪流量转发给该AC。其他VTEP作为该AC的BDF（Backup DF，备份DF），不会向本地站点转发泛洪流量。多归属成员通过发送以太网段路由，向远端VTEP通告ES及其连接的VTEP信息，远端VTEP根据ES、VTEP信息选举出DF。
水平分割 在多归属站点组网中，VTEP接收到站点发送的组播、广播和未知单播数据帧后，判断数据帧所属的VXLAN，通过该VXLAN内除接收接口外的所有本地接口和VXLAN隧道转发该数据帧。同一冗余备份组中的VTEP接收到该数据帧后会在本地所属的VXLAN内泛洪，这样数据帧会通过AC泛洪到本地站点，造成环路和站点的重复接收。EVPN通过水平分割解决该问题。水平分割的机制为：VTEP接收到同一冗余备份组中成员转发的广播、组播、未知单播数据帧后，不向具有相同ESI标识的ES转发该数据帧。
ARP泛洪抑制（代答） 为了避免广播发送的ARP请求报文占用核心网络带宽，VTEP会根据接收到的ARP请求和ARP应答报文、BGP EVPN路由在本地建立ARP泛洪抑制表项。后续当VTEP收到本站点内虚拟机请求其它虚拟机MAC地址的ARP请求时，优先根据ARP泛洪抑制表项进行代答。如果没有对应的表项，则通过VXLAN隧道将ARP请求泛洪到其他站点。ARP泛洪抑制功能可以大大减少ARP泛洪的次数。
ARP泛洪抑制的处理过程如下：
虚拟机VM 1发送ARP请求，获取VM 7的MAC地址。 VTEP 1根据接收到的ARP请求，建立VM 1的ARP泛洪抑制表项，在VXLAN内泛洪该ARP请求。VTEP 1还会通过BGP EVPN将该表项同步给VTEP 2和VTEP 3。 远端VTEP（VTEP 2和VTEP 3）解封装VXLAN报文，获取原始的ARP请求报文后，在本地站点的指定VXLAN内泛洪该ARP请求。 VM 7接收到ARP请求后，回复ARP应答报文。 VTEP 2接收到ARP应答后，建立VM 7的ARP泛洪抑制表项，通过VXLAN隧道将ARP应答发送给VTEP 1。VTEP 2通过BGP EVPN将该表项同步给VTEP 1和VTEP 3。 VTEP 1解封装VXLAN报文，获取原始的ARP应答，将ARP应答报文发送给VM 1。 在VTEP 1上建立ARP泛洪抑制表项后，虚拟机VM 4发送ARP请求，获取VM 1的MAC地址。 VTEP 1接收到ARP请求后，建立VM 4的ARP泛洪抑制表项，并查找本地ARP泛洪抑制表项，根据已有的表项回复ARP应答报文，不会对ARP请求进行泛洪。 虚拟机VM 10发送ARP请求，获取VM 1的MAC地址。 VTEP 3接收到ARP请求后，建立VM 10的ARP泛洪抑制表项，并查找ARP泛洪抑制表项，根据已有的表项（VTEP 1通过BGP EVPN同步回复ARP应答报文，不会对ARP请求进行泛洪。</content></entry><entry><title>Linux virtual devices</title><url>/post/linux-virtual-devices/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"> Linux virtual devices Type Bridge、Veth、Tun、Tap
Linux namespace namespace的本质就是指：一种在空间上隔离的概念，当下盛行的许多容器虚拟化技术（典型代表如LXC、Docker）就是基于linux命名空间的概念而来的。
其主要包括：
Cgroup（物理资源隔离） IPC Mount PID User UTS Network network namespace network namespace 能创建多个隔离的网络空间，它们有独自的网络栈信息。不管是虚拟机还是容器，运行的时候仿佛自己就在独立的网络中。
对于不同的网络空间，因为其互相之间隔离，因此可以使用Veth接口来连通不同网络空间
use //create ns1 network namespace ip nets add ns1 ip nets list //add veth1 interface to ns1 network namespace ip link set veth1 netns ns1 //add IP in veth1 //ip netns exec &lt;network namespace name> &lt;command> ip netns exec ns1 ip addr add &lt;IP> dev veth1 //run shell in network namespace ip netns exec &lt;network namespace name> bash //back: "exit" Bridge linux的Bridge网桥，其有点类似于交换机，也是在其连接的接口间转发数据包。
Veth-Pair veth-pair 是一对的虚拟设备接口，和 tap/tun 设备不同的是，它都是成对出现的。一端连着协议栈，一端彼此相连着。一旦一端设备关闭，那么另外一端的设备也将会被关闭。
//create veth0&lt;-->veth1 ip link add veth0 type veth peer name veth1 Tun/Tap Tun/Tap设备可以在协议栈中把数据包转发给上层用户自定义的程序去处理。
流程 过程1-2-3-4-5-6-7-8 +----------------------------------------------------------------+ | | | +--------------------+ +--------------------+ | | | User Application A | | User Application B |&lt;-----+ | | +--------------------+ +--------------------+ | | | | 1 | 5 | | |...............|......................|...................|.....| | ↓ ↓ | | | +----------+ +----------+ | | | | socket A | | socket B | | | | +----------+ +----------+ | | | | 2 | 6 | | |.................|.................|......................|.....| | ↓ ↓ | | | +------------------------+ 4 | | | | Newwork Protocol Stack | | | | +------------------------+ | | | | 7 | 3 | | |................|...................|.....................|.....| | ↓ ↓ | | | +----------------+ +----------------+ | | | | eth0 | | tun0 | | | | +----------------+ +----------------+ | | | 2.2.2.2 | | 1.1.1.1 | | | | 8 +---------------------+ | | | | +----------------|-----------------------------------------------+ ↓ Physical Network 区别 通过Tun设备只能读写IP数据包，而通过Tap设备能读写链路层数据包
songgao/water 具体操作方法看官网文档：https://github.com/songgao/water
如何实现 由于本人十分好奇它底层是如何创建Tun/Tap设备，因此走了一轮源码。
func ioctl(fd uintptr, request uintptr, argp uintptr) error { _, _, errno := syscall.Syscall(syscall.SYS_IOCTL, fd, uintptr(request), argp) if errno != 0 { return os.NewSyscallError("ioctl", errno) } return nil } //可以发现通过syscall.Syscall发送信号syscall.SYS_IOCTL调用Linux内核函数ioctl来创建Tun/Tap设备 Reference https://gist.github.com/mtds/4c4925c2aa022130e4b7c538fdd5a89f</content></entry><entry><title>linux cgroup</title><url>/post/linux-cgroup/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"> cgroup和namespace类似，但是namespace会隔离不同进程组之间的资源，而cgroup是为了对进程组的资源进行限制和监控。
组成 tasks：在cgroup里面，一个tasks表示系统中一个进程组，在cgroup树中，一个tasks可以被认为是一个Node节点 subsystem：一个资源调度器（Resource Controller）可以通过lssubsys -a命令查看当前内核支持哪些subsystem hierarchy(层级结构)： 一个hierarchy可以理解为一棵cgroup树，树的每个节点就是一个进程组，每棵树都会与零到多个subsystem关联。系统中可以有很多颗cgroup树，每棵树都和不同的subsystem关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组，只是这些进程组和不同的subsystem关联。如果每个cgroup树不与subsystem关联，那么此时cgroup的作用也只是单纯的将进程分组而已 规程 参考：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-relationships_between_subsystems_hierarchies_control_groups_and_tasks
Rule-1 Rule-2 Rule-3 Rule-4 subsystem blkio ：限制进程的块设备 io cpu ：限制进程的 cpu 使用率 cpuacct：统计 cgroups 中的进程的 cpu 使用报告 cpuset：统计 cgroups 中的进程的 cpu 使用报告 devices ：控制进程能够访问某些设备 freezer ：挂起或者恢复 cgroups 中的进程 memory ：限制进程的 memory 使用量 net_cls：标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块（traffic control）对数据包进行控制 ns ：使不同 cgroups 下面的进程使用不同的 namespace perf_event： net_prio： 挂载cgroup树 一般在sys/fs/cgroup下已经创建好各个与subsystem关联的cgroup树
挂载 mkdir /sys/fs/cgroup/&lt; subsystem name > mount -t cgroup -o &lt; subsystem name > &lt;mount name> /sys/fs/cgroup/&lt; subsystem name > 查看 mount | grep cgroup cgroup树目录 根目录下主要文件 cgroup.clone_children：该文件的内容为1时，当cgroup退出时（不再包含任何进程和子cgroup），将调用release_agent里面配置的命令。新cgroup被创建时将默认继承父cgroup的这项配置 cgroup.event_control:用于eventfd的接口，可以在进程组状态发生改变时发送通知 cgroup.procs ：当前cgroup中的所有进程ID notify_on_release：该文件的内容为1时，当cgroup退出时（不再包含任何进程和子cgroup），将调用release_agent里面配置的命令。新cgroup被创建时将默认继承父cgroup的这项配置 tasks：当前cgroup中进程里面所有线程ID release_agent：里面包含了cgroup退出时将会执行的命令，系统调用该命令时会将相应cgroup的相对路径当作参数传进去。 cgroup.sane_behavior 子目录与根目录下文件基本一样，只不过子目录比根目录少了release_agent和cgroup.sane_behavior文件
创建子memory group cd /sys/fs/cgroup/memory mkdir mycgroup echo $pid > /sys/fs/cgroup/memory/mycgroup/cgroup.procs echo 1M > /sys/fs/cgroup/memory/mycgroup/memory.max_usage_in_bytes Reference https://www.kernel.org/doc/Documentation/cgroup-v2.txt https://www.kernel.org/doc/Documentation/ https://tech.meituan.com/2015/03/31/cgroups.html https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-relationships_between_subsystems_hierarchies_control_groups_and_tasks</content></entry><entry><title>linux nf_conntrack</title><url>/post/linux-nf_conntrack/</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"> nf_conntrack是一个内核模块，其用于跟踪一个连接的状态。
为什么需要跟踪连接的状态？ nf_conntrack允许内核”审查”通过此处的所有网络数据包，并能识别出此数据包属于哪条连接。因此，连接跟踪机制使内核能够跟踪并记录通过此处的所有网络连接及其状态。
结合Iptables使用，这条 INPUT 规则用于放行 80 端口上的状态为 NEW 的连接上的包。
iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT 状态 New：匹配连接的第一个包（分组对应的连接在两个方向上都没有进行过分组传输）。意思就是，iptables从连接跟踪表中查到此包是某连接的第一个包。判断此包是某连接的第一个包是依据conntrack当前”只看到一个方向数据包”([UNREPLIED])，不关联特定协议，因此NEW并不单指tcp连接的SYN包
Established：匹配连接的响应包及后续的包。意思是，iptables从连接跟踪表中查到此包是属于一个已经收到响应的连接(即没有[UNREPLIED]字段)。因此在iptables状态中，只要发送并接到响应，连接就认为是Established的了。这个特点使iptables可以控制由谁发起的连接才可以通过，比如A与B通信，A发给B数据包属于NEW状态，B回复给A的数据包就变为Established状态。ICMP的错误和重定向等信息包也被看作是Established，只要它们是我们所发出的信息的应答
Related：匹配那些属于Related连接的包，这句话说了跟没说一样。Related状态有点复杂，当一个连接与另一个已经是Established的连接有关时，这个连接就被认为是Related。这意味着，一个连接要想成为Related，必须首先有一个已经是Established的连接存在。这个Established连接再产生一个主连接之外的新连接，这个新连接就是Related状态了，当然首先Conntrack模块要能”读懂”它是Related。拿ftp来说，FTP数据传输连接就是Related与先前已建立的FTP控制连接，还有通过IRC的DCC连接。有了Related这个状态，ICMP错误消息、FTP传输、DCC等才能穿过防火墙正常工作。有些依赖此机制的TCP协议和UDP协议非常复杂，他们的连接被封装在其它的TCP或UDP包的数据部分(可以了解下overlay/vxlan/gre)，这使得Conntrack需要借助其它辅助模块才能正确”读懂”这些复杂数据包，比如nf_conntrack_ftp这个辅助模块
Invalid：匹配那些无法识别或没有任何状态的数据包。这可能是由于系统内存不足或收到不属于任何已知连接的ICMP错误消息。一般情况下我们应该DROP此类状态的包
Untracked：状态比较简单，它匹配那些带有NOTRACK标签的数据包。需要注意的一点是，如果你在raw表中对某些数据包设置有NOTRACK标签，那上面的4种状态将无法匹配这样的数据包，因此你需要单独考虑NOTRACK包的放行规则
如何存储连接状态 nf_conntrack 主要利用hash来存储其跟踪的连接
源码参考连接
https://code.woboq.org/linux/linux/net/netfilter/nf_conntrack_core.c.html nf_conntrack_tuple 主要存储src和dst 通过下面源码，可以看出其主要支持tcp、udp、icmp、dccp、sctp、gre (具体看最新源码) struct nf_conntrack_tuple { { { union nf_inet_addr u3; -------| __be32 ip; | { .......... struct nf_conntrack_man src; ------| { struct { __be16 port; } tcp | union nf_conntrack_man_proto u;-----| struct { __be16 port; } udp | | struct { __be16 port; } icmp | | struct { __be16 port; } dccp | | struct { __be16 port; } sctp | { struct { __be16 port; } gre { u_int16_t l3num; /* Layer 3 protocol */ /* These are the parts of the tuple which are fixed. */ struct { union nf_inet_addr u3; union { /* Add other protocols here. */ __be16 all; struct { __be16 port; } tcp; struct { __be16 port; } udp; struct { u_int8_t type, code; } icmp; struct { __be16 port; } dccp; struct { __be16 port; } sctp; struct { __be16 key; } gre; } u; /* The protocol. */ u_int8_t protonum; /* The direction (for tuplehash) */ u_int8_t dir; } dst; }; hash_conntrack_raw函数 利用 Tuple 的src、dst字段来计算哈希
static u32 hash_conntrack_raw(const struct nf_conntrack_tuple *tuple, const struct net *net) { unsigned int n; u32 seed; get_random_once(&amp;nf_conntrack_hash_rnd, sizeof(nf_conntrack_hash_rnd)); /* The direction must be ignored, so we hash everything up to the * destination ports (which is a multiple of 4) and treat the last * three bytes manually. */ seed = nf_conntrack_hash_rnd ^ net_hash_mix(net); n = ( sizeof( tuple->src ) + sizeof( tuple->dst.u3 ) ) / sizeof(u32); return jhash2((u32 *)tuple, n, seed ^ (((__force __u16)tuple->dst.u.all &lt;&lt; 16) | tuple->dst.protonum)); } 因此当需要查找数据包是属于哪个连接，可以通过计算hash值去hash表寻找是否存有。
常用Cmd 查看nf_conntrack表当前连接数 cat /proc/sys/net/netfilter/nf_conntrack_count 查看nf_conntrack表最大连接数 cat /proc/sys/net/netfilter/nf_conntrack_max 通过dmesg可以查看nf_conntrack的状况： //小心nf_conntranck : table full, dropping packet dmesg |grep nf_conntrack 查看存储conntrack条目的哈希表大小,此为只读文件 cat /proc/sys/net/netfilter/nf_conntrack_buckets 查看nf_conntrack的TCP连接记录时间 cat /proc/sys/net/netfilter/nf_conntrack_tcp_timeout_established 通过内核参数查看命令，查看所有参数配置 sysctl -a | grep nf_conntrac Iptables使用 iptables -A INPUT -p tcp -m state --state (NEW,ESTABLISHED,RELATED,INVALID) -m tcp --dport 80 -j ACCEPT 相关参数 详见：https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt
nf_conntrack_max：连接跟踪表的大小， 根据wiki上的计算公式 CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (x / 32)，并满足nf_conntrack_max=4*nf_conntrack_buckets，默认262144 nf_conntrack_buckets：哈希表的大小，(nf_conntrack_max/nf_conntrack_buckets就是每条哈希记录链表的长度)，默认65536 nf_conntrack_tcp_timeout_established：tcp会话的超时时间，默认是432000 (5天) Tool 文档：http://conntrack-tools.netfilter.org/manual.html
yum install conntrack-tools Reference https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html#STATEMACHINE https://wiki.khnet.info/index.php/Conntrack_tuning https://wiki.aalto.fi/download/attachments/69901948/netfilter-paper.pdf</content></entry><entry><title>Linux Route Flags and Types</title><url>/post/linux-route-flags-and-types/</url><categories><category>TCP/IP、Linux</category></categories><tags><tag>TCP/IP、Linux</tag></tags><content type="html"> Route Flags U（up）：此路由处于活动（up）中 G（Gateway）：指向网关 H（Host）：指向一个主机 R（Reinstate）：由动态路由重新初始化的路由 D（Dynamically）：dynamically installed by daemon or redirect M（Modified）：modified from routing daemon or redirect ！：拒绝路由 Route Types unicast：单播路由
broadcast：目的路由是广播地址
local：目的地已分配给此主机。数据包被环回并在本地传递
nat：一条特殊的NAT路由。 前缀所覆盖的目的地被认为是虚拟（或外部）地址，在转发之前需要将其转换为真实（或内部）地址。 使用属性via选择要转换为的地址
ip route add nat 193.7.255.184 via 172.16.82.184 unreachable：目的路由无法到达。丢弃数据包并生成ICMP消息主机不可访问，本地发件人收到一个EHostUnreach 错误。
ip route add unreachable 172.16.82.184 prohibit： 目的路由无法到达。数据包将被丢弃，并生成管理上禁止的ICMP消息通信。 本地发件人收到EACCES错误
ip route add prohibit 10.21.82.157 blackhole：路由黑洞（目的路由无法到达。数据包被悄悄丢弃），不发送ICMP也不转发数据包
ip route add blackhole 202.143.170.0/24 throw：与策略规则一起使用的特殊控制路径。 如果选择了这样的路由，则会在未找到路由的情况下终止此表中的查找。 如果没有策略路由，则等同于路由表中没有路由。 数据包被丢弃，并生成ICMP消息net unreachable。 本地发件人收到EHostUnreach错误
ip route add throw 10.79.0.0/16</content></entry><entry><title>STP/RSTP</title><url>/post/stp/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> STP/RSTP ​ 以太网交换网络中为了进行链路备份，提高网络可靠性，通常会使用冗余链路，但是这也带来了网络环路的问题。网络环路会引发广播风暴和MAC地址表震荡等问题，导致用户通信质量差，甚至通信中断。
​ 为了解决交换网络中的环路问题，IEEE提出了基于802.1D标准的生成树协议STP（Spanning Tree Protocol）。STP是局域网中的破环协议，运行该协议的设备通过彼此交互信息来发现网络中的环路，并有选择地对某些端口进行阻塞，最终将环形网络结构修剪成无环路的树形网络结构，达到破除环路的目的。另外，如果当前活动的路径发生故障，STP还可以激活冗余备份链路，恢复网络连通性。
​ 而随着局域网规模的不断增长，STP拓扑收敛速度慢的问题逐渐凸显，因此，IEEE在2001年发布了802.1w标准，定义了快速生成树协议RSTP（Rapid Spanning Tree Protocol），RSTP在STP的基础上进行了改进，可实现网络拓扑的快速收敛。
STP的基本原理 ​ STP的基本原理是，通过在交换机之间传递一种特殊的协议报文，网桥协议数据单元（Bridge Protocol Data Unit，简称BPDU），来确定网络的拓扑结构。BPDU有两种，配置BPDU（Configuration BPDU）和TCN BPDU。前者是用于计算无环的生成树的，后者则是用于在二层网络拓扑发生变化时产生用来缩短MAC表项的刷新时间的（由默认的300s缩短为15s）
​ Spanning Tree Protocol（STP）的基本思想就是按照"树"的结构构造网络的拓扑结构 ，树的根是一个称为根桥的桥设备，根桥的确立是由交换机或网桥的BID（Bridge ID）确定的，BID最小的设备成为二层网络中的根桥。BID又是由网桥优先级和MAC地址构成，不同厂商的设备的网桥优先级的字节个数可能不同。由根桥开始，逐级形成一棵树，根桥定时发送配置BPDU，非根桥接收配置BPDU，刷新最佳BPDU并转发。这里的最佳BPDU指的是当前根桥所发送的BPDU。如果接收到了下级BPDU（新接入的设备会发送BPDU，但该设备的BID比当前根桥大），接收到该下级BPDU的设备将会向新接入的设备发送自己存储的最佳BPDU，以告知其当前网络中根桥；如果接收到的BPDU更优，将会重新计算生成树 拓扑。当非根桥在离上一次接收到最佳BPDU最长寿命（Max Age，默认20s）后还没有接收到最佳BPDU的时候，该端口将进入监听状态，该设备将产生TCN BPDU，并从根端口转发出去，从指定端口接收到TCN BPDU的上级设备将发送确认，然后再向上级设备发送TCN BPDU，此过程持续到根桥为止，然后根桥在其后发送的配置BPDU中将携带标记表明拓扑已发生变化，网络中的所有设备接收到后将MAC表项的刷新时间从300s缩短为15s。整个收敛的时间为50s左右。
STP拓扑 根桥（Root Bridge） STP协议破环的关键在于生成一个树形的网络结构，而树形的网络结构必须有树根，于是STP引入了根桥的概念。对于一个STP网络，根桥就是网桥ID最小的桥，在全网中只有一个，它是整个网络的逻辑中心，但不一定是物理中心。根桥会根据网络拓扑的变化而动态变化。
根端口（Root Port） 根端口就是去往根桥路径开销最小的端口，根端口负责向根桥方向转发数据，这个端口的选择标准是依据根路径开销判定。很显然，在一个运行STP协议的设备上根端口有且只有一个，根桥上没有根端口。
指定桥（Designated Bridge） 指定端口（Designated Port）：转发数据和BPDU 非指定端口：接收BPDU
BPDU报文 配置BPDU（Configuration BPDU）：用来进行生成树计算和维护生成树拓扑的报文。
报文字段
Root Identifier、Root Path Cost、Bridge Identifier、Port Identifier字段是配置BPDU报文的核心内容：
Root Identifier：当前根桥的BID。 Root Path Cost：根路径开销。路径开销（Path Cost）是一个端口变量，是STP协议用于选择链路的参考值。STP协议通过计算路径开销，选择较为“强壮”的链路，阻塞多余的链路，将网络修剪成无环路的树形网络结构。根路径开销就是某端口到根桥所经过的各个桥上的各端口路径开销的累加值。 Bridge Identifier：本交换设备的BID。 Port Identifier：发送该BPDU的端口ID。PID由两部分构成的，高4位是端口优先级，低12位是端口号。 这四个字段构成了消息优先级向量，一般用{ 根桥ID，根路径开销，发送设备BID，发送端口PID }形式表示。当一个网桥收到配置BPDU报文时，只有当发送者的BID或端口的PID两个字段中至少有一个和本桥接收端口不同，BPDU报文才会被处理，否则丢弃。这样避免了处理和本端口信息一致的BPDU报文。
TCN BPDU（Topology Change Notification BPDU）：其TCN BPDU的结构与配置BPDU基本相同，但Payload部分只有Protocol Identifier（协议号）、Protocol Version Identifier（协议版本）和BPDU Type（BPDU类型）。类型字段是固定值0x80，长度只有4个字节。
TCN BPDU是指在下游拓扑发生变化时向上游发送拓扑变化通知，直到根节点。TCN BPDU在如下两种情况下会产生：
端口状态变为Forwarding状态。 指定端口收到TCN BPDU，复制TCN BPDU并发往根桥。 STP运行过程 生成树协议运行生成树算法（STP）。生成树算法很复杂，但是其过程可以归纳为以下三个部分。
（1）选择根网桥
（2）选择根端口
（3）选择指定端口（也有书籍称为转发端口）
选择根网桥的依据是交换机的网桥优先级，网桥优先级是用来衡量网桥在生成树算法中优先级的十进制数，取值范围是0～65535.默认值是32768，网桥ID=网桥优先级+网桥MAC地址组成的，共有8个字节。由于交换机的网桥优先级都是默认，所以在根网桥的选举中比较的一般是网卡MAC地址的大小，选取MAC地址小的为根网桥。
STP端口状态 **Blocking（阻塞状态）：**此时，二层端口为非指定端口，也不会参与数据帧的转发。该端口通过接收BPDU来判断根交换机的位置和根ID，以及在STP拓扑收敛结束之后，各交换机端口应该处于什么状态，在默认情况下，端口会在这种状态下停留20秒钟时间。 **Listening（侦听状态）：**生成树此时已经根据交换机所接收到的BPDU而判断出了这个端口应该参与数据帧的转发。于是交换机端口就将不再满足于接收BPDU，而同时也开始发送自己的BPDU，并以此通告邻接的交换机该端口会在活动拓扑中参与转发数据帧的工作。在默认情况下，该端口会在这种状态下停留15秒钟的时间。 **Learning(学习状态)：**这个二层端口准备参与数据帧的转发，并开始填写MAC表。在默认情况下，端口会在这种状态下停留15秒钟时间。 **Forwarding（转发状态）：**这个二层端口已经成为了活动拓扑的一个组成部分，它会转发数据帧，并同时收发BPDU。 **Disabled（禁用状态）：**这个二层端口不会参与生成树，也不会转发数据帧。 端口状态迁移机制 端口Up或使能了STP，会从Disabled状态进入到Blocking状态。 端口被选举为根端口或指定端口，会进入Listening状态。 端口的Forward Delay定时器超时，会进入Learning/Forwarding状态。 端口不再是根端口或指定端口时，会进入Blocking状态。 端口Down或者去使能STP时，就进入Disabled状态。 STP根桥、根端口和指定端口的选举原则 在STP网络中，选举根桥、根端口和指定端口主要使用配置BPDU中报文中的消息优先级向量：{ 根桥ID，根路径开销，发送设备BID，发送端口PID }
一旦根桥、根端口、指定端口选举成功，则整个树形拓扑建立完毕。在拓扑稳定后，只有根端口和指定端口转发流量，其他的非根非指定端口都处于阻塞（Blocking）状态，它们只接收STP协议报文而不转发用户流量。
STP定时器 对于STP，影响端口状态和端口收敛有以下3个时间参数：
Hello Time
运行STP协议的设备发送配置BPDU的时间间隔。设备每隔Hello Time时间会向周围的设备发送BPDU报文，以确认链路是否存在故障。
当网络拓扑稳定之后，该计时器的修改只有在根桥修改后才有效。新的根桥会在发出的BPDU报文中填充相应的字段以向其他非根桥传递该计时器修改的信息。但当拓扑变化之后，TCN BPDU的发送不受这个计时器的管理。
Forward Delay
设备状态迁移的延迟时间。链路故障会引发网络重新进行生成树的计算，生成树的结构将发生相应的变化。不过重新计算得到的新配置消息无法立刻传遍整个网络，如果新选出的根端口和指定端口立刻就开始数据转发的话，可能会造成临时环路。为此，STP采用了一种状态迁移机制，新选出的根端口和指定端口要经过2倍的Forward Delay延时后才能进入转发状态，这个延时保证了新的配置消息传遍整个网络，从而防止了临时环路的产生。
Forward Delay Timer指一个端口处于Listening和Learning状态的各自持续时间，默认是15秒。即Listening状态持续15秒，随后Learning状态再持续15秒。这两个状态下的端口均不转发用户流量，这正是STP用于避免临时环路的关键。
Max Age
端口的BPDU报文老化时间，可在根桥上通过命令人为改动老化时间。
RSTP</content></entry><entry><title>RIB and FIB</title><url>/post/what-is-the-rib-and-fib/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> RIB RIB（ Routing Information Base）是由节点上的各种路由过程建立的，其维护每种协议 ( 包含来自路由协议，如OSPF、is-is、BGP、静态条目 ) 的网络拓扑和路由表，包括许多到达相同目的地前缀的路由
FIB FIB （forwarding information base) 是由RIB中的所以路由里选择最佳路由，将它放进FIB中再对数据包按路由进行转发
//原文 FIBs are the best route from the possibly many protocols in the RIBs pushed down to fast forwarding lookup memory for the best path(s). Gobgpd、zebra(Quagga) and linux kernel routed Reference RIBs and FIBs (aka IP Routing Table and CEF Table) Juniper and Cisco Comparisons of RIB, LIB, FIB and LFIB Tables Router table and forwarding table [closed] https://www.slideshare.net/shusugimoto1986/tutorial-using-gobgp-as-an-ixp-connecting-router gobgp configure zebra</content></entry><entry><title>iptables 四表五链</title><url>/post/iptables-%E5%9B%9B%E8%A1%A8%E4%BA%94%E9%93%BE/</url><categories><category>network</category></categories><tags><tag>network</tag></tags><content type="html"> 前言 iptables只是一个工具，实际上内部核心是基于Netfilter 框架去实现其功能的一个命令行工具。
四表和五链 四表 首先表这概念主要是把相同功能的规则集中在一起。而不是每条规则都散落和重复的出现在每个不同的链中。
raw表 raw表主要用来决定是否跟踪连接和配置notrack目标。优先级最高。用法：可以配置该表来决定哪一条连接可以被跟踪。其提供两条调用链来供使用：prerouting、output。 例子：
//对来自于ip为192.168.10.117的所有数据包不进行追踪 iptables -t raw -A PREROUTING -s 192.168.10.117 -j NOTRACK //对来自于ip为192.168.10.117的所有数据包进行追踪 iptables -t raw -A PREROUTING -s 192.168.10.117 -j TRACK
mangle表 该表主要用于数据包的修改，例如：TTL，IP数据包首部，给数据包打上标签(&ndash;set-mask)。其提供五条调用链来供使用：prerouting、foward、postrouting、input、output。 参考：what-is-the-mangle-table-in-iptables nat表 该表主要用于网络地址转换。其提供4条调用链来供使用：prerouting、postrouting、input、output。
filter表 该表是用来过滤网络上的数据包，决定数据包是否可以进入上层协议栈进行处理。其提供3条调用链来供使用：input、output、foward。
五个规则链 链可以看出一道道的“关卡”
prerouting链 路由前链，处理刚到达本机的数据包，进行路由决策。可以使用的表：raw、mangle、nat。
foward链 转发链，当prerouting链判断目的地不是本机时，会经过该链并执行规则。可以使用的表：mangle、filter。
postrouting链 数据包经过处理后，在网卡前，再对其进行规则匹配和修改。可以使用的表：mangle、nat。
input链 经prerouting链进行路由决策之后，进入本机所要匹配的规则和修改。可以使用的表：mangle、nat、filter。
output链 经过本机处理好的数据并需要向外发送，经该关卡所要匹配的规则和修改。可以使用的表：raw、mangle、nat、filter。
思维图 第五张表&mdash;security表 该表是用来强制性访问控制(Mandatory Access Control,MAC)，通过设置内核中SElinux来实现。其提供2条调用链来供使用：input、output。不过我记得在某些场景下，这个SElinux通常需要给关闭。因此该表可能在平常用的不多。 工作流程（重） extend module connlimit module
限制每个IP地址同时链接到server端的数量
iptables -t filter -I INPUT -p tcp --dprot 22 -m connlimit --connlimit-above 2 -j REJECT iptables -t filter -I INPUT -p tcp --dprot 22 -m connlimit ! --connlimit-above 2 -j REJECT limit module
限速
iptables -t filter -I INPUT -p icmp -m limit --limit 10/minite -j ACCEPT
iprange module
指定一段连续的IP地址范围，用于匹配package src or des
iptables -t filter -I INPUT -m iprange --src-range 192.168.1.127-192.168.1.146 -j DROP
cmd and example forward packages
查看server 是否开启转发
​ sysctl net.ipv4.ip_forward
​ 已开启：net.ipv4.ip_forward = 1
加入规则：iptables -t nat -A PREROUTING -p tcp -m tcp --dport 8080 -j DNAT --to-destination [destination IP]
parameter -t：select iptables tables -I：select iptables lists -o：use to select which network adapter to out -p：use to match the type of package -m：use tcp extend module -s：match src ip -sport -dport：src and des ip match Reference iptables-Aarchlinux netfilter/iptables: why not using the raw table? iptables(8) - Linux man page A Deep Dive into Iptables and Netfilter Architecture Netfilter</content></entry><entry><title>Disjoint_set</title><url>/post/disjoint-set/</url><categories><category>LeetCode</category></categories><tags><tag>LeetCode</tag></tags><content type="html"> template type unionFind struct { parent, rank []int } func newunionFind(n int) *unionFind { parent := make([]int, n) rank := make([]int, n) for i := range parent { parent[i] = i rank[i] = 1 } return &amp;unionFind{parent, rank} } func (uf *unionFind) find(x int) int { if uf.parent[x] != x { uf.parent[x] = uf.find(uf.parent[x]) } return uf.parent[x] } func (uf *unionFind) union(x, y int) bool { fx, fy := uf.find(x), uf.find(y) if fx == fy { return false } if uf.rank[fx] &lt; uf.rank[fy] { fx, fy = fy, fx } uf.parent[fy] = fx uf.rank[fx] += uf.rank[fy] return true } Reference 维基百科 OI Wiki - 数据结构 - 并查集</content></entry><entry><title>MPLS</title><url>/post/mpls/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> 基本概念 转发等价类 ​ MPLS作为一种分类转发技术，将具有相同转发处理方式的分组归为一类，称为FEC（Forwarding Equivalence Class，转发等价类）。相同FEC的分组在MPLS网络中将获得完全相同的处理。
​ FEC的划分方式非常灵活，可以是以源地址、目的地址、源端口、目的端口、协议类型或VPN等为划分依据的任意组合。例如，在传统的采用最长匹配算法的IP转发中，到同一个目的地址的所有报文就是一个FEC。
标签 标签是一个长度固定，仅具有本地意义的短标识符，长度为4个字节，用于唯一标识一个分组所属的FEC。一个标签只能代表一个FEC。
标签共有4个域：
Label：标签值字段，长度为20bits，用来标识一个FEC。 Exp：3bits，保留，协议中没有明确规定，通常用作CoS。 S：1bit，MPLS支持多重标签。值为1时表示为最底层标签。 TTL：8bits，和IP分组中的TTL意义相同，可以用来防止环路。 标签交换路由器 LSR（Label Switching Router，标签交换路由器）是MPLS网络中的基本元素，所有LSR都支持MPLS技术
标签交换路径 R1------->R2------->R3 一个转发等价类在MPLS网络中经过的路径称为LSP（Label Switched Path，标签交换路径）。在一条LSP上，沿数据传送的方向，相邻的LSR分别称为上游LSR和下游LSR。如上述中，R2为R1的下游LSR，相应的，R1为R2的上游LSR。
标签分发协议 LDP（Label Distribution Protocol，标签分发协议）是MPLS的控制协议，它相当于传统网络中的信令协议，负责FEC的分类、标签的分配以及LSP的建立和维护等一系列操作。
MPLS可以使用多种标签发布协议，包括专为标签发布而制定的协议，例如：LDP、CR-LDP（Constraint-Based Routing using LDP，基于约束路由的LDP）；也包括现有协议扩展后支持标签发布的，例如：BGP（Border Gateway Protocol，边界网关协议）、RSVP（Resource Reservation Protocol，资源预留协议）。同时，还可以手工配置静态LSP。
LSP隧道技术 一条LSP的上游LSR和下游LSR，尽管它们之间的路径可能并不在路由协议所提供的路径上，但是MPLS允许在它们之间建立一条新的LSP，这样，上游LSR和下游LSR分别就是这条LSP的起点和终点。这时，上游LSR和下游LSR间的LSP就是LSP隧道，它避免了采用传统的网络层封装隧道。
如果隧道经由的路由与逐跳从路由协议中取得的路由一致，这种隧道就称为逐跳路由隧道（Hop-by-Hop Routed Tunnel）；否则称为显式路由隧道（Explicitly Routed Tunnel）。
标签栈 如果分组在超过一层的LSP隧道中传送，就会有多层标签，形成标签栈（Label Stack）。在每一隧道的入口和出口处，进行标签的入栈（PUSH）和出栈（POP）操作。
标签栈按照“后进先出”（Last-In-First-Out）方式组织标签，MPLS从栈顶开始处理标签。
MPLS对标签栈的深度没有限制。若一个分组的标签栈深度为m，则位于栈底的标签为1级标签，位于栈顶的标签为m级标签。未压入标签的分组可看作标签栈为空（即标签栈深度为零）的分组
结构 网络结构 MPLS网络的基本构成单元是LSR，由LSR构成的网络称为MPLS域。
位于MPLS域边缘、连接其它用户网络的LSR称为LER（Label Edge Router，边缘LSR），区域内部的LSR称为核心LSR。核心LSR可以是支持MPLS的路由器，也可以是由ATM交换机等升级而成的ATM-LSR。域内部的LSR之间使用MPLS通信，MPLS域的边缘由LER与传统IP技术进行适配。
分组在入口LER被压入标签后，沿着由一系列LSR构成的LSP传送，其中，入口LER被称为Ingress，出口LER被称为Egress，中间的节点则称为Transit。
节点结构 MPLS节点由两部分组成：
控制平面（Control Plane）：负责标签的分配、路由的选择、标签转发表的建立、标签交换路径的建立、拆除等工作； 转发平面（Forwarding Plane）：依据标签转发表对收到的分组进行转发。 对于普通的LSR，在转发平面只需要进行标签分组的转发，需要使用到LFIB（Label Forwarding Information Base，标签转发表）。对于LER，在转发平面不仅需要进行标签分组的转发，也需要进行IP分组的转发，所以既会使用到LFIB，也会使用到FIB（Forwarding Information Base，转发信息表）</content></entry><entry><title>计算机网络</title><url>/post/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url><categories><category>计算机网络</category></categories><tags><tag>计算机网络</tag></tags><content type="html"> 1.互联网组成 边缘部分：所有连接在互联网上的主机（主机指的是所有与网络直接相连的计算机）组成，用户可以直接使用，用来进行主机之间的通信和资源共享。
通信的方式主要有两种：
客户-服务器方式：即C /S方式。客户端发送服务的请求，服务器是服务的提供方。 P2P：对等连接方式。两台通信的主机之间不区分哪个是客户，哪个是服务端，只要两台主机都运行了对等连接软件就可以进行平等、对等的连接通信。 核心部分：大量的网络与连接这些网络所使用的路由器构成，为边缘部分提供服务（提供连通性和交换）
​ 核心部分起到特殊作用的是路由器，它是一个专用的计算机（但是不叫主机）。路由器是实现**分组交换（packet switching）**的关键组件，其任务是转发收到的分组（存储转发），这是网络核心部分最重要的功能。
电路交换：使用在电话机之间的通信，使用电话交换机解决了多个电话机之间通信需要大量的电线的问题。电路交换的过程是：建立连接(开始占用通信资源)—通话(一直占用通信资源)&mdash;-释放连接(归还通信资源)。电路交换的特点是：通话期间，通话的两个用户会始终占用通信资源。使用电路交换传输计算机数据时，传输效率往往会很低。因为计算机数据具有突变式的特点，线路上真正用来传输数据的时间往往不到10%,大部分通信线路资源绝大部分时间都被浪费了。整个报文的比特流连续的从源点直达终点
分组交换：采用存储转发的技术，把一个报文（需要发生出去的整块数据）划分成几组分组后再进行传输。将报文划分成更小的等长数据段，然后加上首部(包含一些控制信息)，构成了一个分组，分组的首部称为一个包头。单个分组（只是整个报文的一部分）传送到相邻结点，存储下来后查找转发表，转发到下一个结点。
报文交换：整个报文先传送到相邻结点，全部存储下来后查找转发表，转发到下一个结点。
2.计算机网络的常见硬件设备介绍： 物理层：实现网络互连的主要设备有中继器和HUB(集线器)。中继器的主要功能是对接收到的信号进行再生整形放大以扩大网络的传输距离；集线器在此基础上将所有的节点集中在以它为中心的节点中，可组成星型拓扑结构。
数据链路层：实现网络互联的主要设备有二层交换机和网桥。交换机是一种基于MAC识别，能完成封装转发数据包功能的网络设备。它可以“学习”MAC地址，并把其存放在内部地址表中，当一个数据帧的目的地址在MAC地址表中有映射时，它被转发到连接目的节点的端口而不是所有端口。 交换机将局域网分为多个冲突域，每个冲突域都是有独立的宽带，因此大大提高了局域网的带宽。网桥是数据链路层互联的设备，在网络互联中可起到数据接收、地址过滤与数据转发的作用，可用来实现多个不同网络系统之间的数据交换。
网络层：实现网络互连的主要设备有三层交换机和路由器。路由器用于连接多个逻辑上分开的网络，具有判断网络地址和选择IP路径的功能，它能在多网络互联环境中，建立灵活的连接，可用完全不同的数据分组和介质访问方法连接各种子网。
传输层（包括传输层）以上：实现网络互连的设备有网关。网关在网络层以上实现网络互连，用于两个高层协议不同的网络互连。与网桥只是简单地传达信息不同，网关对收到的信息要重新打包，以适应目的系统的需求。
3.计算机网络体系结构 3.1. 物理层（Physical Layer） OSI模型的最低层或第一层，规定了激活、维持、关闭通信端点之间的机械特性、电气特性、功能特性以及过程特性，为上层协议提供了一个传输数据的物理媒体。 在这一层，协议数据单元为 比特（bit）。 在物理层的互联设备包括：**集线器（Hub）、中继器（Repeater）**等。 3.2. 数据链路层（Datalink Layer） OSI模型的第二层，它控制网络层与物理层之间的通信，其主要功能是在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。
在这一层，协议数据单元为 帧（frame）。
在数据链路层的互联设备包括：**网桥（Bridge）、交换机（Switch）**等。
基本单位：比特流0和1
特性：
机械特性：指明接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置，等。平时常见的各种规格的接插件都有严格的标准化的规定。 电气特性：在接口电缆的各条线上出现的电压的范围。 功能特性：指明某条线上出现的某一电平的电压的意义。 过程特性：指明对于不同功能的各种可能事件的出现顺序。 调制
信道
概念：一般都是用来表示某一个方向传送信息的媒体。
信道的极限容量
传输媒体
通道（信道）：
单向通道（单工通道）：只有一个方向通信，没有反方向交互，如广播。 双向交替通信（半双工通信）：通信双方都可发消息，但不能同时发送或接收。 双向同时通信（全双工通信）：通信双方可以同时发送和接收信息 通道（信道）复用技术：
频分复用（FDM，Frequency Division Multiplexing）：不同用户在不同频带，所用用户在同样时间占用不同带宽资源 时分复用（TDM，Time Division Multiplexing）：不同用户在同一时间段的不同时间片，所有用户在不同时间占用同样的频带宽度 波分复用（WDM，Wavelength Division Multiplexing）：光的频分复用 码分复用（CDM，Code Division Multiplexing）：不同用户使用不同的码，可以在同样时间使用同样频带通信 3.3. 网络层（Network Layer） OSI模型的第三层，其主要功能是将网络地址翻译成对应的物理地 ，并决定如何将数据从发送方路由到接收方。该层的作用包括：对子网间的数据包进行路由选择，实现拥塞控制、网际互连等功能。 在这一层，协议数据单元为 数据包（packet）。 在网络层的互联设备包括：**路由器（Router）**等。 3.4. 传输层（Transport Layer） OSI模型中最重要的一层，是第一个端到端，即主机到主机的层次。其主要功能是负责将上层数据分段并提供端到端的、可靠的或不可靠的传输。此外，传输层还要处理端到端的差错控制和流量控制问题。 在这一层，协议数据单元为 数据段（segment）。 传输层协议的代表包括：TCP、UDP、SPX等。 3.4.1 TCP（Transmission Control Protocol，传输控制协议） ​ TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议
TCP报文段的首部格式 ​ TCP虽然是面向字节流的，但TCP传送的数据单元却是报文段。一个TCP报文段分为首部和数据两部分，而TCP 的全部功能都体现在它首部中各字段的作用。因此，只有弄清TCP首部各字段的作用才能掌握TCP的工作原理。下面讨论TCP报文段的首部格式。
​ TCP报文段首部的前20个字节是固定的，后面有4n字节是根据需要而增加的选项（n是整数)。因此TCP首部的最小长度是20字节。
源端口：范围 0~65525
目的端口：范围 0~65525
序号（sequence number）：TCP 连接中传送的数据流中的每一个字节都编上一个序号。序号字段的值则指的是本报文段所发送的数据的第一个字节的序号。
确认号（acknoledgement number）：期望收到对方的下一个报文段的数据的第一个字节的序号。
数据偏移：TCP报文段的数据起始处距离TCP报文段的起始处有多远。这个字段实际上是指出TCP报文段的首部长度。由于首部中还有长度不确定的选项字段，因此数据偏移字段是必要的。但应注意，“数据偏移”的单位是32位字（即以4字节长的字为计算单位)。由于4位二进制数能够表示的最大十进制数字是15，因此数据偏移的最大值是60字节，这也是TCP首部的最大长度（即选项长度不能超过40字节)。
保留
6个控制位
URG（URGent）：当URG =1时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送（相当于高优先级的数据)，而不要按原来的排队顺序来传送。例如，已经发送了很长的一个程序要在远地的主机上运行。但后来发现了一些问题，需要取消该程序的运行。因此用户从键盘发出中断命令(Control + C)。如果不使用紧急数据，那么这两个字符将存储在接收TCP的缓存末尾。只有在所有的数据被处理完毕后这两个字符才被交付接收方的应用进程。这样做就浪费了许多时间。
​ 当URG置1时，发送应用进程就告诉发送方的 TCP有紧急数据要传送。于是发送方TCP就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。这时要与首部中紧急指针(Urgent Pointer)字段配合使用。
​ 然而在紧急指针字段的具体实现上，由于过去的有些文档有错误或有不太明确的地方，因而导致对有关的RFC文档产生了不同的理解。于是，在2011年公布的建议标准 RFC6093，把紧急指针字段的使用方法做出了更加明确的解释，并更新了几个重要的RFC 文档，如 RFC793,RFC 1011,RFC 1122等。
ACK（ACKnowledgment）：仅当ACK =1时确认号字段才有效。当ACK = 0时，确认号无效。TCP规定，在连接建立后所有传送的报文段都必须把ACK置1。
PSH（PuSH）：当两个应用进程进行交互式的通信时，有时在一端的应用进程希望在键入一个命令后立即就能够收到对方的响应。在这种情况下，TCP 就可以使用推送(push)操作。这时，发送方TCP把 PSH 置1，并立即创建一个报文段发送出去。接收方TCP收到PSH =1的报文段，就尽快地（即“推送”向前）交付接收应用进程，而不再等到整个缓存都填满了后再向上交付。
RST（ReSeT）：当RST = 1时，表明TCP 连接中出现严重差错（如由于主机崩溃或其他原因)，必须释放连接，然后再重新建立运输连接。RST置1还用来拒绝一个非法的报文段或拒绝打开一个连接。RST 也可称为重建位或重置位。
SYN（SYNchronization）：在连接建立时用来同步序号。当SYN=1而ACK=0 时，表明这是一个连接请求报文段。对方若同意建立连接，则应在响应的报文段中使SYN= 1和ACK = 1。因此，SYN置为1就表示这是一个连接请求或连接接受报文。
FIN（FINis）：用来释放一个连接。当FIN=1时，表明此报文段的发送方的数据已发送完毕，并要求释放运输连接。
窗口大小：窗口值是[0,216- 1]之间的整数。窗口指的是发送本报文段的一方的接收窗口（而不是自己的发送窗口)。窗口值告诉对方:从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量（以字节为单位)。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。总之，窗口值作为接收方让发送方设置其发送窗口的依据。
​ 例如，发送了一个报文段，其确认号是701，窗口字段是1000。这就是告诉对方:“从701号算起，我（即发送此报文段的一方）的接收缓存空间还可接收1000个字节数据（字节序号是701~1700)，你在给我发送数据时，必须考虑到这一点。”
​ 总之，应当记住: 窗口字段明确指出了现在允许对方发送的数据量。窗口值经常在动态变化着。
检验和
紧急指针：紧急指针仅在URG = 1时才有意义，它指出本报文段中的紧急数据的字节数（紧急数据结束后就是普通数据)。因此，紧急指针指出了紧急数据的末尾在报文段中的位置。当所有紧急数据都处理完时，TCP 就告诉应用程序恢复到正常操作。值得注意的是，即使窗口为零时也可发送紧急数据。
选项：长度可变，最长可达40字节。当没有使用“选项“时，TCP的首部长度是20字节。
TCP可靠传输的实现 TCP超时重传机制 TCP的流量控制 ​ 一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制(flow control)就是让发送方的发送速率不要太快，要让接收方来得及接收。
​ 利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制下面通过图5-22的例子说明如何利用滑动窗口机制进行流量控制。
​ 设A向B发送数据。在连接建立时，B告诉了A:“我的接收窗口rwnd= 400”(这里rwnd表示receiver window)。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。TCP连接建立时的窗口协商过程在图中没有显示出来。再设每一个报文段为100字节长，而数据报文段序号的初始值设为1(见图中第一个箭头上面的序号seq = 1。图中右边的注释可帮助理解整个过程)。请注意，图中箭头上面大写ACK表示首部中的确认位ACK，小写ack表示确认字段的值。
​ 我们应注意到，接收方的主机B进行了三次流量控制。第一次把窗口减小到rwnd = 300，第二次又减到rwnd = 100，最后减到rwnd = o，即不允许发送方再发送数据了。这种使发送方暂停发送的状态将持续到主机B重新发出一个新的窗口值为止。我们还应注意到，B向A发送的三个报文段都设置了ACK=1，只有在ACK= 1时确认号字段才有意义。
​ 现在我们考虑一种情况。在图5-22中，B向A发送了零窗口的报文段后不久，B的接收缓存又有了一些存储空间。于是B向A发送了rwnd = 400的报文段。然而这个报文段在传送过程中丢失了。A一直等待收到B发送的非零窗口的通知，而B也一直等待A发送的数据。如果没有其他措施，这种互相等待的死锁局面将一直延续下去。
​ 为了解决这个问题，TCP为每一个连接设有一个 **持续计时器(persistence timer) **。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口探测报文段（仅携带Ⅰ字节的数据)R，而对方就在确认这个探测报文段时给出了现在的窗口值。如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口不是零，那么死锁的僵局就可以打破了。
糊涂窗口综合症 发送端引起的糊涂窗口综合症
​ 如果发送端为产生数据很慢的应用程序服务(典型的有telnet应用)，例如，一次产生一个字节。这个应用程序一次将一个字节的数据写入发送端的TCP的缓存。如果发送端的TCP没有特定的指令，它就产生只包括一个字节数据的报文段。结果有很多41字节的IP数据报就在互连网中传来传去。
​ 解决的方法是防止发送端的TCP逐个字节地发送数据。必须强迫发送端的TCP收集数据，然后用一个更大的数据块来发送。发送端的TCP要等待多长时间呢？如果它等待过长，它就会使整个的过程产生较长的时延。如果它的等待时间不够长，它就可能发送较小的报文段。Nagle找到了一个很好的解决方法，发明了Nagle算法 。
接收端引起的糊涂窗口综合症
​ 接收端的TCP可能产生糊涂窗口综合症，如果它为消耗数据很慢的应用程序服务，例如，一次消耗一个字节。假定发送应用程序产生了1000字节的数据块，但接收应用程序每次只吸收1字节的数据。再假定接收端的TCP的输入缓存为4000字节。发送端先发送第一个4000字节的数据。接收端将它存储在其缓存中。缓存满了。它通知窗口大小为零，这表示发送端必须停止发送数据。接收应用程序从接收端的TCP的输入缓存中读取第一个字节的数据。在入缓存中有了1字节的空间。接收端的TCP宣布其窗口大小为1字节，这表示正渴望等待发送数据的发送端的TCP会把这个宣布当作一个好消息，并发送只包括一个字节数据的报文段。这样的过程一直继续下去。一个字节的数据被消耗掉，然后发送只包含一个字节数据的报文段。
对于这种糊涂窗口综合症，即应用程序消耗数据比到达的慢，有两种建议的解决方法。
Clark解决方法
​ Clark解决方法是只要有数据到达就发送确认，但宣布的窗口大小为零，直到或者缓存空间已能放入具有最大长度的报文段，或者缓存空间的一半已经空了。
延迟确认
​ 这表示当一个报文段到达时并不立即发送确认。接收端在确认收到的报文段之前一直等待，直到入缓存有足够的空间为止。延迟的确认防止了发送端的TCP滑动其窗口。当发送端的TCP发送完其数据后，它就停下来了。这样就防止了这种症状。迟延的确认还有另一个优点：它减少了通信量。接收端不需要确认每一个报文段。但它也有一个缺点，就是迟延的确认有可能迫使发送端重传其未被确认的报文段。可以用协议来平衡这个优点和缺点，例如定义了确认的延迟不能超过500毫秒。
TCP的拥塞控制 ​ TCP的拥塞控制的算法有四种，即慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。
慢开始和拥塞避免 ​ 下面讨论的拥塞控制也叫做基于窗口的拥塞控制。为此，发送方维持-一个叫做拥塞窗cwnd (congestion window)的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口。
​ 发送方控制拥塞窗口的原则是:只要网络没有出现拥塞，拥塞窗口就可以再增大一些，以便把更多的分组发送出去，这样就可以提高网络的利用率。但只要网络出现拥塞或有可能出现拥塞，就必须把拥塞窗口减小一些，以减少注入到网络中的分组数，以便缓解网络出现的拥塞。
​ 发送方又是如何知道网络发生了拥塞呢﹖我们知道，当网络发生拥塞时，路由器就要丢弃分组。因此只要发送方没有按时收到应当到达的确认报文，也就是说，只要出现了超时，就可以猜想网络可能出现了拥塞。现在通信线路的传输质量一般都很好，因传输出差错而丢弃分组的概率是很小的（远小于1%)。因此，判断网络拥塞的依据就是出现了超时。
​ 发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞。发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。
慢开始
当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。
​ 在一开始发送方先设置cwnd= 1，发送第一个报文段M1，接收方收到后确认M1。发送方收到对M1的确认后，把 cwnd 从1增大到2，于是发送方接着发送 M2和M3两个报文段。接收方收到后发回对M2和 M3的确认。发送方每收到一个对新报文段的确认（重传的不算在内）就使发送方的拥塞窗口加1，因此发送方在收到两个确认后，cwnd就从2增大到4，并可发送M4~M7共4个报文段（见图5-24)。因此使用慢开始算法后，每经过一个传输轮次(transmission round)，拥塞窗口cwnd就加倍。
​ 每经过一个传输轮次，拥塞窗口cwnd 就加倍。一个传输轮次所经历的时间其实就是往返时间RTT。不过传输轮次更加强调：把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。
​ 慢开始的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd= 1，使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况)，然后再逐渐增大cwnd。这当然比设置大的cwnd值一下子把许多报文段注入到网络中要“慢得多”。这对防止网络出现拥塞是一个非常好的方法。
​ 顺便指出，图5-24只是为了说明慢开始的原理。在TCP 的实际运行中，发送方只要收到一个对新报文段的确认，其拥塞窗口cwnd就立即加1，并可以立即发送新的报文段，而不需要等这个轮次中所有的确认都收到后（如图5-24所示的那样）再发送新的报文段。
​ 为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量（如何设置ssthresh，后面还要讲)。慢开始门限ssthresh 的用法如下:
当cwnd &lt; ssthresh 时，使用上述的慢开始算法。 当cwnd > ssthresh时，停止使用慢开始算法而改用拥塞避免算法。 当cwnd = ssthresh时，既可使用慢开始算法，也可使用拥塞避免算法。 拥塞避免算法
​ 拥塞避免算法的思路是让拥塞窗口cwnd 缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd 加 10，而不是像慢开始阶段那样加倍增长。因此在拥塞避免阶段就有“加法增大”AI (Additive Increase)的特点。这表明在拥塞避免阶段，拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。
​ 上图拥塞控制的过程：
当TCP连接进行初始化时，把拥塞窗口cwnd置为1。为了便于理解，图中的窗口单位不使用字节而使用报文段的个数。慢开始门限的初始值设置为16个报文段，即 cwnd = 16
在执行慢开始算法时，拥塞窗口 cwnd 的初始值为1。以后发送方每收到一个对新报文段的确认ACK，就把拥塞窗口值另1，然后开始下一轮的传输（图中横坐标为传输轮次）。因此拥塞窗口cwnd随着传输轮次按指数规律增长。当拥塞窗口cwnd增长到慢开始门限值ssthresh时（即当cwnd=16时），就改为执行拥塞控制算法，拥塞窗口按线性规律增长
假定拥塞窗口的数值增长到24时，网络出现超时（这很可能就是网络发生拥塞了）。更新后的ssthresh值变为12（即变为出现超时时的拥塞窗口数值24的一半），拥塞窗口再重新设置为1，并执行慢开始算法。当cwnd=ssthresh=12时改为执行拥塞避免算法，拥塞窗口按线性规律增长，每经过一个往返时间增加一个MSS的大小
当拥塞窗口 cwnd = 16时（图中的点4)，出现了一个新的情况，就是发送方一连收到3个对同一个报文段的重复确认（图中记为3-ACK)。关于这个问题要解释如下：
​ 有时，个别报文段会在网络中丢失，但实际上网络并未发生拥塞。如果发送方迟迟收不到确认，就会产生超时，就会误认为网络发生了拥塞。这就导致发送方错误地启动慢开始，把拥塞窗口 cwnd 又设置为1，因而降低了传输效率。
快重传和快恢复 ​ 采用快重传算法可以让发送方尽早知道发生了个别报文段的丢失。快重传算法首先要求接收方不要等待自己发送数据时才进行捎带确认，而是要立即发送确认，即使收到了失序的报文段也要立即发出对已收到的报文段的重复确认。如图5-26所示，接收方收到了M1和M2后都分别及时发出了确认。现假定接收方没有收到M3但却收到了M4。本来接收方可以什么都不做。但按照快重传算法，接收方必须立即发送对M2的重复确认，以便让发送方及早知道接收方没有收到报文段M3;。发送方接着发送M5和 M6,。接收方收到后也仍要再次分别发出对M2的重复确认。这样，发送方共收到了接收方的4个对M2的确认，其中后3个都是重复确认。快重传算法规定，发送方只要一连收到3个重复确认，就知道接收方确实没有收到报文段 M3，因而应当立即进行重传（即“快重传”)，这样就不会出现超时，发送方也不就会误认为出现了网络拥塞。使用快重传可以使整个网络的吞吐量提高约20%。
​ 因此，在图5-25中的点4，发送方知道现在只是丢失了个别的报文段。于是不启动慢开始，而是执行快恢复算法。这时，发送方调整门限值ssthresh = cwnd / 2 = 8，同时设置拥塞窗口 cwnd = ssthresh =8（见图5-25中的点5)，并开始执行拥塞避免算法。
TCP的运输连接管理 三次握手（three-way handshake） TCP建立连接的过程叫做握手，握手需要在客户和服务器之间交换三个TCP报文段。（三次握手three-way handshake）
刚开始客户端处于 Closed 的状态，而服务端处于 Listen 状态：
CLOSED ：没有任何连接状态
LISTEN ：侦听来自远方 TCP 端口的连接请求
1）第一次握手：客户端向服务端发送一个 SYN 报文（SYN = 1），并指明客户端的初始化序列号 ISN(x)，即图中的 seq = x，表示本报文段所发送的数据的第一个字节的序号。此时客户端处于 SYN_Send 状态。
SYN-SENT ：在发送连接请求后等待匹配的连接请求
2）第二次握手：服务器收到客户端的 SYN 报文之后，会发送 SYN 报文作为应答（SYN = 1），并且指定自己的初始化序列号 ISN(y)，即图中的 seq = y。同时会把客户端的 ISN + 1 作为确认号 ack 的值，表示已经收到了客户端发来的的 SYN 报文，希望收到的下一个数据的第一个字节的序号是 x + 1，此时服务器处于 SYN_REVD 的状态。
SYN-RECEIVED：在收到和发送一个连接请求后等待对连接请求的确认
3）第三次握手：客户端收到服务器端响应的 SYN 报文之后，会发送一个 ACK 报文，也是一样把服务器的 ISN + 1 作为 ack 的值，表示已经收到了服务端发来的的 SYN 报文，希望收到的下一个数据的第一个字节的序号是 y + 1，并指明此时客户端的序列号 seq = x + 1（初始为 seq = x，所以第二个报文段要 +1），此时客户端处于 Establised 状态。
服务器收到 ACK 报文之后，也处于 Establised 状态，至此，双方建立起了 TCP 连接。
ESTABLISHED （established-əˈstabliSHt）：代表一个打开的连接，数据可以传送给用户
四次挥手 刚开始双方都处于ESTABLISHED 状态，假设是客户端先发起关闭请求。四次挥手的过程如下：
1）第一次挥手：客户端发送一个 FIN 报文（请求连接终止：FIN = 1），报文中会指定一个序列号 seq = u。并停止再发送数据，主动关闭 TCP 连接。此时客户端处于 FIN_WAIT1 状态，等待服务端的确认。
FIN-WAIT-1 - 等待远程TCP的连接中断请求，或先前的连接中断请求的确认；
2）第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。
CLOSE-WAIT - 等待从本地用户发来的连接中断请求；
此时的 TCP 处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待 2）状态，等待服务端发出的连接释放报文段。
FIN-WAIT-2 - 从远程TCP等待连接中断请求；
3）第三次挥手：如果服务端也想断开连接了（没有要向客户端发出的数据），和客户端的第一次挥手一样，发送 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态，等待客户端的确认。
LAST-ACK - 等待原来发向远程TCP的连接中断请求的确认；
4）第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答（ack = w+1），且把服务端的序列值 +1 作为自己 ACK 报文的序号值（seq=u+1），此时客户端处于 TIME_WAIT （时间等待）状态。
2MSL = TIME-WAIT - 等待足够的时间以确保远程TCP接收到连接中断请求的确认；
为什么A在 TIME-WAIT状态必须等待2MSL的时间呢?这有两个理由。
为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN + ACK报文段的确认。B会超时重传这个FIN + ACK报文段，而A就能在2MSL时间内收到这个重传的FIN +ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后，A 和B都正常进入到CLOSED 状态。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN + ACK 报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常步骤进入 CLOSED状态。
防止上一节提到的“已失效的连接请求报文段”出现在本连接中。A在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。B只要收到了A发出的确认，就进入CLOSED 状态。同样，B在撤销相应的传输控制块TCB后，就结束了这次的TCP连接。我们注意到，B结束TCP连接的时间要比A早一些。
保活计时器（keepalive timer） ​ 除时间等待计时器外，TCP还设有一个保活计时器(keepalive timer)。设想有这样的情况:客户已主动与服务器建立了TCP连接。但后来客户端的主机突然出故障。显然，服务器以后就不能再收到客户发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就是使用保活计时器。服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两小时。若两小时没有收到客户的数据，服务器就发送一个探测报文段，以后则每隔75秒钟发送一次。若一连发送10个探测报文段后仍无客户的响应，服务器就认为客户端出了故障，接着就关闭这个连接。
TCP的有限状态机 3.4.2 UDP（User Datagram Protocol，用户数据报协议） 概述 ​ 用户数据报协议UDP只在IP的数据报服务之上增加了很少一点的功能，这就是复用和分用的功能以及差错检测的功能。
​ UDP的主要特点是:
UDP是无连接的，即发送数据之前不需要建立连接（当然，发送数据结束时也没有连接可释放)，因此减少了开销和发送数据之前的时延。 UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的连接状态表（这里面有许多参数)。 UDP是面向报文的。发送方的 UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文，如图5-4所示。在接收方的UDP，对IP层交上来的UDP用户数据报，在去除首部后就原封不动地交付上层的应用进程。也就是说，UDP一次交付一个完整的报文。因此，应用程序必须选择合适大小的报文。若报文太长，UDP把它交给P层后，IP层在传送时可能要进行分片，这会降低IP层的效率。反之，若报文太短，UDP把它交给IP层后，会使IP数据报的首部的相对长度太大，这也降低了IP层的效率。 UDP没有拥塞控制，因此网络出现的拥塞不会使源主机的发送速率降低。这对某些实时应用是很重要的。很多的实时应用（如IP电话、实时视频会议等）要求源主机以恒定的速率发送数据，并且允许在网络发生拥塞时丢失一些数据，但却不允许数据有太大的时延。UDP正好适合这种要求。 UDP支持一对一、一对多、多对一和多对多的交互通信。 UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。 虽然某些实时应用需要使用没有拥塞控制的UDP，但当很多的源主机同时都向网络发送高速率的实时视频流时，网络就有可能发生拥塞，结果大家都无法正常接收。因此，不使用拥塞控制功能的UDP有可能会引起网络产生严重的拥塞问题。
UDP的首部格式 用户数据报UDP有两个字段：数据字段和首部字段。首部字段很简单，只有8个字节(图5-5)，由四个字段组成，每个字段的长度都是两个字节。各字段意义如下:
源端口：源端口号。在需要对方回信时选用。不需要时可用全0。 目的端口：目的端口号。这在终点交付报文时必须使用。 长度：UDP用户数据报的长度，其最小值是8（仅有首部)。 检验和：检测UDP用户数据报在传输中是否有错。有错就丢弃。 ​ 当运输层从IP层收到UDP 数据报时，就根据首部中的目的端口，把UDP 数据报通过相应的端口，上交最后的终点一&mdash;应用进程。图5-6是UDP基于端口分用的示意图。
​ 如果接收方UDP发现收到的报文中的目的端口号不正确（即不存在对应于该端口号的应用进程)，就丢弃该报文，并由网际控制报文协议ICMP 发送“端口不可达”差错报文给发送方。
​ 请注意，虽然在 UDP之间的通信要用到其端口号，但由于UDP的通信是无连接的，因此不需要使用套接字（TCP之间的通信必须要在两个套接字之间建立连接)。
​ UDP用户数据报首部中检验和的计算方法有些特殊。在计算检验和时，要在UDP用户数据报之前增加12个字节的伪首部。所谓“伪首部”是因为这种伪首部并不是UDP用户数据报真正的首部。只是在计算检验和时，临时添加在UDP用户数据报前面，得到一个临时的UDP用户数据报。检验和就是按照这个临时的UDP用户数据报来计算的。伪首部既不向下传送也不向上递交，而仅仅是为了计算检验和。
​ UDP计算检验和的方法和计算IP数据报首部检验和的方法相似。但不同的是:IP 数据报的检验和只检验IP数据报的首部，但UDP的检验和是把首部和数据部分一起都检验。
3.5. 会话层（Session Layer） OSI模型的第五层，管理主机之间的会话进程，即负责建立、管理、终止进程之间的会话。其主要功能是建立通信链接，保持会话过程通信链接的畅通，利用在数据中插入校验点来同步两个结点之间的对话，决定通信是否被中断以及通信中断时决定从何处重新发送。 3.6. 表示层（Presentation Layer） OSI模型的第六层，应用程序和网络之间的翻译官，负责对上层数据或信息进行变换以保证一个主机应用层信息可以被另一个主机的应用程序理解。表示层的数据转换包括数据的解密和加密、压缩、格式转换等。 3.7. 应用层（Application Layer） OSI模型的第七层，负责为操作系统或网络应用程序提供访问网络服务的接口。术语“应用层”并不是指运行在网络上的某个特别应用程序，应用层提供的服务包括文件传输、文件管理以及电子邮件的信息处理。 在应用层的互联设备包括：**网关（Gateway）**等。 文件传输协议FTP（File Transfer Protocol），端口号为21； 超文本传输协议HTTP（HypertextTransfer Protocol），端口号为80； 简单网络管理协议SNMP（SimpleNetwork Management Protocol） 域名服务协议DNS（Domain Name Service） 网络文件系统NFS（Network File System） 等&hellip;&hellip; 超文本传输协议HTTP 报文格式 HTTP有两类报文:
​ (1)请求报文——从客户向服务器发送请求报文，见图6-12(a)
​ (2)响应报文—从服务器到客户的回答，见图6-12(b)
​ 由于HTTP是面向文本的(text-oriented)，因此在报文中的每一个字段都是一些ASCII码串，因而各个字段的长度都是不确定的。
​ HTTP请求报文和响应报文都是由三个部分组成的。可以看出，这两种报文格式的区别就是开始行不同。
开始行，用于区分是请求报文还是响应报文。在请求报文中的开始行叫做请求行(Request-Line)，而在响应报文中的开始行叫做状态行(Status-Line)。在开始行的三个字段之间都以空格分隔开，最后的“CR”和“LF”分别代表“回车”和“换行” 首部行，用来说明浏览器、服务器或报文主体的一些信息。首部可以有好几行，但也可以不使用。在每一个首部行中都有首部字段名和它的值，每一行在结束的地方都要有“回车”和“换行”。整个首部行结束时，还有一空行将首部行和后面的实体主体分开。 实体主体(entity body)，在请求报文中一般都不用这个字段，而在响应报文中也可能没有这个字段。 请求方法 考点 get和post的区别
（在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同）
post请求更安全（不会作为url的一部分，不会被缓存、保存在服务器日志、以及浏览器浏览记录中，get请求的是静态资源，则会缓存，如果是数据，则不会缓存） post请求发送的数据更大（get请求有url长度限制，http协议本身不限制，请求长度限制是由浏览器和web服务器决定和设置） post请求能发送更多的数据类型（get请求只能发送ASCII字符） 传参方式不同（get请求参数一般通过url传递，post请求一般放在request body中传递） get请求的是静态资源，则会缓存，如果是数据，则不会缓存 GET和POST：辩证看100 continue，以及最根本区别：https://github.com/amandakelake/blog/issues/20
状态码 ​ 状态行包括三项内容，即 HTTP的版本，状态码，以及解释状态码的简单短语。 ​ 状态码(Status-Code)都是三位数字的，分为5大类，原先有33种[RFC 2616]，后来又增加了几种[RFC 6585]。这5大类的状态码都是以不同的数字开头的。
1xx表示通知信息，如请求收到了或正在进行处理。 2xx表示成功，如接受或知道了。 3xx表示重定向，如要完成请求还必须采取进-一步的行动。 4xx表示客户的差错，如请求中有错误的语法或不能完成。 5xx表示服务器的差错，如服务器失效无法完成请求。 下面三种状态行在响应报文中是经常见到的。
HTTP/1.1 202 Accepted {接受} HTTP/1.1 400 Bad Request {错误的请求} Http/1.1 404 Not Found {找不到} 4. 考点 三次握手简要流程
TCP 三次握手，其实就是建立一个 TCP 连接，客户端与服务器交互需要 3 个数据包。握手的主要作用就是为了确认双方的接收和发送能力是否正常，初始序列号，交换窗口大小以及 MSS 等信息。
第一次握手：客户端发送 SYN 报文，并进入 SYN_SENT 状态，等待服务器的确认； 第二次握手：服务器收到 SYN 报文，需要给客户端发送 ACK 确认报文，同时服务器也要向客户端发送一个 SYN 报文，所以也就是向客户端发送 SYN + ACK 报文，此时服务器进入 SYN_RCVD 状态； 第三次握手：客户端收到 SYN + ACK 报文，向服务器发送确认包，客户端进入 ESTABLISHED 状态。待服务器收到客户端发送的 ACK 包也会进入 ESTABLISHED 状态，完成三次握手。 为什么 TCP 采用三次握手，二次握手可以吗？
确认双方的收发能力
TCP 建立连接之前，需要确认客户端与服务器双方的收包和发包的能力。
第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。
第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。
第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。
所以，只有三次握手才能确认双方的接收与发送能力是否正常。
什么是半连接队列？
​ 服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立连接。服务器会把这种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。
​ 当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。
三次握手过程中，可以携带数据吗？
第一次、第二次握手不可以携带数据，而第三次握手是可以携带数据的。
TCP 四次挥手
第一次挥手。客户端发起 FIN 包（FIN = 1）,客户端进入 FIN_WAIT_1 状态。TCP 规定，即使 FIN 包不携带数据，也要消耗一个序号。 第二次挥手。服务器端收到 FIN 包，发出确认包 ACK（ack = u + 1），并带上自己的序号 seq=v，服务器端进入了 CLOSE_WAIT 状态。这个时候客户端已经没有数据要发送了，不过服务器端有数据发送的话，客户端依然需要接收。客户端接收到服务器端发送的 ACK 后，进入了 FIN_WAIT_2 状态。 第三次挥手。服务器端数据发送完毕后，向客户端发送 FIN 包（seq=w ack=u+1），半连接状态下服务器可能又发送了一些数据，假设发送 seq 为 w。服务器此时进入了 LAST_ACK 状态。 第四次挥手。客户端收到服务器的 FIN 包后，发出确认包（ACK=1，ack=w+1），此时客户端就进入了 TIME_WAIT 状态。注意此时 TCP 连接还没有释放，必须经过 2*MSL 后，才进入 CLOSED 状态。而服务器端收到客户端的确认包 ACK 后就进入了 CLOSED 状态，可以看出服务器端结束 TCP 连接的时间要比客户端早一些。 为什么建立连接握手三次，关闭连接时需要是四次呢？
​ 在 TCP 握手的时候，接收端发送 SYN+ACK 的包是将一个 ACK 和一个 SYN 合并到一个包中，所以减少了一次包的发送，三次完成握手。
​ 对于四次挥手，因为 TCP 是全双工通信，在主动关闭方发送 FIN 包后，接收端可能还要发送数据，不能立即关闭服务器端到客户端的数据通道，所以也就不能将服务器端的 FIN 包与对客户端的 ACK 包合并发送，只能先确认 ACK，然后服务器待无需发送数据时再发送 FIN 包，所以四次挥手时必须是四次数据包的交互。
为什么TIME_WAIT 状态需要经过 2MSL 才能返回到 CLOSE 状态
​ MSL 指的是报文在网络中最大生存时间。在客户端发送对服务器端的 FIN 的确认包 ACK 后，这个 ACK 包是有可能不可达的，服务器端如果收不到 ACK 的话需要重新发送 FIN 包。
​ 所以客户端发送 ACK 后需要留出 2MSL 时间（ACK 到达服务器 + 服务器发送 FIN 重传包，一来一回）等待确认服务器端确实收到了 ACK 包。
​ 也就是说客户端如果等待 2MSL 时间也没有收到服务器端的重传包 FIN，说明可以确认服务器已经收到客户端发送的 ACK。
网络模型
TCP和UDP有什么区别
1）连接方面 : TCP面向连接。UDP是无连接的，发送数据之前不需要建立连接
2）安全方面 : TCP提供可靠的服务，保证传送的数据，无差错，不丢失，不重复，且按序到达。UDP则是尽最大努力交付，不保证可靠交付
3）传输效率：TCP传输效率相对较低，UDP传输效率高
TCP是可靠的连接，它是怎么实现的
​ TCP的连接是基于三次握手，而断开则是四次挥手。为了保障数据不丢失及错误（可靠性），它有报文校验、ACK应答、超时重传(发送方)、失序数据重传（接收方）、丢弃重复数据、流量控制（滑动窗口）和拥塞控制等机制
SYN Flood
​ SYN Flood 伪造 SYN 报文向服务器发起连接，服务器在收到报文后用 SYN_ACK 应答，此应答发出去后，不会收到 ACK 报文
​ 若攻击者发送大量这样的报文，会在被攻击主机上出现大量的半连接，耗尽其资源，使正常的用户无法访问，直到半连接超时
TCP的粘包和拆包
​ 程序需要发送的数据大小和TCP报文段能发送MSS（Maximum Segment Size，最大报文长度）是不一样的
​ 大于MSS时，而需要把程序数据拆分为多个TCP报文段，称之为拆包；小于时，则会考虑合并多个程序数据为一个TCP报文段，则是粘包；其中MSS = TCP报文段长度-TCP首部长度
解决粘包和拆包的方法都有哪些
1）在数据尾部增加特殊字符进行分割
2）将数据定为固定大小
3）将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小</content></entry><entry><title>C language - TCP/IP .h file and use Xdp</title><url>/post/c-language-tcpip-.h-file-and-use-xdp/</url><categories><category>C</category></categories><tags><tag>C</tag></tags><content type="html"> Commonly use .h network file
IPv4 header netinet/ip.h：struct ip
struct ip { #if BYTE_ORDER == LITTLE_ENDIAN u_char ip_hl:4, /* header length */ ip_v:4; /* version */ #endif #if BYTE_ORDER == BIG_ENDIAN u_char ip_v:4, /* version */ ip_hl:4; /* header length */ #endif u_char ip_tos; /* type of service */ short ip_len; /* total length */ u_short ip_id; /* identification */ short ip_off; /* fragment offset field */ #define IP_DF 0x4000 /* dont fragment flag */ #define IP_MF 0x2000 /* more fragments flag */ u_char ip_ttl; /* time to live */ u_char ip_p; /* protocol */ u_short ip_sum; /* checksum */ struct in_addr ip_src,ip_dst; /* source and dest address */ }; linux/ip.h：struct iphdr struct iphdr { #if defined(__LITTLE_ENDIAN_BITFIELD) __u8 ihl:4, version:4; #elif defined (__BIG_ENDIAN_BITFIELD) __u8 version:4, ihl:4; #else #error "Please fix &lt;asm/byteorder.h>" #endif __u8 tos; __u16 tot_len; __u16 id; __u16 frag_off; __u8 ttl; __u8 protocol; __u16 check; __u32 saddr; __u32 daddr; /*The options start here. */ }; Tcp header netinet/tcp.h ： struct tcphdr
struct tcphdr { u_short th_sport; /* source port */ u_short th_dport; /* destination port */ tcp_seq th_seq; /* sequence number */ tcp_seq th_ack; /* acknowledgement number */ #if BYTE_ORDER == LITTLE_ENDIAN u_char th_x2:4, /* (unused) */ th_off:4; /* data offset */ #endif #if BYTE_ORDER == BIG_ENDIAN u_char th_off:4, /* data offset */ th_x2:4; /* (unused) */ #endif u_char th_flags; #define TH_FIN 0x01 #define TH_SYN 0x02 #define TH_RST 0x04 #define TH_PUSH 0x08 #define TH_ACK 0x10 #define TH_URG 0x20 u_short th_win; /* window */ u_short th_sum; /* checksum */ u_short th_urp; /* urgent pointer */ }; linux/tcp.h ： struct tcphdr
Udp header netinet/udp.h： struct udphdr
struct udphdr { u_int16_t uh_sport; /* source port */ u_int16_t uh_dport; /* destination port */ u_int16_t uh_ulen; /* udp length */ u_int16_t uh_sum; /* udp checksum */ }; linux/udp.h： struct udphdr
sk_buff linux/skbuff.h：struck sk_buff
struct sk_buff { /* These two members must be first. */ struct sk_buff *next; struct sk_buff *prev; struct sock *sk; struct skb_timeval tstamp; struct net_device *dev; struct net_device *input_dev; //transport union { struct tcphdr *th; struct udphdr *uh; struct icmphdr *icmph; struct igmphdr *igmph; struct iphdr *ipiph; struct ipv6hdr *ipv6h; unsigned char *raw; } h; //network union { struct iphdr *iph; struct ipv6hdr *ipv6h; struct arphdr *arph; unsigned char *raw; } nh; //mac union { unsigned char *raw; } mac; //route struct dst_entry *dst; struct sec_path *sp; ......... }; Icmp header netinet/ip_icmp.h： struct icmphdr
linux/icmp.h： struct icmphdr
type and code
reference：https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol
Arp header net/if_arp.h： struct arphdr linux/if_arp.h ：struct arphdr Ethernet header netinet/if_ether.h： linux/if_ether.h ： struct ethhdr Use iovisor/Bcc linux/bpf.h：
enum xdp_action { XDP_ABORTED = 0, XDP_DROP, XDP_PASS, XDP_TX, XDP_REDIRECT, }; struct xdp_md { __u32 data; __u32 data_end; __u32 data_meta; /* Below access go through struct xdp_rxq_info */ __u32 ingress_ifindex; /* rxq->dev->ifindex */ __u32 rx_queue_index; /* rxq->queue_index */ __u32 egress_ifindex; /* txq->dev->ifindex */ }; example
filter.c #include &lt;linux/bpf.h> #include &lt;linux/if_ether.h> #include &lt;linux/ip.h> #include &lt;linux/in.h> #include &lt;linux/udp.h> int filter(struct xdp_md *ctx) { void *data = (void *)(long)ctx->data; void *data_end = (void *)(long)ctx->data_end; struct ethhdr *eth = data; //指向数据包开头 if ((void*)eth + sizeof(*eth) &lt;= data_end) { struct iphhdr *ip = data + sizeof(*eth); //指向IP if ((void*)ip + sizeof(*ip) &lt;= data_end) { ...... } } return XDP_PASS; } run.py
from bcc import BPF device = "lo" b = BPF(src_file="filter.c") f = b.load_func("filter", BPF.XDP) b.attach_xdp(device, f, 0) try: b.trace_print() except ... : .... b.remove_xdp(device, 0) Bcc Related Reference
What exactly is usage of cursor_advance in BPF? Reference Guide https://github.com/iovisor/bcc/tree/7e3f0c08c7c28757711c0a173b5bd7d9a31cf7ee/examples Function Guide https://github.com/iovisor/bcc/blob/master/docs/reference_guide.md</content></entry><entry><title>OSPF</title><url>/post/review-the-ospf/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> Ospf作为链路状态路由协议，路由器之间交互的是LSA（链路状态通告），路由器将网络中泛洪的LSA搜集到自己的LSDB（链路状态数据库）中，这有助于Ospf理解整张网络拓扑，并在此基础上通过SPF最短路径算法计算出以自己为根的、到达网络各个角落的、无环的树，最终，路由器将计算出来的路由装载进路由表中。
Router ID 在DR和BDR选举的过程中，如果两台路由器的DR优先级相等，需要进一步比较两台路由器的Router ID，Router ID大的路由器将被选为DR或BDR。
Router ID是用于在自治系统中唯一标识一台运行OSPF的路由器的32位整数。每个运行OSPF的路由器都有一个Router ID。Router ID的格式和IP地址的格式是一样的。
Cost OSPF使用cost“开销”作为路由度量值。每一个激活OSPF的接口都有一个cost值，一条OSPF路由的cost由该路由从路由的起源一路到达本地的所有入接口cost值的总和。
Ospf Tables 邻居表（Peer Table）：主要记录形成邻居关系路由器 链路状态数据库（LSDB）：Ospf用LSA（link state Advertisement 链路状态通告）来描述网络拓扑信息，然后Ospf路由器用链路状态数据库来存储网络的这些LSA。OSPF将自己产生的以及邻居通告的LSA搜集并存储在链路状态数据库LSDB中 OSPF路由表（Routing Table）：对链路状态数据库进行SPF（Dijkstra）计算，而得出的OSPF路由表 五种报文 报文类型 报文作用 Hello报文 周期性发送，用来发现和维持OSPF邻居关系 DD报文（DataBase Description packet） 描述本地LSDB（Link State Database）的摘要信息，用于两台设备进行数据库同步 LSR报文（Link State Request packet） 用于向对方请求所需的LSA。设备只有在OSPF邻居双方成功交换DD报文后才会向对方发出LSR报文 LSU报文（Link State Update packet） 用于向对方发送其所需要的LSA LSAck 用来对收到的LSA进行确认 Ospf状态机 OSPF接口状态机 Down：接口的初始状态。表明此时接口不可用，不能用于收发流量。 Loopback：设备到网络的接口处于环回状态。环回接口不能用于正常的数据传输，但可以通过Router-LSA进行通告。因此，进行连通性测试时能够发现到达这个接口的路径。 Waiting：设备正在判定网络上的DR和BDR。在设备参与DR和BDR选举前，接口上会启动Waiting定时器。在这个定时器超时前，设备发送的Hello报文不包含DR和BDR信息，设备不能被选举为DR或BDR。这样可以避免不必要地改变链路中已存在的DR和BDR。仅NMBA网络、广播网络有此状态。 P-2-P：接口连接到物理点对点网络或者是虚拟链路，这个时候设备会与链路连接的另一端设备建立邻接关系。仅P2P、P2MP网络有此状态。 DROther：设备没有被选为DR或BDR，但连接到广播网络或NBMA网络上的其他设备被选举为DR。它会与DR和BDR建立邻接关系。 BDR：设备是相连的网络中的BDR，并将在当前的DR失效时成为DR。该设备与接入该网络的所有其他设备建立邻接关系。 DR：设备是相连的网络中的DR。该设备与接入该网络的所有其他设备建立邻接关系。 OSPF邻居状态机 Down：邻居会话的初始阶段。表明没有在邻居失效时间间隔内收到来自邻居设备的Hello报文。除了NBMA网络OSPF路由器会每隔PollInterval时间对外轮询发送Hello报文，包括向处于Down状态的邻居路由器（即失效的邻居路由器）发送之外，其他网络是不会向失效的邻居路由器发送Hello报文的。 Init：本状态表示已经收到了邻居的Hello报文，但是对端并没有收到本端发送的Hello报文，收到的Hello报文的邻居列表并没有包含本端的Router ID，双向通信仍然没有建立。 2-Way：互为邻居。本状态表示双方互相收到了对端发送的Hello报文，报文中的邻居列表也包含本端的Router ID，邻居关系建立。如果不形成邻接关系则邻居状态机就停留在此状态，否则进入ExStart状态。DR和BDR只有在邻居状态处于2-Way及之后的状态才会被选举出来。 ExStart：协商主从关系。建立主从关系主要是为了保证在后续的DD报文交换中能够有序的发送。邻居间从此时才开始正式建立邻接关系。 Exchange：交换DD报文。本端设备将本地的LSDB用DD报文来描述，并发给邻居设备。 Loading：正在同步LSDB。两端设备发送LSR报文向邻居请求对方的LSA，同步LSDB。 Full：建立邻接。两端设备的LSDB已同步，本端设备和邻居设备建立了完全的邻接关系。 Ospf邻居建立过程 Ospf运行机制 通过交互Hello报文形成邻居关系
路由器运行OSPF协议后，会从所有启动OSPF协议的接口上发送Hello报文。如果两台路由器共享一条公共数据链路，并且能够成功协商各自Hello报文中所指定的某些参数，就能形成邻居关系。
通过泛洪LSA通告链路状态信息
每台路由器根据自己周围的网络拓扑结构生成LSA，LSA描述了路由器所有的链路、接口、邻居及链路状态等信息，路由器通过交互这些链路信息来了解整个网络的拓扑信息。
通过泛洪LSA通告链路状态信息
通过LSA的泛洪，路由器会把收到的LSA汇总记录在LSDB中。
通过SPF算法计算并形成路由
当LSDB同步完成之后，每一台路由器都将以其自身为根，使用SPF算法来计算一个无环路的拓扑图来描述它所知道的到达每一个目的地的最短路径（最小的路径代价）。这个拓扑图就是最短路径树，有了这棵树，路由器就能知道到达自治系统中各个节点的最优路径。
维护和更新路由表
根据SPF算法得出最短路径树后，每台路由器将计算得出的最短路径加载到OSPF路由表形成指导数据转发的路由表项。
Ospf组播地址 224.0.0.5 － 所有的OSPF路由器(OSPF 发送HELLO包) 224.0.0.6 － DR/BDR路由监听这个地址 DR 与 BDR DR:路由器的核心者，称为指定路由器。邻居关系只会发给DR
BDR：被指定路由器，如果DR失效，那么BDR就会顶上去工作
为什么需要有DR？ 在广播网络和NBMA网络中，任意两台路由器之间都要传递路由信息。若网络中有n台路由器，则需要建立n*(n-1)/2个邻接关系。这使得任何一台路由器的路由变化都会导致多次传递，浪费了带宽资源。
为解决这一问题，OSPF定义了DR。通过选举产生DR后，所有其他设备都只将信息发送给DR，由DR将网络链路状态LSA广播出去。
为了防止DR发生故障，重新选举DR时会造成业务中断，除了DR之外，还会选举一个备份指定路由器BDR。这样除DR和BDR之外的路由器（称为DR Other）之间将不再建立邻接关系，也不再交换任何路由信息，这样就减少了广播网和NBMA网络上各路由器之间邻接关系的数量。
DR和BDR的选举依据 比较优先级，优先级越大越优。默认优先级为1，如果优先级为0，则没有选举权。如果优先级一致，比较router-id，越大越优。
DR和BDR的选举原则 如果同一网段中的DR和BDR为空，首先选举BDR。将所有优先级大于0的设备放入到候选列表中（优先级为0的不参与选举），比较优先级，优先级越大越优。如果优先级一致，比较router-id，越大越优。BDR选举后，由BDR升级为DR并重新选举BDR。 如果同一网段中已有设备通告自己是BDR，但DR为空，则BDR升级为DR，重新选举BDR。 如果同一网段中已有设备通告自己是DR，但BDR为空，则选举BDR。 如果同一网段中只有唯一的一台设备通告自己为DR或BDR，则通告者为DR或BDR，DR或BDR角色不抢占。 如果在同一网段中存在多台设备同时通告自己为DR或BDR，则DR或者BDR需从新选举 选举制 选举制是指DR和BDR不是人为指定的，而是由本网段中所有的路由器共同选举出来的。路由器接口的DR优先级决定了该接口在选举DR、BDR时所具有的资格，本网段内DR优先级大于0的路由器都可作为“候选人”。选举中使用的“选票”就是Hello报文，每台路由器将自己选出的DR写入Hello报文中，发给网段上的其他路由器。当处于同一网段的两台路由器同时宣布自己是DR时，DR优先级高者胜出。如果优先级相等，则Router ID大者胜出。如果一台路由器的优先级为0，则它不会被选举为DR或BDR
终身制 终身制也叫非抢占制。每一台新加入的路由器并不急于参加选举，而是先考察一下本网段中是否已存在DR。如果目前网段中已经存在DR，即使本路由器的DR优先级比现有的DR还高，也不会再声称自己是DR，而是承认现有的DR。因为网段中的每台路由器都只和DR、BDR建立邻接关系，如果DR频繁更换，则会引起本网段内的所有路由器重新与新的DR、BDR建立邻接关系。这样会导致短时间内网段中有大量的OSPF协议报文在传输，降低网络的可用带宽。终身制有利于增加网络的稳定性、提高网络的可用带宽。实际上，在一个广播网络或NBMA网络上，最先启动的两台具有DR选举资格的路由器将成为DR和BDR
继承制 继承制是指如果DR发生故障了，那么下一个当选为DR的一定是BDR，其他的路由器只能去竞选BDR的位置。这个原则可以保证DR的稳定，避免频繁地进行选举，并且DR是有备份的（BDR），一旦DR失效，可以立刻由BDR来承担DR的角色。由于DR和BDR的数据库是完全同步的，这样当DR故障后，BDR立即成为DR，履行DR的职责，而且邻接关系已经建立，所以从角色切换到承载业务的时间会很短。同时，在BDR成为新的DR之后，还会选举出一个新的BDR，虽然这个过程所需的时间比较长，但已经不会影响路由的计算了
Ospf LSA 类型 LSA 类型 LSA 类型编号 LAS用处 Router LSA 1 所有的OSPF speaker都会产生该类LSA，只在区域内传播，这种类型的LSA用于描述设备的链路状态和开销，在路由器所属的区域内传播 Network LSA 2 由ABR产生，描述区域内某个网段的路由，并通告给发布或接收此LSA的非Totally STUB或NSSA区域 Network-summary-LSA 3 由ABR发布，用来描述区域间的路由信息。ABR将Network-summary-LSA发布到一个区域，通告该区域到其他区域的目的地址。实际上，ABR是将区域内部的Type1和Type2的信息收集起来并汇总之后扩散出去，这就是Summary的含义 ASBR summary LSA 4 由ABR发布，描述到ASBR的路由信息，并通告给除ASBR所在区域的其他相关区域 Autonomous system external LSA 5 由ASBR产生，描述到AS外部的路由，通告到所有的区域（除了STUB区域和NSSA区域） NSSA External LSA 7 由ASBR产生，描述到AS外部的路由，仅在NSSA区域内传播。NSSA区域的ABR收到NSSA LSA时，会有选择地将其转化为Type5 LSA，以便将外部路由信息通告到OSPF网络的其它区域。 Opaque LSA（链路本地范围）、Opaque LSA（本地区域范围）、Opaque LSA（ AS 范围） 9、10、11 Opaque LSA提供用于OSPF的扩展的通用机制。其中：Type9 LSA仅在接口所在网段范围内传播。用于支持GR的Grace LSA就是Type9 LSA的一种。Type10 LSA在区域内传播。用于支持TE的LSA就是Type10 LSA的一种。Type11 LSA在自治域内传播，目前还没有实际应用的例子 Ospf区域 为什么需要划分区域 随着网络规模日益扩大，当一个大型网络中的路由器都运行OSPF路由协议时，会出现以下问题：
网络拓扑发生变化概率增大，LSA泛洪严重，降低网络带宽利用率。 路由器数量增多，LSDB庞大，占用大量存储空间，并使得运行SPF算法的复杂度增加。 每台路由器需要维护的路由表越来越大。 OSPF协议通过将自治系统划分成不同的区域，将LSA泛洪限制在一个区域内，提高网络的利用率和路由的收敛速率；每个区域内的路由器数量减少，维护的LSDB规模降低，SPF计算也仅限于区域内的LSA；每台路由器需要维护的路由表也越来越小。 此外，多区域提高了网络的扩展性，有利于组建大规模的网络。
区域是从逻辑上将路由器划分为不同的组，每个组用区域号（Area ID）来标识。区域的边界是路由器，而不是链路。一个网段（链路）只能属于一个区域，或者说每个运行OSPF的接口必须指明属于哪一个区域。
Ospf 路由器类型 区域内路由器（Internal Router）该类设备的所有接口都属于同一个OSPF区域。 区域边界路由器ABR（Area Border Router） 该类设备可以同时属于两个以上的区域，但其中一个必须是骨干区域。ABR用来连接骨干区域和非骨干区域，它与骨干区域之间既可以是物理连接，也可以是逻辑上的连接。 骨干路由器（Backbone Router）该类设备至少有一个接口属于骨干区域。所有的ABR和位于Area0的内部设备都是骨干路由器。 自治系统边界路由器ASBR（AS Boundary Router）与其他AS交换路由信息的设备称为ASBR。 ASBR并不一定位于AS的边界，它可能是区域内设备，也可能是ABR。只要一台OSPF设备引入了外部路由的信息，它就成为ASBR。
![](/images/Review-The-OSPF/Ospf 路由器类型.png)
区域类型 普通区域，必须有且只能有一个。
骨干区域：负责在非骨干区域之间发布由区域边界路由器汇总的路由信息（并非详细的链路状态信息），为避免区域间路由环路，非骨干区域之间不允许直接相互发布区域间路由。因此，所有区域边界路由器都至少有一个接口属于Area 0，即每个区域都必须连接到骨干区域。
标准区域：最通用的区域，它传输区域内路由，区域间路由和外部路由。所有非骨干区域必须与骨干区域保持连通
Stub 末梢区域：
Stub区域的ABR不传播它们接收到的自治系统外部路由。
一般情况下，Stub区域位于自治系统的边界，是只有一个ABR的非骨干区域，为保证到自治系统外的路由依旧可达，Stub区域的ABR将生成一条缺省路由，并发布给Stub区域中的其他非ABR路由器
骨干区域不能配置成Stub区域。 如果要将一个区域配置成Stub区域，则该区域中的所有路由器都要配置Stub区域属性。 Stub区域内不能存在ASBR，因此自治系统外部的路由不能在本区域内传播。 虚连接不能穿过Stub区域。 Total Stub 绝对末梢区域：允许ABR发布Type3缺省路由，不允许发布自治系统外部路由和区域间的路由，只允许发布区域内路由。
NSSA 次末梢区域：
不允许存在Type5 LSA。
NSSA区域允许引入自治系统外部路由，携带这些外部路由信息的Type7 LSA由NSSA的ASBR产生，仅在本NSSA内传播。
当Type7 LSA到达NSSA的ABR时，由ABR将Type7 LSA转换成Type5 LSA，泛洪到整个OSPF域中。
骨干区域不能配置成NSSA区域。 如果要将一个区域配置成NSSA区域，则该区域中的所有路由器都要配置NSSA区域属性。 NSSA区域的ABR会发布Type7 LSA缺省路由传播到本区域内。 所有域间路由都必须通过ABR才能发布。 虚连接不能穿过NSSA区域。 Total NSSA 绝对次末梢区域：不允许发布自治系统外部路由和区域间的路由，只允许发布区域内路由。
Ospf区域间环路及防环方法 Ospf在区域内部运行的是SPF算法，这个算法能够保证区域内部的路由不会成环。然而划分区域后，区域之间的路由传递实际上是一种类似距离矢量算法的方式，这种方式容易产生环路。
为了避免区域间的环路，Ospf规定直接在两个非骨干区域之间发布路由信息是不允许的，只允许在一个区域内部或者在骨干区域和非骨干区域之间发布路由信息。因此，每个ABR都必须连接到骨干区域。
假设Ospf允许非骨干区域之间直接传递路由，则可能会导致区域间环路。如下图所示，骨干区连接到其他网络的路由信息会传递至Area 1。假设非骨干区之间允许直接传递路由信息，那么这条路由信息最终又被传递回去，形成区域间的路由环路。为了防止这种区域间环路，OSPF禁止Area 1和Area 3，以及Area 2和Area 3之间直接进行路由交互，而必须通过骨干区域进行路由交互。这样就能防止区域间环路的产生。
Ospf 虚连接 虚连接（Virtual link）是指在两台ABR之间通过一个非骨干区域建立的一条逻辑上的连接通道。
根据RFC 2328，在部署OSPF时，要求所有的非骨干区域与骨干区域相连，否则会出现有的区域不可达的问题。但是在实际应用中，可能会因为各方面条件的限制，无法满足所有非骨干区域与骨干区域保持连通的要求，此时可以通过配置OSPF虚连接来解决这个问题。
通过虚连接，两台ABR之间直接传递OSPF报文信息，两者之间的OSPF设备只是起到一个转发报文的作用。由于OSPF协议报文的目的地址不是这些设备，所以这些报文对于两者而言是透明的，只是当作普通的IP报文来转发。
Ospf路由聚合和路由过滤 Ospf路由聚合 路由聚合是指ABR可以将具有相同前缀的路由信息聚合到一起，只发布一条路由到其它区域。
通过路由聚合，可以减少路由信息，从而减小路由表的规模，提高设备的性能。
OSPF有两种路由聚合方式：
区域间路由聚合
区域间路由聚合在ABR上完成，主要用于聚合AS内区域之间的路由。ABR向其它区域发送路由信息时，以网段为单位生成Type3 LSA。如果该区域中存在一些连续的网段，ABR可以将这些连续的网段聚合成一个网段。这样ABR只发送一条聚合后的LSA，所有属于命令指定的聚合网段范围的LSA将不会再被单独发送出去。
外部路由聚合
外部路由聚合在ASBR上完成，主要用于聚合OSPF引入的外部路由。ASBR将对引入的聚合地址范围内的Type5 LSA进行聚合。当配置了NSSA区域时，还要对引入的聚合地址范围内的Type7 LSA进行聚合。
如果本地设备既是ASBR又是ABR，则对由Type7 LSA转化成的Type5 LSA进行聚合处理。
Ospf路由过滤 OSPF支持使用路由策略对路由信息进行过滤。缺省情况下，OSPF不进行路由过滤。
OSPF可以使用的路由策略包括route-policy、访问控制列表（access-list）和地址前缀列表（prefix-list）。
Ospf多进程 OSPF支持多进程，在同一台路由器上可以运行多个不同的OSPF进程，它们之间互不影响，彼此独立。不同OSPF进程之间的路由交互相当于不同路由协议之间的路由交互。
路由器的一个接口只能属于某一个OSPF进程。
OSPF多进程的一个典型应用就是在VPN场景中PE和CE之间运行OSPF协议，同时VPN骨干网上的IGP也采用OSPF。在PE上，这两个OSPF进程互不影响。
Ospf GTSM Ospf GR Ospf 邻居震荡抑制 Ospf TE</content></entry><entry><title>BGP</title><url>/post/review-the-bgp-protocol/</url><categories><category>路由交换</category></categories><tags><tag>路由交换</tag></tags><content type="html"> BGP对等体之间的交互原则 从IBGP对等体获得的BGP路由，BGP设备只发布给它的EBGP对等体。 从EBGP对等体获得的BGP路由，BGP设备发布给它所有EBGP和IBGP对等体。 当存在多条到达同一目的地址的有效路由时，BGP设备只将最优路由发布给对等体。 路由更新时，BGP设备只发送更新的BGP路由。 所有对等体发送的路由，BGP设备都会接收。 BGP引入IGP路由 BGP协议本身不发现路由，因此需要将其他路由引入到BGP路由表，实现AS间的路由互通。当一个AS需要将路由发布给其他AS时，AS边缘路由器会在BGP路由表中引入IGP的路由。为了更好的规划网络，BGP在引入IGP的路由时，可以使用路由策略进行路由过滤和路由属性设置，也可以设置MED值指导EBGP对等体判断流量进入AS时选路。
BGP引入路由时支持Import和Network两种方式： Import方式是按协议类型，将RIP、OSPF、ISIS等协议的路由引入到BGP路由表中。为了保证引入的IGP路由的有效性，Import方式还可以引入静态路由和直连路由。 Network方式是逐条将IP路由表中已经存在的路由引入到BGP路由表中，比Import方式更精确。 BGP Commuity Internet：表示所有路由，可以通过匹配Internet 属性来匹配所有的路由条目 No_export：不通告给任何EBGP邻居，如果存在BGP联邦，则会在联邦内的EBGP邻居之间传递 No_advertise：不通告给任何BGP邻居 As_Path Type 组成：Path Segment Type, Path Segment Length, Path Segment Value
As_Set: 由一系列As号无序地组成，包含在Updata消息里 As_Sequence: 由一系列As号顺序地组成，包含在Updata消息里 As_Confed_Sequence: 在本地联盟内由一系列成员As号按顺序地组成，包含在Updata消息中，只能在本地联盟内传递 As_Confed_Set: 在本地联盟内由一系列成员As无序地组成，包含在Updata消息中，同样只能在本地联盟内传递 BGP Ring Protection As内部防环：通过iBGP水平分割实现。从iBGP邻居学到的路由不会更新给其他IBGP邻居。
若无此机制，那么当三个路由器两两相连，并建立了iBGP邻居时，那么此时其中一个路由器发送的更新路由将在三个路由器之间无限循环
要实现As内部每台路由器都可以学习到路由，需要建立iBGP邻居全互联（路由器数目很大的时候，那将导致网络中的路由器因为需要维护过多的BGP邻居关系导致性能下降）。可以通过路由反射器，eBGP联邦解决
As间防环：通过 As path 防环，每经过一个As，会添加该As的As编号在As_Path字段的最前面,当从eBGP邻居得到一条路由时，会检查该路由的As_path字段有没有自身所在As，如果有则，丢弃，如果没有则继续
RR防环
SOO防环
Reference
https://forum.huawei.com/enterprise/en/bgp-ring-protection-mechanisms/thread/481269-861 BGP Route Reflector ( RR ) 在BGP的网络中，为保证IBGP对等体之间的连通性，需要在IBGP对等体之间建立全连接关系。假设在一个AS内部有n台路由器，那么应该建立的IBGP连接数就为n(n-1)/2.当IBGP对等体数目很多时，对网络资源和CPU资源的消耗都很大
​ 在一个AS内，其中一台路由器作为路由反射器RR（Route Reflector），RR和Client组成i一个集群（Cluster），其它路由器作为客户机（Client）与路由反射器之间建立IBGP连接。路由反射器在客户机之间传递（反射）路由信息，而客户机之间不需要建立BGP连接
​ 既不是反射器也不是客户机的BGP路由器被称为非客户机（Non-Client）。非客户机与路由反射器之间，以及所有的非客户机之间仍然必须建立全连接关系
总结：
Client只需维护与RR之间的IBGP会话
Non-Client与Non-Client之间需要建立IBGP全互连
RR与Non-Client之间需要建立IBGP全互连
RR与RR之间需要建立IBGP的全互连
rule
从非客户机IBGP对等体学到的路由，发布给此RR的所有客户机
从EBGP对等体学到的路由，发布给所有的非客户机和客户机
从客户机学到的路由，发布给此RR的所有非客户机和客户机（发起此路由的客户机除外）
从EBGP对等体学到的路由，发布给所有的非客户机和客户机
Router Reflector Cluster
当一个AS内存在多台RR为Client提供冗余时，RR间的路由更新很有可能会形成环路，为防止该现象，引入簇（Cluster）的概念
通过4字节的Cluster_ID来标识Cluster，通常会使用Loopback地址作为Cluster_ID
一个Cluster里可以包括一个或多个RR；一个Client可以同时属于多个Cluster
通常，一个客户的簇只拥有一个RR，并由RR的BGP Router-id去标识该簇。有时，为了防止单点失效，在单一簇里引入多个RR
Ring Protection
Originator_ID
Originator_ID属性用于防止在反射器和客户机/非客户机之间产生环路 Originator_ID属性长4字节，可选非过渡属性，属性类型为9 ，是由路由反射器（RR）产生的，携带了本地AS内部路由发起者的Router ID 当一条路由第一次被RR反射的时候，RR将Originator_ID属性加入到这条路由，标识这条路由的始发路由器。如果一条路由中已经存在了Originator_ID属性，则RR将不会创建新的Originator_ID 当其它BGP Speaker接收到这条路由的时候，将比较收到的Originator_ID和本地的Router ID，如果两个ID相同，BGP Speaker会忽略掉这条路由，不做处理 Cluster_List
Originator_ID属性用于防止在RR之间产生环路 Cluster_List是可选非过渡属性，属性类型编码为10 Cluster_List由一系列的Cluster_ID组成，描述了一条路由所经过的反射器路径，这和描述路由经过的As路径的AS_Path属性有相似之处。Cluster_List由路由反射器产生 Cluster_List只在AS内部传播，从EBGP对等体收到的含有Cluster_List的路由将被丢弃 当RR在它的客户机之间或客户机与非客户机之间反射路由时，RR会把本地Cluster_ID添加到Cluster_List的前面。如果Cluster_List为空，RR就创建一个 当RR接收到一条更新路由时，RR会检查Cluster_List。如果Cluster_List中已经有本地Cluster_ID，丢弃该路由不需要再反射；如果没有本地Cluster_ID，将其加入Cluster_List，然后反射该更新路由 Cluster_List只被RR用来检测路由环路，不是RR的客户机和非客户机不会检测该属性 BGP Confederation 将一个大的As分割成多个小型的As，让As内部拥有足够数量的eBGP邻居关系来解决路由限制问题。
对于外部邻居来说(联盟外的的对等体)，成员AS拓扑是不可见的。也就是说，在发向eBGP邻居的更新消息中，已经剥去了联盟内被修改的As_PATH。从其他的自治系统来看，联盟就像单个As一样。
Ring Protection
Confederation内采用As_Confed防止子AS间路由环路
Confederation内As_Path属性变化
//示意图 |--------> As 200 &lt;----------------| As(100) --> As((1000),100) --> As((1000,1001),100) --> As(200,100) Confederation内eBGP
子As号被添加到As_Path中的As_Confed_Sequence前面 Confederation内iBGP
不做任何改变 外部eBGP
As号从As_Path中清除，而大As号被添加到As_Path前面 Routing Priority BGP Routing Switching Principles 从上到下
Weight：优先选择 highest weight 的路由（ cisco私有属性 ）
Local Preference：优先选择 highest local preference 的路由
Originate：优先选择自己本地的路由
As path length：优先选择 shortest As path length 的路由
Origin code：优先选择 lowest origin code 的路由 ( IGP &lt; EGP &lt; incomplete)
MED：优先选择 lowest MED 的路由
eBGP path over iBGP path：eBGP的路由优于iBGP的路由
Shortest IGP path to BGP next hop：
Oldest Path：从更老的eBGP邻居学过来的路由，因为eBGP邻居建立时间越久，说明越稳定
Router ID：优先选择lowest BGP neighbor router ID的路由
Neighbor IP address：优先选择lowest neighbor IP address的路由
Reference
https://networklessons.com/bgp/bgp-attributes-and-path-selection BGP Tables BGP Neighbor Table：包含有关BGP邻居信息的表 BGP Table (BGP RIB)：包含从network layer reachability information (NLRI)学习到的路由和来自所以邻居的所以路由 BGP Routing Table：包含由BGP Table的路由中选择最优路由 BGP Message Format Field Marker: Included for compatibility, must be set to all ones. Length: Total length of the message in octets , including the header. Type：Type of BGP message. The following values are defined: Open (1)：发送一些参数以协商和建立连接，两个邻居都会发送Open报文，一个先发Open，另一个收到后，发送一个Open+Keeplive报文。Open报文里携带了version、BGP AS号、Holdtime、BGP identifier(router-id) Update (2)：用于对等体之间交换路由信息 Notification (3)：只要检测到error，便发送并关闭BGP连接 KeepAlive (4)：周期性（Holdtime）发送以确认邻居是否存活 （extend：Error Code, Error Subcode） Route-Refresh (5)：用来要求对等体重新发送指定地址族的路由信息 Message Header Format 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | + + | | + + | Marker | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Length | Type | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ BGP报文由BGP报文头和具体报文内容两部分组成
BGP报文头包括三的部分，总长19字节。
Marker：占16字节，用于检查BGP对等体的同步信息是否完整，以及用于BGP验证的计算。不使用验证时所有比特均为1（十六进制则全“FF”）。
Length：占2个字节（无符号位），BGP消息总长度（包括报文头在内），以字节为单位。长度范围是19～4096。
Type：占1个字节（无符号位），BGP消息的类型。Type有5个可选值，表示BGP报文头后面所接的5类报文（其中，前四种消息是在RFC4271中定义的，而Type5的消息则是在RFC2918中定义的）
TYPE值 报文类型 1 OPEN 2 UPDATE 3 NOTIFICATION 4 KEEPALIVE 5 REFRESH Open Message Format 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+ | Version | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | My Autonomous System | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Hold Time | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | BGP Identifier | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Opt Parm Len | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | | | Optional Parameters (variable变长) | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ version：表示协议的版本号，现在BGP的版本号为4。 My autonomous System:发送者自己的AS域号 Hold Time:发送者自己设定的hold time值（单位：秒），用于协商BGP对等体间保持建立连接关系，发送KEEPALIVE或UPDATE等报文的时间间隔。BGP的状态机必须在收到对等体的OPEN报文后，对发出的OPEN报文和收到的OPEN报文两者的hold time时间作比较，选择较小的时间作为协商结果。Hold Time的值可为零（不发KEEPALIVE报文）或大于等于3，我们系统的默认为180。 BGP Identifier：发送者的router id。 Opt Parm Len：表示Optional Parameters（可选参数）的长度。如果此值为0，表示没有可选参数。 Optional Paramters：此值为BGP可选参数列表，每一个可选参数是一个TLV格式的单元(RFC3392)。 Update Message Format +-----------------------------------------------------+ | Withdrawn Routes Length (2 octets) | +-----------------------------------------------------+ | Withdrawn Routes (variable变长) | +-----------------------------------------------------+ | Total Path Attribute Length (2 octets) | +-----------------------------------------------------+ | Path Attributes (variable变长) | +-----------------------------------------------------+ |Network Layer Reachability Information (variable变长)| +-----------------------------------------------------+ Unfeasible Routes Length:标明Withdrawn Routes部分的长度。其值为零时，表示没有撤销的路由。 Withdrawn Routes:包含要撤销的路由列表，列表中的每个单元包含1字节的Length域和可变长度的Prefix域。 Total Path Attribute Length：标明Path Attributes部分的长度。其值为零时，表示没有路由及其路由属性要通告。 Path_Attributes：含要更新的路由属性列表，按其类型号从小到大的顺序排序，填写更新的路由的所有属性。每一个属性单元包括属性类型，属性长度，属性值三部分。 Network Layer Reachability Information（NLRI）：含要更新的地址前缀列表，每一个地址前缀单元由一个LV二元组（prefix length, the prefix of the reachable route）组成，其编码填写方法与Withdrawn Routes的填写方法相同。 --------------------路由属性的类型号列表------------------------------- 属性类型 属性值 1：Origin IGP EGP Incomplete 2：As_Path AS_SET AS_SEQUENCE AS_CONFED_SET AS_CONFED_SEQUENCE 3：Next_Hop 下一跳的IP地址 4：Multi_Exit_Disc MED用于判断流量进入AS时的最佳路由 5：Local_Pref Local_Pref用于判断流量离开AS时的最佳路由 6：Atomic_Aggregate BGP Speaker选择聚合后的路由，而非具体的路由 7：Aggregator 发起聚合的路由器ID和AS号 8：Community 团体属性 9：Originator_ID 反射路由发起者的Router ID 10：Cluster_List 反射路由经过的反射器列表 14：MP_REACH_NLRI 多协议可达NLRI 15：MP_UNREACH_NLRI 多协议不可达NLRI 16：Extended Communtities 扩展团体属性 Notification Message Format 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Error code | Error subcode | Data (variable变长) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ Error code：占1个字节（无符号位），定义错误的类型，非特定的错误类型用零表示。 Error subcode：占1个字节（无符号位），指定错误细节编号，非特定的错误细节编号用零表示。 Data：指定错误数据内容。 Keepalive Message Format Keepalive 报文只有BGP报文头，没有具体内容，故其报文长度应固定为19个字节。
Refresh Message Format Refresh 报文用于动态的请求BGP路由发布者重新发布Update报文，进行路由更新。
BGP Connection State Idle State
在空闲状态，BGP在等待一个启动事件，启动事件出现以后，BGP初始化资源，复位连接重试计时器（Connect-Retry），发起一条TCP连接，同时转入Connect 状态
在BGP FSM中，发生任何错误，BGP session都将中止会话并返回该状态
Connect State
如果TCP Negotiation (TCP三次握手)成功，BGP将 connect Retry 清零，完成初始化并发送一个 open 消息包给邻居并把自己的状态置为 Open send 状态 如果失败，BGP会继续监听邻居发出的连接，重置 Connect Retry 并将自己状态转移到 Active 状态 如果 Connect Retry 时间超时，将重新开始，再次试图与邻居建立TCP连接，BGP 保持 Connect 状态，出现其他事件转入 Idle 状态 Active State
如果 TCP 连接成功，BGP 将 Connect Retry 清零，完成初始化，给邻居发送Open 消息并将状态置为 Open，hold 时间置为4mins 如果在 Active 状态，Connect Retry 超时回 Connect 状态并重置 Connect Retry 计时器，如果再次连接失败，会导致重回 Idle 状态 OpenSent State
该状态是在三次握手成功后，发送了 open 报文后，等待对端回应 open 报文时的状态
收到open消息，如果发现有差错，将给邻居发送一个 notification 消息并将状态置为 Idle 如果收到 open 包消息校验正确，将发送 keeplive 包给邻居，并建立IBGP或者EBGP状态置为 Open confire state ，否则 如果收到TCP断开消息则断开BGP连接重置 Connect Retry ，状态置为 Active OpenConfirm State
该状态，BGP等待对端的 keepalive 报文，收到后，转入 Established 状态
如果收到一个 keeplive 消息包，会将状态置为 Establish
如果收到 notification 消息包，会将状态置为 Idle 并断开 TCP 连接
Established State
此状态下 BGP 对等体间的连接已经完全建立，此时可以相互交换路由信息，若收到 notification，则会将状态置为 Idle 中断连接
示意图：
BGP Synchronization //拓补图 AS100(RTA) --------- AS200(RTB-RTC-RTD) ---------- AS300(RTE) 当AS100中的RTA发送关于1.1.1.1的路由update包，RTB和RTD运行着IBGP，理想情况下RTD在update包中学习到如何到达1.1.1.1的网络信息，接着RTD传播update包给RTE
假如RTB没有将1.1.1.1的路由信息重发布到IGP中，那么RTC便无法得知如何去往1.1.1.1这段网络，那么RTD和RTE如果向1.1.1.1网段发送数据包，便会在RTC这被丢弃
BGP同步规则：BGP路由器不应该使用或向EBGP邻居通告从IBGP邻居那里学习到的BGP路由信息，除非该路由是本地的或者该路由存在于IGP数据库，即该路由也能从IGP学习到
The BGP synchronization rule states that if an AS provides transit service to another AS, BGP should not advertise a route until all of the routers within the AS have learned about the route via an IGP. BGP Router Server IX/IXP的理念就是帮助不同运营商之间为连通各自网络而建立的集中交换平台，而多个As通过IX/IXP互联则需要在多个As边界路由器之间建立Full的eBGP邻居关系，很大程度上浪费网络资源和本身As边界路由器的Cpu和memory
而引入了BGP Router Server之间，每个As边界路由器只需要与其建立eBGP关系即可
个人感觉BGP Router Server 有点类似与BGP Router Reflector
Reference https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/irg-route-server.pdf Internet Exchange Point ( IX / IXP ) IX history BGP Flowspec BGP Flow Spec可以在BGP路由黑洞的基础上，对流量进行更加细致的分类、限速、过滤和重定向等操作，而不是像路由黑洞那样"一棒打死"
Reference https://github.com/osrg/gobgp/blob/master/docs/sources/flowspec.md https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-3s/irg-xe-3s-book/C3PL-BGP-Flowspec-Client.pdf BMP BMP能够对网络中的设备的BGP运行状态进行实时监控。BMP会话是单向的，即设备向监控服务器上报消息但忽略监控服务器发送的任何消息
Common Header：
0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+ | Version | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Message Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Msg. Type | +---------------+ Message Type
Type = 0: Route Monitoring（RM）：路由监控消息，向监控服务器发送从对等体收到的所有路由，并随时向监控服务器上报路由的新增或撤销 Type = 1: Statistics Report（SR）：向监控服务器上报设备运行状态的统计信息 Type = 2: Peer Down Notification：向监控服务器上报与对等体BGP连接的中断 Type = 3: Peer Up Notification：向监控服务器上报与对等体BGP连接的建立 Type = 4: Initiation Message：初始化消息，向监控服务器通告厂商信息、版本号等 Type = 5: Termination Message：结束消息，向监控服务器通告关闭BMP会话的原因 Type = 6: Route Mirroring Message Reference
BGP Monitoring Protocol (BMP) BFD BFD（Bidirectional Forwarding Detection，双向转发检测）是一种快速故障检测机制，常用于链路的连通性检测，可以为各种上层协议（静态路由、Track等）快速检测设备之间的路径故障
BGP安全性 BGP GTSM（Generalized TTL Security Mechanism）
BGP GTSM检测IP报文头中的TTL（time-to-live）值是否在一个预先设置好的特定范围内，并对不符合TTL值范围的报文进行允许通过或丢弃的操作
BGP 认证
MD5认证、Keychain认证
BGP RPKI
RPKI（Resource Public Key Infrastructure）通过验证BGP路由起源是否正确来保证BGP的安全性。
RPKI主要用来解决路由传递过程中攻击者通过发布更精确的路由导致数据传输过程中流量被窃取的问题。例如，某运营商向用户发布了目的地址为10.10.0.0/16的路由，路由攻击者发布了目的地址为10.10.153.0/16的路由，由于攻击者发布的路由更详细，从用户端发往10.10.153.0/16的路由将被攻击者窃取。
为了解决上述问题，路由器与RPKI服务器建立连接，将路由起源的相关数据ROA（Route Origination Authorization）保存到本地。通过验证从邻居收到的BGP路由是否合法来控制选路结果，从而确保域内的主机能够安全地访问外部服务。
Route Flapping 路由不稳定的主要表现形式是路由振荡（Route Flapping），即路由表中的某条路由反复消失和重现
发生路由振荡时，路由器就会向邻居发布路由更新，收到更新报文的路由器需要重新计算路由并修改路由表。所以频繁的路由振荡会消耗大量的带宽资源和CPU资源，严重时会影响到网络的正常工作
路由衰减（Route Dampening）用来解决路由不稳定的问题。多数情况下，BGP协议都应用于复杂的网络环境中，路由变化十分频繁。为了防止持续的路由振荡带来的不利影响，BGP使用路由衰减来抑制不稳定的路由
BGP衰减使用惩罚值（Penalty Value）来衡量一条路由的稳定性，惩罚值越高则说明路由越不稳定。路由每发生一次振荡，BGP便会给此路由增加一定的惩罚值（路由从激活状态变为未激活状态，惩罚值增加1000，路由在激活状态，收到新的路由更新，惩罚值加500）。当惩罚值超过抑制阈值（Suppress Value）时，此路由被抑制，不加入到路由表中，也不再向其他BGP对等体发布更新报文
被抑制的路由每经过一段时间，惩罚值便会减少一半，这个时间称为半衰期（Half-life）。当惩罚值降到再使用阈值（Reuse Value）时，此路由变为可用并被加入到路由表中，同时向其他BGP对等体发布更新报文。上文提到的惩罚值、抑制阈值和半衰期都可以手动配置
路由衰减只适用于EBGP路由。对于从IBGP收来的路由不能进行衰减，因为IBGP路由经常含有本AS的路由，内部网络路由要求转发表尽可能一致，IGP快速收敛就是为了达到信息同步，转发一致。如果衰减对IBGP路由起作用，不同路由器的衰减参数不一致时，会导致转发表不一致
BGP Auto FRR BGP Auto FRR（Auto Fast ReRoute）是一种链路故障保护措施，应用于有主备链路的网络拓扑结构中。使能BGP Auto FRR，可以使BGP的两个邻居切换或者两个下一跳切换达到亚秒级的收敛速度
BGP Auto FRR对于从不同对等体学到的相同前缀的路由，利用最优路由作为主链路进行转发，并自动将次优路由作为备份链路。当主链路出现故障的时候，系统快速响应BGP路由不可达的通知，并将转发路径切换到备份链路上
BGP Aggregate 将多个特定路由聚合为一个路由，可能会有路径属性丢失。 可分为手动聚合和自动聚合 为了避免路由聚合可能引起的路由环路，BGP设计了AS_Set属性。AS_Set属性是一种无序的AS_Path属性，标明聚合路由所经过的AS号。当聚合路由重新进入AS_Set属性中列出的任何一个AS时，BGP将会检测到自己的AS号在聚合路由的AS_Set属性中，于是会丢弃该聚合路由，从而避免了路由环路的形成。
BGP GR and NSR BGP的平滑重启GR（Graceful Restart）和不间断路由NSR（Non-Stop Routing）作为高可靠性的解决方案，其根本目的都是为了保证用户业务在设备故障的时候不受影响或者影响最小
GR机制的核心在于：当某设备进行协议重启时，能够通知其周边设备在一定时间内将到该设备的邻居关系和路由保持稳定。在协议重启完毕后，周边设备协助其进行信息（包括支持GR的路由/MPLS相关协议所维护的各种拓扑、路由和会话信息）同步，在尽量短的时间内使该设备恢复到重启前的状态。在整个协议重启过程中不会产生路由振荡，报文转发路径也没有任何改变，整个系统可以不间断地转发数据。这个过程即称为平滑重启。
GR基本概念 配置了GR功能的设备称为“具备GR能力”的设备。具备GR能力的设备在协议重启时，能实现平滑重启，保证转发业务不中断；而不具备GR能力的设备在协议重启时，则只能遵循普通的重启过程。GR中涉及到的基本概念如下：
GR Restarter GR重启路由器，指由管理员或故障触发而协议重启的设备，它必须具备GR能力。 GR Helper 即GR Restarter的邻居，能协助重启的GR Restarter保持路由关系的稳定，它也必须具备GR能力。 GR Session GR会话，是GR Restarter和GR Helper之间的协商过程。包括协议重启通告，协议重启过程中的信息交互等。通过该会话，GR Restarter和GR Helper可以掌握彼此的GR能力。 GR Time GR时间，是GR Restarter和GR Helper协商建立一个会话所用的时间。当某GR路由器发现邻居路由器处于down状态时，将在该时间内仍保留其发出的拓扑或路由信息。 BGP GR的过程是： 利用BGP的能力协商机制，GR Restarter和GR Helper了解彼此的GR能力，建立有GR能力的会话。 当GR Helper检查到GR Restarter重启或者主备倒换后，不删除和GR Restarter相关的路由和转发表项，也不通知其他邻居，而是等待重建BGP连接。 GR Restarter在GR Time超时前与重启前的所有GR Helper新建立好邻居关系。
BGP NSR（Nonstop Routing，不间断路由）是一种通过在BGP协议主备进程之间备份必要的协议状态和数据（如BGP邻居信息和路由信息），使得BGP协议的主进程中断时，备份进程能够无缝地接管主进程的工作，从而确保对等体感知不到BGP协议中断，保持BGP路由，并保证转发不会中断的技术。
BGP 定时器 Advertisement Interval：发布路由通告的间隔。在这个间隔内的事件会被缓存，然后时间到了一起发送 Keepalive and Hold Timers：每个节点会向它的 peer 发送心跳消息。如果一段时间内（称为 hold time）没收到 peer 的心跳，就会清除所有从这个 peer 收到的消息 Connect Timer：节点和 peer 建立连接失败后，再次尝试建立连接之前需要等待的时长 BGP Tracking 为了实现BGP快速收敛，可以通过配置BFD来探测邻居状态变化，但BFD需要全网部署，扩展性较差。在无法部署BFD检测邻居状态时，可以本地配置BGP Peer Tracking功能，快速感知链路不可达或者邻居不可达，实现网络的快速收敛。 通过部署BGP Tracking功能，调整从发现邻居不可达到中断连接的时间间隔，可以抑制路由震荡引发的BGP邻居关系震荡，提高BGP网络的稳定性。
BGP Redistributes Routes 一种协议收到的路由以另一种协议再发送出去，称为路由再分发（redistributing routes）
MP-BGP MP-BGP EVPN BGP EVPN路由类型 传统的BGP-4使用Update报文在对等体之间交换路由信息。一条Update报文可以通告一类具有相同路径属性的可达路由，这些路由放在NLRI（Network Layer Reachable Information，网络层可达信息）字段中。
因为BGP-4只能管理IPv4单播路由信息，为了提供对多种网络层协议的支持（例如IPv6、组播），发展出了MP-BGP（MultiProtocol BGP）。MP-BGP在BGP-4基础上对NLRI作了新扩展。玄机就在于新扩展的NLRI上，扩展之后的NLRI增加了地址族的描述，可以用来区分不同的网络层协议，例如IPv6单播地址族、VPN实例地址族等。
类似的，EVPN在L2VPN地址族下定义了新的子地址族——EVPN地址族，并新增了一种NLRI，即EVPN NLRI。EVPN NLRI定义了以下几种BGP EVPN路由类型，通过在EVPN对等体之间发布这些路由，就可以实现VXLAN隧道的自动建立、主机地址的学习。
Type2路由——MAC/IP路由：用来通告主机MAC地址、主机ARP和主机路由信息。 Type3路由——Inclusive Multicast路由：用于VTEP的自动发现和VXLAN隧道的动态建立。 Type5路由——IP前缀路由：用于通告引入的外部路由，也可以通告主机路由信息。 EVPN路由在发布时，会携带RD（Route Distinguisher，路由标识符）和VPN Target（也称为Route Target）。RD用来区分不同的VXLAN EVPN路由。VPN Target是一种BGP扩展团体属性，用于控制EVPN路由的发布与接收。也就是说，VPN Target定义了本端的EVPN路由可以被哪些对端所接收，以及本端是否接收对端发来的EVPN路由。
VPN Target属性分为两类：
Export Target（ERT）：本端发送EVPN路由时，将消息中携带的VPN Target属性设置为Export Target。 Import Target（IRT）：本端在接收到对端的EVPN路由时，将消息中携带的Export Target与本端的Import Target进行比较，只有两者相等时才接收该路由，否则丢弃该路由。 Type 2 类型路由 Type 3 类型路由 Type 5 类型路由</content></entry><entry><title>nginx Compilation parameters</title><url>/post/%E8%AE%B0%E5%BD%95%E8%B4%B4nginx-compilation-parameters/</url><categories><category>nginx</category></categories><tags/><content type="html"> 记录一下一些常见的nginx Compilation parameters。
基础配置 --prefix=_path_
设置nginx安装目录路径
--sbin-path=_path_
设置nginx可执行程序文件目录路径
--modules-path=_path_
设置nginx模块目录路径
--conf-path=_path_
设置 nginx.conf 配置文件路径
--error-log-path=_path_
设置nginx 错误日志文件路径
--pid-path=_path_
设置 nginx.pid 文件路径
--lock-path=_path_
设置 nginx.lock 文件路径
--user=_name_
设置用户
--group=_name_
设置用户组
模块配置 零散 --with-select_module 、``--without-select_module
是否使用IO多路复用中的select()。
--with-poll_module、``--without-poll_module
是否使用IO多路复用中的poll()。
--with-threads
开启线程池。
--with-http_ssl_module
开启HTTPS，具体配置请看HTTPS protocol support ，依赖于openssl。
--with-http_v2_module
开启并支持HTTP/2。
--with-http_realip_module
ngx_http_realip_module 模块，ngx_http_realip_module模块可以在反向代理中，返回真实用户IP给真实服务器，否则返回的是nginx服务器IP。
--with-http_addition_module
ngx_http_addition_module 模块。作用在响应前后增加或者删除一些字段。
--with-http_image_filter_module --with-http_image_filter_module=dynamic
ngx_http_image_filter_module 模块。用于不同格式的图片转换。
--with-http_sub_module
ngx_http_sub_module 模块，可修改指定字符串。
--with-http_dav_module
官方文档翻译过来说是使用webdav对文件管理自动化，具体用法不清楚。
--with-http_flv_module
ngx_http_flv_module 模块，为Flash视频（FLV）文件提供伪流服务器端支持。
--with-http_mp4_module
ngx_http_mp4_module module。提供对mp4文件的流式服务的支持。
--with-http_gunzip_module
enables building the ngx_http_gunzip_module 模块。为那些不支持gzip压缩的用户提供类似于gzip的服务，有助于减少数据传输量。
--with-http_gzip_static_module
ngx_http_gzip_static_module 模块。使用“gzip”方法压缩响应的过滤器。这通常有助于将传输数据的大小减少一半甚至更多。依赖于zlib
--with-http_auth_request_module
ngx_http_auth_request_module 模块。提供对于请求的访问控制和Auth认证协议。
--with-http_random_index_module
ngx_http_random_index_module 模块，选取设定目录中的随机文件作为索引文件。玩法：同一网站不同主页。
--with-http_secure_link_module
ngx_http_secure_link_module 模块.，用于检查请求的链接的真实性，保护资源免受未经授权的访问并限制链接的寿命。
--with-http_stub_status_module
ngx_http_stub_status_module 模块，提供了基本访问信息的查询。
fastcgi 、uwsgi 、scgi 模块临时文件路径 --http-fastcgi-temp-path=_path、_``--http-uwsgi-temp-path=_path、_``--http-scgi-temp-path=_path_
--without-http_fastcgi_module、``--without-http_uwsgi_module、``--without-http_scgi_module
stream模块 用来实现四层协议的转发、代理或者负载均衡
--with-stream
--with-stream=dynamic
--with-stream_ssl_module
--with-stream_realip_module
--with-stream_geoip_module --with-stream_geoip_module=dynamic
--with-stream_ssl_preread_module
pcre The library is required for regular expressions support in the location directive and for the ngx_http_rewrite_module module.
--without-pcre、``--with-pcre、``--with-pcre=_path、_``--with-pcre-opt=_parameters、_``--with-pcre-jit
zlib The library is required for the ngx_http_gzip_module module.
--with-zlib=_path_
--with-zlib-opt=_parameters_
--with-zlib-asm=_cpu_
enables the use of the zlib assembler sources optimized for one of the specified CPUs: pentium, pentiumpro.
openssl --with-openssl=_path、_``--with-openssl-opt=_parameters_
Perl语言相关 --with-http_perl_module、``--with-http_perl_module=dynamic、``--with-perl_modules_path=_path、_``--with-perl=_path_
邮件服务相关 --with-mail、``--with-mail=dynamic、``--with-mail_ssl_module、``--without-mail_pop3_module、``--without-mail_imap_module、``--without-mail_smtp_module
第三方模块添加 --add-module=_path、_``--add-dynamic-module=_path_
编译安装 输出下面命令，选择你所需要的模块
./configure &ndash;prefix&hellip;&hellip;&hellip;&hellip;
接着就make &amp;&amp; make install。注意make install 会把原来安装nginx的路径全部覆盖，建议先进行备份。
Reference http://nginx.org/en/docs/configure.html https://cloud.tencent.com/developer/doc/1158</content></entry><entry><title>Go的几种编译模式</title><url>/post/go-compile-mode/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> Go与C家族的互调，Go的几种编译模式。
一个程序编译成可执行程需要经过哪些 会经过4个步骤：
预编译：又称为预处理，是做些代码文本的替换工作。是整个编译过程的最先做的工作，生成**.i文件**。参考 编译：主要对代码进行语法、语义等分析。检查无误后，翻译成二进制机器指令，生成汇编代码，将.i文件转化成**.s文件**。参考 汇编：把上述的汇编代码翻译成二进制机器指令，将.s文件转化成**.o文件**。 链接：又分为静态链接和动态链接，将众多的.o合成一个完整的可执行文件（静态/动态库文件）&mdash;Windows下为**.lib和.dll文件(lib是编译时需要的，dll是运行时需要的。)，Linux下为.a和.so**文件。 注：不同平台下，生成文件可能不同。例如window下，在汇编阶段会产生.obj文件。
go 编译模式 关于go build 这个工具集是常用工具，能够将所写的代码打包编译成可执行文件(类型取决于所处平台)，也可以加上-buildmode指定编译模式。
-buildmode参数 -buildmode=archive 原版文档解释：This is the default build mode for a package that is not main. It means to build the package into a .a file. This is already supported for all targets. 个人理解：这个参数主要用来将那么不是名为main的.go编译成.a文件，不过这种方式生成的.a文件。目前尚未弄清生成在哪个文件夹下。 -buildmode=default 原版文档解释： Listed main packages are built into executables and listed non-main packages are built into .a files (the default behavior). 个人理解：这个是默认情况下buildmode所采取的方法，其最终生成一个可执行文件和把除main.go外的其他.go编译成.a文件 -buildmode=shared： 这个参数主要编译成一个.so文件、.a文件，不过要在命令后面配上-linkshared参数，可以参考一下这篇文章 。 -buildmode=c-archive （ C动态链接文件） 原版文档解释：Build the listed main package, plus all packages it imports, into a C archive file. The only callable symbols will be those functions exported using a cgo //export comment. Requires exactly one main package to be listed. 注意：在使用go文件编译成能让C/C++引用的文件时，要主要必须要有的2个要求：第一是要在头部加入import &ldquo;C&rdquo;。第二是要在你想让C/C++引用的函数上方加入 //export 你的函数名。 产生文件：.a 、.h。.h文件可以在C/C++引用![](Go compile mode/20200826095638331.png) 参考链接-stackoverflow -buildmode=c-shared 原版文档解释：Build the listed main package, plus all packages it imports, into a C shared library. The only callable symbols will be those functions exported using a cgo //export comment. Requires exactly one main package to be listed. 大体上和c-archive模式差不多，主要是C archive file和C shared library的区别。按字面意思是file和library的区别。 产生文件：一个文件(不知用来干啥)，.h文件供C/C++调用。![](Go compile mode/20200826095532942.png) 关于buildmode的参数还有exe、pie、plugin，这些参数就不展开多说。可以通过go help buildmode查看文档解析。
C调Go、Go调C例子 C调Go //A.go import "C" import "fmt" //export A func A() { fmt.Println("From DLL: A!") } func main() {}
通过**go build -buildmode=c-archive -o ××.a A.go ** 生成A.a 和 A.h文件。然后编写.c文件
#include "h.h" #include &lt;stdio.h> int main() { Bar(); return 0; } 编译C+运行：
gcc -o main hello.c h.a -undefined reference to \`pthread\_create' 注：这里的-lpthread参数还需研究，就目前来看是能够调用成功。与其相关的还有一个-pthread参数，仍需自行查阅相关资料。 就目前来看-lpthread是与POSIX(可移植操作系统接口) thread相关 ![](Go compile mode/20200828140739493.png) 注意：上述只是个简单的例子，具体更复杂的问题涉及到C与Go直接的值范围等到一些问题还需要注意，因此在调用中更应该注重两种语言之间的类型转换。
Go调C 主要是运用了Go实现的CGO库调用，参考及内部实现：链接 加油！
Reference 静态链接(Static link) 动态链接(Dynamic Linking) Go Execution Modes</content></entry><entry><title>当TCP出现了大量的短链接时......</title><url>/post/%E5%BD%93tcp%E5%87%BA%E7%8E%B0%E4%BA%86%E5%A4%A7%E9%87%8F%E7%9A%84%E7%9F%AD%E9%93%BE%E6%8E%A5%E6%97%B6/</url><categories><category>TCP/IP</category></categories><tags><tag>TCP/IP</tag></tags><content type="html"> 问题 在一段时间里，有大量的短链接请求服务器，服务器会发生什么情况？
time_wait状态与可能导致情况 当TCP进入了四次挥手阶段，当主动请求关闭连接时就会在接下来的过程中进入time_wait状态，其时长为2msl。具体就不多说，TCP三次握手和四次挥手都是基础知识。 那么会不会出现一种情况，当大量的短链接请求过来，但每条短链接请求的数据量都比较小，服务器飞速的处理完毕，这时候没有发生任何的错误，但是仍要等待2msl时间才能关闭结束TCP连接。因此思考一下，如果服务器处理时间为万分之2msl，那么这个在不发生任何包丢失的情况下，这个等待2msl时间是不是有点不合理？ 因此在有无数个短链接请求过来时，大量的socket都停留在了time_wait状态导致服务器里的socket数量爆满(换句话说必须要等到2msl后才能释放)。
可能会发生的情况 在linux中，有着对这种情况的解释，本人通过man tcp的命令找到里面对一个字段tcp_max_tw_buckets解释如下图 这段话说明了当linux中处于time_wait状态的socket一旦超过规定的值，就会导致部分socket关闭。
解决方法 关于解决方法网上已经有很多，不过主要还是涉及到几个参数分别为tcp_tw_reuse、tcp_tw_recycle，前者是对time_wait socket重用，后者是对 time_wait socket的快速回收。 可以通过linux参数说明得知，当启动tcp_tw_recycle可以对socket快速回收但是当网络是在NAT的情况下，就可能会引起一系列问题，因此该参数要慎用。 不过一般情况下，服务器都是作为被动连接的一方，而tcp_tw_reuse这个参数是否开启主要还是基于你是主动发起还是被动连接，因为time_wait状态只出现在主动发起一方。 注：查看当前TCP状态：netstat -an grep tcp
Reference kernel.org (相关参数) 以讹传讹的“tcp_tw_reuse”</content></entry><entry><title>Go interface的interfacetype与_type</title><url>/post/%E5%85%B3%E4%BA%8Ego-interface%E7%9A%84interfacetype%E4%B8%8E-type/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 题目一 ........ type T interface{} type X string type Y = string func main() { var t T = "abc" var x X = "abc" var y Y = "abc" fmt.Println(t == x) fmt.Println(t == string(x)) fmt.Println(t == y) fmt.Println(t == string(y)) } // 输出false true true true interface的内部结构 总所周知，在Go里面任何类型都实现了空接口，而非空接口则必须要实现其里面的方法才算是实现了这个非空接口。 当比较t==x时，t的动态类型是string，而x的动态类型是
空接口结构 type eface struct { _type *_type data unsafe.Pointer } 空接口的结构也十分简单，_type是指向Go内部所定义的类型系统，在这就不展开述说，而data这指向接口的数据。
非空接口结构 非空接口的构造相对复杂，这里就贴个本人画的图： 由此可见，在Go中的interface内部构造还是挺复杂的。
解题 解决题目的关键是要理解Go interface。
前提 比如说请看下面一题
var rw io.ReadWriter v, _ := os.Open("test.txt") rw = v fmt.Printf("%T", rw) //*os.File 问题来了，我们用一个var定义了一个io.ReadWriter类型的rw，之后一系列操作结果显示rw的类型为*os.File 这里就涉及到iface结构里面的interfacetype与_type这俩个字段了。interfacetype字段是指向接口所定义的类型信息，也可以理解为一个静态类型。而_type字段则是接口实际指向的类型信息，也就是你人为的手动的赋值(也可称动态类型)。 由此这就可以解释清楚了，在Go里面所有类型都实现了空接口，而用var定义的rw初始的interfacetype指向了io.ReadWriter类型，之后通过os包打开了一个文件v，其类型为*os.File。但又赋给rw，这时rw的_type字段便指向了*os.File类型。因此最后结果就为*os.File。
题目一的解法 首先分析一下三个type，type X string只是一个自定义类型，底层类型还是string，但X与string是不同类型的。而type Y = string，Y只是string的一个别名。 根据Go语言规范里面的一句话：
A value `x` of non-interface type `X` and a value `t` of interface type `T` are comparable when values of type `X` are comparable and `X` implements `T`. They are equal if `t`'s dynamic type is identical to `X` and `t`'s dynamic value is equal to `x`. 再结合上述所说的interface内部构造，可以得知t的动态类型(_type)为string，动态值为abc，而x动态类型(_type)为X，动态值为abc，因此 t != x 。
Reference The Go Programming Language Specification(Go语言规范) 题目来源</content></entry><entry><title>Thread-Caching Malloc</title><url>/post/%E8%81%8A%E8%81%8Athread-caching-malloc/</url><categories><category>memory</category></categories><tags/><content type="html"> 前言 关于TCMalloc的相关资料和讲解，网上有很多关于其细节文章，在这就不一一阐述。主要目的是大体的结合官方文档和图片来阐述，但是具体细节个人是希望通过查阅资料去理解，最重要是靠个人理解。
图解 虽然图是在网上借鉴的，但个人查阅相关资料，并图上加了一点自己的注释。 Central Free Lists 、Thread Cache Free Lists 关于Central Free Lists 、Thread Cache Free Lists这两个list分别对应着CentralCache和ThreadCache。CentralCache的主要工作是管理分配回收ThreadCache中的span。而ThreadCache的创建与销毁都是随着线程的创建与销毁进行的。 初始化的主要工作是对PageHeap、CentralCache初始化、分配器和Size Class的初始化。当有线程需要申请内存时，ThreadCache会分配内存给它，如果不够就去CentralCache取。
内存回收 关于内存回收这方面，建议参考文档和网上的一些资料，这方面偏理解。
内存碎片 可以参考这片文章，注意里面的关键词语“对齐”&ndash;也就是所谓的内存对齐(可以参考我这篇博客 Data Structure Alignment )，链接：点击 思考：TCmalloc的好处 在上面的第一张图，可以知道TCmalloc给小对象做了三层的缓存服务，如果不够就会一步一步的向后面的服务获取内存，而向PageHeap、CentralCache内的内存都是提前向系统申请好的。因此线程不需要直接通过系统调用申请，而是申请TCmalloc所分配好的内存，从而使得整个过程都处于一种用户态的形式，不需要进入内核态。比如C函数库中的内存分配函数malloc()就涉及到一次用户态到内核态之间的切换。通过以这种形式来提供性能。
应用 Go的内存模型也是基于TCmalloc的设计思想。不过我在百度百科 上面还发现了TCmalloc还可以应用在MySQL上，让它在高并发下内存占用更加稳定，这也是对mysql的一种不错的优化。
Reference https://gperftools.github.io/gperftools/tcmalloc.html https://github.com/google/tcmalloc https://github.com/google/tcmalloc/blob/master/docs/design.md https://wallenwang.com/2018/11/tcmalloc/#ftoc-heading-36</content></entry><entry><title>go 内存对齐</title><url>/post/data-structure-alignment/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 问题 在下面2个结构体所占的字节数又为多少？
type A struct{ a int8 d int16 c int32 b int64 } type B struct{ a int8 c int32 b int64 d int16 } 答案 答案显而易见，如果仔细观察就会发现A、B两个结构体中的字段类型排序不同。这就涉及到了Data Structure Alignment(也就是内存对齐)的知识点了。
32位与64位的区别 在了解什么是内存对齐前，首先要知道CPU、内存、地址总线、数据总线的概念。当cpu想要从内存中获取数据，就要通过地址总线传送地址给内存，内存在根据传送过来的地址得到数据，最后将数据从数据总线传给cpu。通常8根数据总线就寻址255byte的内存空间，而平常我们所说的32位和64位实际上是代表32根/64根数据总线，它们所对应可寻址空间位2^32和2^64。也就是说64位的机子当对于32位的机子的寻址空间更大了。
什么是Data Structure Alignment 内存条的bank，chip，rank (这里偏理解的东西，个人感觉不好口述，还是贴一下大佬的图（会在下面Refrence注明）) 而8个Bank所对应的同一个位置的字节(8个位置)连起来就称为连续8个字节(这里标注为a0到a8)。但是如果不是从a0开始，而是从a1到a9呢？这时候就不能在一次操作中一次取出，而是通过二次操作，第一次取a1到a8，第二次取a9。这样就浪费了一次cpu的操作，而且更极端的情况就是一个占2字节的值分别在a8，a9上，这样不仅消费cpu，更浪费内存(因此明明可用a0、a1来解决同时又可以在一次操作完全取出)。所以编译器就会给不同类型的数据安排适合的内存地址，以此来提高性能。简单来说就是争取在cpu一次操作(操作8个字节)中，一次性读取出想要的数据。 因此Data Structure Alignment(内存对齐)出现便是作用于此。注意不同平台每种类型的对齐边界都会有所不同。
题目图解(64位window下) Reference Data Structure Alignment in Wikipedia Is it necessary to use Data structure alignment Bilibili</content></entry><entry><title>Go Race Detector</title><url>/post/%E5%85%B3%E4%BA%8Ego-race-detector/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 碰巧在看Go question的PDF时，意外发现在其上面所推荐的博客上看到一篇关于map 并发崩溃一例 的文章，他所使用的检查bug方法引起了我的注意和回想。
前言 在那篇文章中，他所使用检测bug命令是go run -race ×.go。当时-race就引起了我的注意，因此在前几篇关于Go的文章中，在阅读源码的过程中我经常看到这个关于race包的引用，当时我是不知道这里这一小段的代码具体作用是干啥的，直到今晚看到这篇关于map 并发崩溃一例 的文章我才知道其的实际作用。 下面随手贴几段关于race包在各处的代码
//chan.go func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if raceenabled { if c.dataqsiz == 0 { racesync(c, sg) } else { qp := chanbuf(c, c.recvx) raceacquire(qp) racerelease(qp) raceacquireg(sg.g, qp) racereleaseg(sg.g, qp) c.recvx++ ....... } } ....... } func closechan(c *hchan) { ....... if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, funcPC(closechan)) racerelease(c.raceaddr()) } ......... } //rwmutex.go func (rw *RWMutex) RLock() { if race.Enabled { _ = rw.w.state race.Disable() } ..... if race.Enabled { race.Enable() race.Acquire(unsafe.Pointer(&amp;rw.readerSem)) } } 可以看出race包在Go源码中是十分常见的。
race是什么 根据官网文档，-race选项用于检测数据竞争。在开启了race检测之后，如果程序发生了例如map的竞争，它会一层一层的将错误打印出来。常用于并发程序的检测。具体的操作和说明在这就不多说，请移步到下面的参考文献查阅。
使用方法 race检测已经添加到go tool chan中，可以在命令行中加入-race，即可开启。 类如：go run -race 、go test -race等 参考文献及案例
The Go Blog-Introducing the Go Race Detector Data Race Detector</content></entry><entry><title>wordpress backup</title><url>/post/%E5%85%B3%E4%BA%8Ewordpress-backup%E9%97%AE%E9%A2%98/</url><categories><category>wordpress</category></categories><tags><tag>wordpress</tag></tags><content type="html"> 需求 对于个人而言，关于wordpress站点的备份，主要还是使用的内置的wordpress-importer来对站点里的内容进行备份，它会把站点里面的数据存到一个xml文件中。相对于其他备份，我只需将重要的部分(文章，分类标签等)保存起来，然后再通过wordpress-importer导回另外一个站点，而且xml文件也比较小，占用空间不大。
操作 wordpress官网也提供相关的CLI供我们使用，具体下载链接为：官网链接 。对于命令行的导出，可以使用wp export + 参数 的方式进行导出。相应的命令参数可以去Github上查看文档&mdash;-wp-cli/export-command ，wp-cli 这里就不多说了。 个人思路是通过脚本定时执行wp export命令导出xml文件，再上传到其他个人服务器上面去。
坑 在操作wp export遇到了几个坑，第一点是当前目录是否存在wp-config.php，看过wordprss后台数据库的都知道，wordpress把文章都存在wp-post的表中，而数据库的连接是在wp-config.php中设置的，wordpress-importer应该就是找到wp-config.php从数据库中提取数据打包成xml文件。因此确保在执行wp export命令时当前目录要有wp-config.php文件。 第二点是关于执行wp export之后，报错信息为
PHP Fatal error: Uncaught Error: Class 'DOMDocument' not found in phar:///usr/local/bin/wp/vendor/nb/oxymel/Oxymel.php:147 Stack trace: 这点首先确定问题为PHP的DOMDocument的事，具体相应的解决方法在Github 上也找到了。
解决方案 解决方法可以去我GitHub 上面查看(注:上面的Go脚本是我随手编写的，在实际中我已经改成用shell编写了，可以无视！)，下面放一些效果图 可以看到文章，分类标签等通过wp-cli导出来也就164KB(对应着目前本文章之前的文章等资料)，因此也十分让我满意。
总结 关于wordpress站点的备份目前的思路大概也比较清楚，主要是对于wp-cli的一些坑需要留意。</content></entry><entry><title>nginx epoll</title><url>/post/%E5%AF%B9epoll%E7%9A%84%E7%90%86%E8%A7%A3/</url><categories><category>nginx</category></categories><tags><tag>nginx</tag></tags><content type="html"> 前言 最近在调试个人博客时，发现服务器上的nginx居然没有配置epoll事件驱动，在之前学习Go的net包时，也接触过底层源码，也了解过其底层也是使用了epoll。为此我也专门去了解epoll，还有select poll。
Nginx配置epoll 在nginx.conf下配置事件驱动模型epoll
events { accept_mutex on; #设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; #设置一个进程是否同时接受多个网络连接，默认为off use epoll; #事件驱动模型，selectpollkqueueepollresig/dev/polleventport worker_connections 1024; #最大连接数，默认为512 } 特别要注意一点在events里的accept_mutex配置涉及到一个Thundering herd problem (惊群问题)需要我们留意。 其他的一些配置就不再多说，配置也比较简单，主要还是走官网文档。记得修改过后要重启nginx
关于epoll 其实epoll、select、poll都是I/O多路复用的机制，本质上还是同步I/O需要自己负责进行读写。而select、poll都是出现在epoll之前，select的机制就是使用轮询的方式扫描fd，同时在其过程还涉及到内核态与用户态来回复制fd，效率低下。poll的机制原理与select十分相似，主要一点不同是poll采取fd的结构是链式，没有最大连接数限制，而select的最大连接数默认为1024。 关于epoll的一些组成在这就不必多说，网上都有，关键是要理解其内部的各组件的作用。
API 关于epoll，它有三个API分epoll_create、epoll_ctl、epoll_wait，原本对于这三个API我还有不太理解除了epoll_create。于是Google了一下，发现里面大有文章。 在epoll_create会在内核产生一个epoll的fd，而之后的epoll_ctl、epoll_wait都是以它为中心进行操作，epoll_ctl可以理解为一个事件监听器，它可以把需要监听的事件全都注册进epoll的fd中，并用红黑树进行存储。如果事件触发，那么epoll会从这个事件插入到一个链表中，最后通过epoll_wait进行调用。
源码 关于源码部分，我在Github上找到一个epoll的源码分析，里面分析的十分详细，不仅包括了完整源码，而且还分析了epoll是如何得知fd的状态变化的，具体深入到内核的poll技术，可以去参考一下这里 。</content></entry><entry><title>Copy-on-write</title><url>/post/%E6%B5%85%E8%B0%88copy-on-write/</url><categories/><tags/><content type="html"> 最近在看到了一种Copy-on-write(写时复制，COW)的技术，因此在这做一下记录
概念 根据Wikipedia和StackOverflow上面的描述，COW是一种计算机资源管理方面的优化技术，目前我所了解到用到的地方有redis持久化、docker的AUFS文件系统等。
过程 目前有一个数据块A，进程1、2、3、4都想访问数据块A，这时候进程1234都是以只读的方式去读取数据A。在cow中，比如进程2想要修改数据块A，这时候触发cow的机制，此时数据块A会被复制一份（这里为了方便区别我们称为数据块B）出来给进程2修改（注意这时候已经有2份数据块A分布在2个不同的内存地址中），而其他进程134不会受到任何影响仍在读取数据块A，而进程2现在在读取数据块B。
Linux的Copy-on-write linux的cow主要是在进程方面，主要涉及fork()和exec()，通过fork产生一个子进程。传统的fork会把父进程的所有资源（代码段、数据段、堆、栈）都copy一份给子进程，而使用了cow机制的fork不会复制整个父进程的资源，而是父进程和子进程都共用一个资源，只有当子进程需要写入数据时，才会复制一份给子进程，在此之前都是以只读方式来共享资源。
redis的Copy-on-write redis的写时复制主要在其持久化这里，当redis要执行持久化时，其会fork一个子进程来从内存中读取数据，写到磁盘上。如果这时候redis要更新数据或者写操作，那么就会触发cow，开辟一个新的内存块让更新的数据存放。当然如果子进程存在期间发生大量的更新或者写操作，那么此时机子很可能将耗费比较多的性能在copy的操作上。附：《Redis设计与实现》 总结 对于Copy-on-write，主要还是要理解其思想，下面是摘录自Wikipedia的概括： 其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的（transparently）。此作法主要的优点是如果调用者没有修改该资源，就不会有副本（private copy）被建立，因此多个调用者只是读取操作时可以共享同一份资源。</content></entry><entry><title>Explicit Congestion Notification (ECN) in IP</title><url>/post/explicit-congestion-notification-ecn-in-ip/</url><categories><category>TCP/IP</category></categories><tags><tag>TCP/IP</tag></tags><content type="html"> TCP/IP的标记位
IP的ECN(显式拥塞通知) ECN使用IP首部的Tos字段最后2位(也称最低有效位)，根据RFC 3168 ，可以得知这2位分别成为ECT和CE，ECT由发送方来设定是否支持ECN，而CE是由路由器来设定，通知接收方当前拥塞，这也意味着路由器要支持ECN。 ECT和CE有4种不同的组合00、01、10、11分别表示不同的状态。
00表示不支持ECN传输(Not-ECT) 01和10表示支持ECT传输——ECT(0)、ECT(1) 11表示发生拥塞(CE)。 TCP的ECN TCP头中新增三个flag来支持ECN，分别是Nonce Sum(NS)、ECN-Echo(ECE)和Congestion Window Reduced(CWR)。 NS用于防止TCP发送者的数据包标记被意外或恶意改动，而另外2位则是回传拥塞指示。当接收者收到IP报文，发现IP头部的ECN位为11，则会通知上层标记ECE位用于通知发送方减少发送速率。而发送方收到这个标记了ECE位的包后，发送速率减半，同时也会标记CWR位用于通知接收方已经确认阻塞指令。
附一张图：
ECN与现有的TCP拥塞控制 现有的TCP拥塞控制都是在中间路由器丢包之后，TCP协议根据RTO超时来重传丢失的包，但是对于主机之间的TCP连接来说它不知道当前发送的链路正在拥塞，它要等待RTO计时器超时后开始重传报文和降低发送速率，这个RTO计时器超时过程花费个几秒或者十几秒等。(网络波动从轻载-过载-轻载) 而ECN机制可以让TCP发送端能够感知当前链路的拥塞程度，以此通过设置ECE标记来通知对方要降低发送速率(依靠路由器设置ECN位为11)，而不是静静的等待RTO超时再发送数据。
补充另外几个标记位 URG:一个正向偏移值，表示在包中的第几个字节开始有紧急数据要处理，直接交给上层协议，不需要经过缓冲区 PSH:表示需要将收到的数据立即交给上次协议，但是不需要等到整个缓冲区填满，直接上交，经过缓冲区。</content></entry><entry><title>go sync.Mutex</title><url>/post/go-sync.mutex/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> Mutex mutex有2种操作模式：正常和饥饿。在正常模式下，等待者按FIFO顺序排队，但唤醒的等待者不能马上获取锁，它还要和刚到达的goroutine们竞争，而且刚到的goroutine们有一个优势（它们已经在CPU上运行了）。因此刚唤醒的等待者可能很大几率会竞争失败。在这种情况下，它还会在这个队列中前面继续请求锁，如果请求时间超过1ms都没有就会将这个mutex对象（也就是锁）切换成饥饿模式。 在处于饥饿模式下，在队列最前面的等待者会直接获得这个锁的所有权，而其他刚到的goroutine们也不会尝试去获取锁的所有权，这时候也不会自旋。而是会加入到队列的末尾。 从饥饿模式切回到正常模式，触发条件为：（1）当前获得锁的所有权的等待者发现它是队列种的最后一个。（2）请求锁的时间小于1ms。符合这2种条件，那么当前获得锁的goroutine就会将锁切回正常模式。
常量 const ( mutexLocked = 1 &lt;&lt; iota // mutex is locked mutexWoken //mutex唤醒 mutexStarving //mutex饥饿 mutexWaiterShift = iota //mutex转移 starvationThresholdNs = 1e6 //mutex饥饿阈值 ) mutexLocked = 0001, mutexWoken = 0010, mutexStarving = 0100, mutexWaiterShift = 1000
结构 type Mutex struct { state int32 //当前Mutex的状态，0表示当前为unlocked状态 sema uint32 //信号量 } 上锁 上锁这个过程分为fast path 和 slow path。 首先fast path过程十分简单，就是通过原子操作，对mutex.state检查和交换（如果mutex.state==0）
func (m *Mutex) Lock() { // Fast path if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } ..... } 接着如果fast path路径执行不了，说明该mutex.state!=0，于是进入了slow path过程，跟着代码走。
func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false awoke := false //被唤醒标记，关于这个被唤醒标记，个人理解为，当自选到一定次数时，就会达到饥饿状态，那么我这个goroutine不能一直在这 //进行自旋，因此到了饥饿状态，在下一次请求锁时，是一定可以获得锁的所有权的。因此在后面又要判断awoke将mutexWoken位置位0 iter := 0 //自旋次数 old := m.state for { //如果当前m.state = mutexLocked，通过runtime\_canSpin判断是否可以自旋 //这里面的mutexLockedmutexStarving表示将mutexStarving位置位1，用于判断old是否为饥饿状态 if old&amp;(mutexLockedmutexStarving) == mutexLocked &amp;&amp; runtime\_canSpin(iter) { //将m.state(old)置位为mutexWoken if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old>>mutexWaiterShift != 0 &amp;&amp; atomic.CompareAndSwapInt32(&amp;m.state, old, oldmutexWoken) { awoke = true } runtime_doSpin() //自旋 iter++ old = m.state continue } new := old //如果old的mutexStarving位不为0，那么将new值的mutexLocker位置为1，表示不要取获取一个出于饥饿模式的锁，新来的goroutine要去到队列尾部 if old&amp;mutexStarving == 0 { new = mutexLocked } //如果old的mutexLocked和mutexStarving都不为0，那么说明当前已经上锁而且还处于饥饿状态， //将m.state的饥饿阈值+1 if old&amp;(mutexLockedmutexStarving) != 0 { new += 1 &lt;&lt; mutexWaiterShift } //starving = true 且 old的mutexLocked位不为0，将new值的mutexStarving位置位1 if starving &amp;&amp; old&amp;mutexLocked != 0 { new = mutexStarving } if awoke { //如果awoke为true，说明是被唤醒。 if new&amp;mutexWoken == 0 { throw("sync: inconsistent mutex state") } //重置mutexWoken位为0 new &amp;^= mutexWoken } //进行原子操作，可能要和其他刚到的goroutine们竞争 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { //如果old的mutexLocked位=0，说明抢锁成功 if old&amp;(mutexLockedmutexStarving) == 0 { break //正常模式抢锁成功，直接退出 } //饥饿/正常模式抢锁 queueLifo := waitStartTime != 0 第一次 queue 一定为false if waitStartTime == 0 { waitStartTime = runtime_nanotime() //获取等待时间 } //如果queueLifo为true，将当前goroutine调到queue最前面 runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1) starving = starving runtime_nanotime()-waitStartTime > starvationThresholdNs //当前waitStartTime是否大于1ms old = m.state if old&amp;mutexStarving != 0 { //如果是饥饿模式 if old&amp;(mutexLockedmutexWoken) != 0 old>>mutexWaiterShift == 0 { throw("sync: inconsistent mutex state") } delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) //饥饿模式变回正常模式 if !starving old>>mutexWaiterShift == 1 { delta -= mutexStarving } atomic.AddInt32(&amp;m.state, delta)//更新，完成 break } awoke = true //不是饥饿模式，又抢锁失败，重新发起抢锁竞争 iter = 0 } else { old = m.state //重新发起竞争 } } } 上锁过程尤其要注意slow path过程，首先它先判断当前模式是否为饥饿模式，如果是饥饿模式不让自旋。接着自旋后面是判断计算new的值分3种情况。接着就进行CAS原子操作，也分几种情况，一种是正常/饥饿模式抢锁失败，另一种是正常/饥饿模式抢锁成功，具体流程结合上面代码理解。
解锁 解锁过程相对于上锁过程比较简洁
func (m *Mutex) Unlock() { // 将m.state中的mutexLocked位置位0 new := atomic.AddInt32(&amp;m.state, -mutexLocked) if new != 0 { m.unlockSlow(new) } } func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)&amp;mutexLocked == 0 { throw("sync: unlock of unlocked mutex") } //查看new中的mutexStarving是否位0 if new&amp;mutexStarving == 0 { old := new for { //如果没有goroutine在等待锁，那么直接return if old>>mutexWaiterShift == 0 old&amp;(mutexLockedmutexWokenmutexStarving) != 0 { return } new = (old - 1&lt;&lt;mutexWaiterShift) mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) { runtime_Semrelease(&amp;m.sema, false, 1) return } old = m.state } } else { //释放信号量唤醒在等待的goroutine runtime_Semrelease(&amp;m.sema, true, 1) } }</content></entry><entry><title>go map</title><url>/post/map-in-go/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 哈希表 ​ 散列表，又叫哈希表（Hash Table），是能够通过给定的关键字的值直接访问到具体对应的值的一个数据结构。也就是说，把关键字映射到一个表中的位置来直接访问记录，以加快访问速度。
​ 通常，我们把这个关键字称为 Key，把对应的记录称为 Value，所以也可以说是通过 Key 访问一个映射表来得到 Value 的地址。而这个映射表，也叫作散列函数或者哈希函数，存放记录的数组叫作散列表。
其中有个特殊情况，就是通过不同的 Key，可能访问到同一个地址，这种现象叫作碰撞（Collision）。
哈希函数 直接寻址法：取关键字或关键字的某个线性函数值为散列地址。 数字分析法：通过对数据的分析，发现数据中冲突较少的部分，并构造散列地址。例如同学们的学号，通常同一届学生的学号，其中前面的部分差别不太大，所以用后面的部分来构造散列地址。 平方取中法：当无法确定关键字里哪几位的分布相对比较均匀时，可以先求出关键字的平方值，然后按需要取平方值的中间几位作为散列地址。这是因为：计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。 取随机数法：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。 除留取余法：取关键字被某个不大于散列表的表长 n 的数 m 除后所得的余数 p 为散列地址。这种方式也可以在用过其他方法后再使用。该函数对 m 的选择很重要，一般取素数或者直接用 n。 产生冲突的解决办法 开放地址法（也叫开放寻址法）：实际上就是当需要存储值时，对Key哈希之后，发现这个地址已经有值了，这时该怎么办？不能放在这个地址，不然之前的映射会被覆盖。这时对计算出来的地址进行一个探测再哈希，比如往后移动一个地址，如果没人占用，就用这个地址。如果超过最大长度，则可以对总长度取余。这里移动的地址是产生冲突时的增列序量。 再哈希法：在产生冲突之后，使用关键字的其他部分继续计算地址，如果还是有冲突，则继续使用其他部分再计算地址。这种方式的缺点是时间增加了。 链地址法（拉链法）：链地址法其实就是对Key通过哈希之后落在同一个地址上的值，做一个链表。其实在很多高级语言的实现当中，也是使用这种方式处理冲突的，我们会在后面着重学习这种方式。 建立一个公共溢出区：这种方式是建立一个公共溢出区，当地址存在冲突时，把新的地址放在公共溢出区里。 哈希表特点 访问速度很快
​ 由于散列表有散列函数，可以将指定的 Key 都映射到一个地址上，所以在访问一个 Key（键）对应的 Value（值）时，根本不需要一个一个地进行查找，可以直接跳到那个地址。所以我们在对散列表进行添加、删除、修改、查找等任何操作时，速度都很快。
需要额外的空间
​ 首先，散列表实际上是存不满的，如果一个散列表刚好能够存满，那么肯定是个巧合。而且当散列表中元素的使用率越来越高时，性能会下降，所以一般会选择扩容来解决这个问题。另外，如果有冲突的话，则也是需要额外的空间去存储的，比如链地址法，不但需要额外的空间，甚至需要使用其他数据结构。
​ 这个特点有个很常用的词可以表达，叫作“空间换时间”，在大多数时候，对于算法的实现，为了能够有更好的性能，往往会考虑牺牲些空间，让算法能够更快些。
无序
​ 散列表还有一个非常明显的特点，那就是无序。为了能够更快地访问元素，散列表是根据散列函数直接找到存储地址的，这样我们的访问速度就能够更快，但是对于有序访问却没有办法应对。
可能会产生碰撞
没有完美的散列函数，无论如何总会产生冲突，这时就需要采用冲突解决方案，这也使散列表更加复杂。通常在不同的高级语言的实现中，对于冲突的解决方案不一定一样。
Go的map ​ map 的底层实现是一个哈希表，因此实现 map 的过程实际上就是实现哈希的过程。在这个散列表中，主要出现的结构体有两个，一个叫 hmap(a header for a go map)，一个叫 bmap(a bucket for a Go map，通常叫其bucket)。
hmap 在Go中hmap是map的主要结构体，在这主要关注字段B、buckets、oldbuckets。
type hmap struct { count int // 元素个数，len() flags uint8 B uint8 // 2^B = 桶的个数 noverflow uint16 // 溢出桶的大概数量 hash0 uint32 // hash seed 哈希函数的”种子“， buckets unsafe.Pointer // 指向buckets数组，大小为2^B oldbuckets unsafe.Pointer // 旧buckets，发生在bucket需要增大的情况下 nevacuate uintptr //已经搬迁了的桶数--搬迁进度 extra *mapextra } flag标记位
// flags iterator = 1 // 当前有迭代器在使用buckets oldIterator = 2 // 当前有迭代器在使用oldbuckets hashWriting = 4 //当前有goroutine在对map进行写操作 sameSizeGrow = 8 //等量增长 mapextra type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap } bmap 在hmap中的buckets字段是个指针，其指向了bmap，buckerCnt = 1&laquo;3 = 8。另外bmap结构中存放的是tophash（高位哈希）,它包括此存储桶的每个键的哈希值的最高位。 Tip:在bmap中8个key-value的排序，是8个key紧接着8个value这样的排序，这里就引出了一个内存对齐的概念，如果有兴趣请移步此视频 了解。
const { // Maximum number of key/elem pairs a bucket can hold. bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits } type bmap struct { tophash [bucketCnt]uint8 } 但是在查阅Google之后，Go在编译期间会给这个bmap加料，这便解决了我的一个疑惑：它里面的overflow字段哪来的。 加料之后的bmap：
type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } hiter 哈希迭代器，从字段和Google查询中，不难判断，这个hiter应该时与在hmap扩容时，对map的遍历相关联的。
type hiter struct { key unsafe.Pointer // key字段 elem unsafe.Pointer //value字段 t *maptype h *hmap buckets unsafe.Pointer // 初始化时指向的buckets bptr *bmap // 当前指向的buckets overflow *[]*bmap oldoverflow *[]*bmap startBucket uintptr // 开始遍历时的编号 offset uint8 // 偏移量 wrapped bool // 是否已经从头开始遍历 B uint8 //hmap中的B i uint8 bucket uintptr checkBucket uintptr } 结构图 ![](map in go/20200622172122708.png)
小结 在处理哈希函数时，一定会遇到哈希冲突的问题，一般的解决方法是开放地址法和拉链法。而这里Go中就采用了拉链法，在编译期间给bmap后面增加了一个字段overflow来连接着产生哈希冲突的bmap。另外bmap中key和value的排放位置也十分讲究，这里就引出了这个内存对齐的概念就不详细讲述。
初始化 makemap返回的也是一个*hmap
func makemap(t *maptype, hint int, h *hmap) *hmap { mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow mem > maxAlloc { hint = 0 } // 初始化 Hmap if h == nil { h = new(hmap) } // 获取一个随机哈希种子 h.hash0 = fastrand() // 根据hint，计算B B := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B // 创建buckets if h.B != 0 { var nextOverflow *bmap h.buckets, nextOverflow = **makeBucketArray**(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h } 读写操作 访问 访问操作主要有2种函数分别是：mapaccess1和mapaccess2。俩者的区别是前一个是不到bool类型的返回，后一个是带bool类型的返回。
a := A['A'] //mapaccess1 a, ok := A['A'] //mapaccess2 流程：先根据key计算hash，然后计算获得所对应的桶地址b。接着对桶进行遍历。
func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { ......... alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) // 计算hash m := bucketMask(h.B) //返回 1&lt;&lt;B - 1 //计算桶的地址 b := (*bmap)(add(h.buckets, *(hash &amp; m)*uintptr(t.bucketsize))) top := tophash(hash) //计算高位的哈希值(tophash) bucketloop: //先从正常桶遍历，再从溢出桶遍历 for ; b != nil; b = b.overflow(t) { //bucketCnt = 8 for i := uintptr(0); i &lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) // 判断是否符合 if alg.equal(key, k) { // 计算k对于的value地址 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) return e } } } // 找不到输出nil return unsafe.Pointer(&amp;zeroVal[0]) } 代码整体来说算是直接。
写入 func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { //计算哈希值和桶的位置 alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) h.flags ^= hashWriting again: bucket := hash &amp; bucketMask(h.B) if h.growing() { // 判断当前map是否在扩容 growWork(t, h, bucket) } b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) 遍历比较桶的hash和key，查找对应的key
var inserti *uint8 //索引 var insertk unsafe.Pointer //键 var elem unsafe.Pointer //值 bucketloop: for { for i := uintptr(0); i &lt; bucketCnt; i++ { //判断tophash是否相等 if b.tophash[i] != top { if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil { inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) //判断key是否相等 if !alg.equal(key, k) { continue } // 如果当前key已经存在value，则更新 if t.needkeyupdate() { typedmemmove(t.key, k, key) } elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done //转到done流程 } //切换到溢出桶遍历 ovf := b.overflow(t) if ovf == nil { break } b = ovf } 如果当前桶满了，就会创建一个newoverflow
if inserti == nil { newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) } typedmemmove(t.key, insertk, key) *inserti = top h.count++ 扩容 由于Go中的map是由哈希表构成，当查找key时，它先定位到某个buckets再定位到其中的key。涉及到哈希表，那么必定会牵扯到哈希表的装载因子。在这里Go是这样定义装载因子。
扩容条件 mapassign函数触发扩容条件：
当装载因子超过6.5 当溢出桶过多时 触发扩容相关的函数
//判断装载因子是否超过6.5
func overLoadFactor(count int, B uint8) bool { return count > bucketCnt &amp;&amp; uintptr(count) > loadFactorNum*(bucketShift(B)/loadFactorDen) } //溢出桶过多
func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { if B > 15 { B = 15 } return noverflow >= uint16(1)&lt;&lt;(B&amp;15) } 根据触发条件的不同所对应的扩容条件，扩容方式也会不同。第一种是因为元素多，桶少。第二种是因为元素少，桶多，例如当向map不断的插入数据并把他们删除，这会造成溢出桶不断增加，而到了最后溢出桶太多但是元素却很少的情况。
buckets的搬迁 hashGrow函数主要是对创建一些溢出桶，具体的搬迁操作位于growWork() 和 evacuate()
func hashGrow(t *maptype, h *hmap) { bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags = sameSizeGrow } //将旧的buckets挂到oldbuckets oldbuckets := h.buckets //创建溢出桶 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) //更新标记位 flags := h.flags &amp;^ (iterator oldIterator) if h.flags&amp;iterator != 0 { flags = oldIterator } h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets //更新进度 h.nevacuate = 0 h.noverflow = 0 ......... } evacDst evacDst是表示搬迁后的目的地址的一个结构体
type evacDst struct { b *bmap //当前目的buckets i int // 指向当前key/value的index b k unsafe.Pointer // 指向当前key e unsafe.Pointer // 指向当前value } evacuate func evacuate(t *maptype, h *hmap, oldbucket uintptr) { //找出旧的buckets b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) newbit := h.noldbuckets() //计算当前map的存储桶的数量 if !evacuated(b) { //搬迁地址 **var xy [2]evacDst** x := &amp;xy[0] x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) x.k = add(unsafe.Pointer(x.b), dataOffset) x.e = add(x.k, bucketCnt*uintptr(t.keysize)) //判断当前是否是增加相等的map大小 if !h.sameSizeGrow() { y := &amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.e = add(y.k, bucketCnt*uintptr(t.keysize)) } //遍历所有的buckets for ; b != nil; b = b.overflow(t) { k := add(unsafe.Pointer(b), dataOffset) //原key e := add(k, bucketCnt*uintptr(t.keysize)) //原value for i := 0; i &lt; bucketCnt; i, k, e = i+1, add(k, uintptr(t.keysize)), add(e, uintptr(t.elemsize)) { top := b.tophash[i] //判断当前cell是否为空 if isEmpty(top) { b.tophash[i] = evacuatedEmpty //标志为已经被搬空 continue } if top &lt; minTopHash { throw("bad map state") } k2 := k if t.indirectkey() { k2 = *((*unsafe.Pointer)(k2)) } var useY uint8 if !h.sameSizeGrow() { //通过计算hash来确认目的地址是前半部分x或者是后半部分y hash := t.key.alg.hash(k2, uintptr(h.hash0)) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.alg.equal(k2, k2) { useY = top &amp; 1 top = tophash(hash) } else { if hash&amp;newbit != 0 { useY = 1 } } } //这里不太理解具体意思 ** b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY** dst := &amp;xy[useY] // 目的地址 if dst.i == bucketCnt { dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.e = add(dst.k, bucketCnt\*uintptr(t.keysize)) } dst.b.tophash\[dst.i&amp;(bucketCnt-1)\] = top //迁移key if t.indirectkey() { *(\*unsafe.Pointer)(dst.k) = k2 } else { typedmemmove(t.key, dst.k, k) } //迁移value if t.indirectelem() { *(*unsafe.Pointer)(dst.e) = *(*unsafe.Pointer)(e) } else { typedmemmove(t.elem, dst.e, e) } dst.i++ dst.k = add(dst.k, uintptr(t.keysize)) dst.e = add(dst.e, uintptr(t.elemsize)) } } if oldbucket == h.nevacuate { advanceEvacuationMark(h, t, newbit) //更新搬迁进度 } } 这里有一个点需要注意，在上述evacuated函数加粗的代码，它定义了xy [2]evacDst，里面只含2个evacDst，同时结合上面所定义的cell的状态，可以推测在搬迁过程中，把原来的buckets看出X，而后来等量扩容的部分看成Y，如下图所示。 ![](map in go/20200626082230923.png)
正因为如此，在搬迁的过程中，由于所计算的地址可能会跟原来的不同，map的遍历是按顺序遍历的，扩容之后有的key就可能被迁移到溢出桶那边，因此map的遍历是无序的。
删除 map的删除是由mapdelete函数来完成，也是两层循环，从正常桶到溢出桶查找对应的key（查找过程和访问过程类似），删除时先删除key再删除value。如果当前map正在扩容那么删除操作会在扩容之后执行。
if h.growing() { growWork(t, h, bucket) } ...... //将key清零 if t.indirectkey() { *(*unsafe.Pointer)(k) = nil } else if t.key.ptrdata != 0 { memclrHasPointers(k, t.key.size) } e := add(unsafe.Pointer(b), dataOffset+bucketCnt\*uintptr(t.keysize)+i\*uintptr(t.elemsize)) //对value清零 if t.indirectelem() { *(*unsafe.Pointer)(e) = nil } else if t.elem.ptrdata != 0 { memclrHasPointers(e, t.elem.size) } else { memclrNoHeapPointers(e, t.elem.size) } ........ 为什么map是无序的 ​ 因为 map 在扩容后，可能会将部分 key 移至新内存，那么这一部分实际上就已经是无序的了。而遍历的过程，其实就是按顺序遍历内存地址，同时按顺序遍历内存地址中的 key。但这时已经是无序的了。
总结 ​ Go使用拉链法来解决哈希碰撞来实现哈希表。哈希值在每一个桶中存储对应的哈希值的前8位，每一个桶只能存放8个key/value，一旦超过，新的key/value会被存储到溢出桶中。之后随着key/value的增加，溢出桶的数量和装载因子也会升高，这时候就会触发扩容机制，对key/value迁移到不同的桶中。</content></entry><entry><title>最长上升子序列</title><url>/post/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97/</url><categories><category>LeetCode</category></categories><tags><tag>LeetCode</tag></tags><content type="html"> 理解最长上升子序列的二分法（题目 ）
首先我们要维护数组d[i]来记录当前符合上升子序列的所有下标，len表示上升子序列的长度，那么情况下len=1，dp[i]=nums[0] 对于上升的子序列，分2种情况。第一种情况是最简单的，nums数组里面的数都按升序排列，那么我们只需要将这一部分下标按顺序插进d[i]的尾部，同时len=len+1即可。 第二种情况比较复杂，比如当前我们遍历到第i个元素，当d[len]>num[i]时，说明这时候已经不符合第一种情况，因此我们需要对0~len的区间内找一个数使得d[mid]&lt;nums[i]，然后更新d[mid] = i。请看下图： 最后我们只需判断数组d的长度即可。 代码如下：
func lengthOfLIS(nums []int) int { dp := make([]int,len(nums)) for i := 0 ; i &lt; len(nums) ; i++ { dp[i]++ } for i := 0 ; i &lt; len(nums) ; i++ { for j := 0 ; j &lt; i ; j++ { if nums[j] &lt; nums[i] { dp[i] = max(dp[i],dp[j]+1) } } } return maxs(dp) } func max(a,b int) int { if a > b { return a } return b } func maxs(dp []int) int { max := 0 for i := 0 ; i &lt; len(dp) ; i++ { if dp[i] > max { max = dp[i] } } return max }</content></entry><entry><title>go channel中的自旋锁</title><url>/post/go-channel%E4%B8%AD%E7%9A%84%E8%87%AA%E6%97%8B%E9%94%81/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 首先了解几个参数 const ( active_spin = 4 //主动自旋 active_spin_cnt = 30 passive_spin = 1 //被动自旋 ) 关于active_spin_cnt目前没有找到相关解释，后面找到会补上。
hcan.lock 关键代码如下：
func lock(l *mutex) { ....... // 在单核CPU上没有自旋 // 在多核CPU上存在自旋且不断的进行ACTIVE_SPIN的尝试 spin := 0 if ncpu > 1 { spin = active_spin } Loop: for i := 0; ; i++ { v := atomic.Loaduintptr(&amp;l.key) if v&amp;locked == 0 { // 如果没有锁，尝试去锁上（抢占） if atomic.Casuintptr(&amp;l.key, v, vlocked) { return } i = 0 } if i &lt; spin { procyield(active_spin_cnt) //主动自旋 } else if i &lt; spin+passive_spin { osyield()//被动自旋 } else { //如果自旋之后还没获得，那么就插入到一个等待锁的队列中 for { gp.m.nextwaitm = muintptr(v &amp;^ locked) if atomic.Casuintptr(&amp;l.key, v, uintptr(unsafe.Pointer(gp.m))locked) { break } v = atomic.Loaduintptr(&amp;l.key) if v&amp;locked == 0 { continue Loop } } if v&amp;locked != 0 { // Queued. Wait. semasleep(-1) i = 0 } } } } 总结 关于channel的自旋锁大致就这些，无论它如果操作，最终都是有一段自旋的短暂时间。这个时间取决于是主动还是被动（i &lt; spin 或者 i &lt; spin+passive_spin）。同时需要注意在channel中的lock字段是一个名为key的uinptr类型的值，在上锁或者解锁时，都是通过对其进行原子操作来改变其的值。因此由于这种机制，在这种情况下，goroutine就省去了从Grunning到Gopark再到Gready的过程。</content></entry><entry><title>Go Goroutine的状态</title><url>/post/goroutine%E7%9A%84%E7%8A%B6%E6%80%81/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> Goroutine status，一个Goroutine从生到死，一共有8种状态，2个GC状态。
Gidle 表示该goroutine已经被分配，但是尚未初始化。_Gidle = 0
Grunnable 表示该goroutine正在排队（在等待运行队列），尚未执行用户代码，还没拥有stack。_Grunnable=1
Grunning 表示该goroutine正在运行，已经和M、P配对成功，拥有自己的stack。_Grunning=2
Gsyscall 表示该goroutine正在执行系统调用，不执行用户代码，拥有自己的堆栈，不在run queue。它被分配一个M。_Gsyscall=3
Gwaiting 表示该goroutine被阻塞，不执行用户代码，不在run queue。_Gwaiting=4
Gmoribund_unused 这个本人尚不清楚用处。_Gmoribund_unused=5
Gdead 表示该goroutine还没使用，可以刚刚退出、处于空闲列表或者刚刚被初始化。不执行用户代码，可能有stack。_Gdead=6
Genqueue_unused 表示当前未使用。_Genqueue_unused=7
Gcopystack 表示该goroutine的stack正在复制扩容。_Gcopystack=8
Gscan与Gscanning 表示GC正在扫描的堆栈。与Gscan不同，Gscanning是短暂的阻塞一下g，这对应了Go的GC的发展路线，从起初的STW到后面的引入混合写屏障的三色标记。而Gscan会让当前的goroutine停止。
_Gscan = 0x1000 _Gscanrunnable = _Gscan + _Grunnable // 0x1001 _Gscanrunning = _Gscan + _Grunning // 0x1002 _Gscansyscall = _Gscan + _Gsyscall // 0x1003 _Gscanwaiting = _Gscan + _Gwaiting // 0x1004</content></entry><entry><title>Channel in Go</title><url>/post/go-channel-/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> chan结构 type hchan struct { qcount uint // buffer的总数据量 dataqsiz uint // circular(圆) queue的大小（可以理解为环形链表） buf unsafe.Pointer // 一个指向buffer某个数据的指针 elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // 已发送的索引位置的下标 recvx uint // 已接受的索引位置的下标 recvq waitq // 接收队列 sendq waitq // 发送队列 lock mutex } type waitq struct { first *sudog //sudog表示一个在等待队列的g last *sudog } makechan makechan里面主要是对hchan、buffer等一些状态检测和调用mallocgc函数分配内存等。不过值得主要的地方是最后return的是一个*hchan。因此在平常使用channel是依据指针操作的，而不是值的复制。
chan send 初始 func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, **true**, getcallerpc()) } 这里主要得知block字段都是以true值向下转递。
func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {} c表示某个chan，ep表示发送数据的地址，block表示是否为阻塞，callerpc表示调用地址，通过getcallerpc函数获取，返回的是一个程序的PC地址。同时其还有一个孪生函数getcallersp，返回的是一个函数的SP地址。
判断状态 1、如果c为nil，是非阻塞状态直接返回false。否则通过gopark函数去永远关闭goroutine。这里提醒一句在golang中基本的channel的读写都是阻塞的。如果要非阻塞，可以在select中加入defalut语句。
if c == nil { if !block { return false } gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw("unreachable") } 2、这里有点绕。个人见解为，当c不为nil和非阻塞时，要判断当前channel是否为关闭状态，因此channel可以在一瞬间就关闭了。同时如果buffer中没有数据和没有接收者就返回false。
if !block &amp;&amp; c.closed == 0 &amp;&amp; ((c.dataqsiz == 0 &amp;&amp; c.recvq.first == nil) (c.dataqsiz > 0 &amp;&amp; c.qcount == c.dataqsiz)) { return false } 加锁、添加数据 3、加锁，判断channel是否关闭。这里结合第2步可得知，c.closed = 0 表示该channel没有被关闭，而c.closed != 0 表示channel已经被关闭。
lock(&amp;c.lock) if c.closed != 0 { unlock(&amp;c.lock) panic(plainError("send on closed channel")) } 4、从接收队列中取出一个接收者通过send函数发送数据
if sg := c.recvq.dequeue(); sg != nil { send(c, sg, ep, func() { unlock(&amp;c.lock) }, 3) return true } func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { ......... if sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } 先说下send函数的参数，c表示某个channel，sg表示某个goroutine接收者，ep表示要发送的数据，unlockf表示一个解锁channel的函数，skip字面意思是跳跃，具体不清楚，个人感觉是跟内存偏移量之类的相关。 具体流程大致为，先判断sg.elem是否为nil，如果不为nil的话，就sendDirect（重定向）到sg的指定位置，这里很好理解，因此sg代表的是一个goroutine，其在堆栈里面必然含有一定的数据，因此要重定向到其堆栈的空内存块存储。之后就通过goready函数将其唤醒，加入到go的调度中。
5、向缓冲区添加数据
if c.qcount &lt; c.dataqsiz { qp := chanbuf(c, c.sendx) if raceenabled { raceacquire(qp) racerelease(qp) } typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ unlock(&amp;c.lock) return true } 6、当缓冲区满时 对于非阻塞channel，直接返回false
if !block { unlock(&amp;c.lock) return false } 对于阻塞channel，挂起goroutine并将其抽象成sudog放进发送队列中 gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil **c.sendq.enqueue(mysg)** goparkunlock(&amp;c.lock, waitReasonChanSend, traceEvGoBlockSend, 3) 标记可达 7、通过keepAlive函数把发送数据ep标记为可达，防止gc。同时清理工作现场。
KeepAlive(ep) // someone woke us up. if mysg != gp.waiting { throw("G waiting list is corrupted") } gp.waiting = nil if gp.param == nil { if c.closed == 0 { throw("chansend: spurious wakeup") } panic(plainError("send on closed channel")) } gp.param = nil if mysg.releasetime > 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) return true chan recv 代码与chan send流程大致相同但具体细节方面不同，不再多述。
chan close 主要是对reader和wirter进行关闭
// release all readers //glist就是存放g的列表 for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem)//这里可能就是对channel还存放的数据进行操作 sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil glist.push(gp) } // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = nil glist.push(gp) } race0.go与race.go 在channel的代码上出现了很多调用race0.go里面的方法，个人百度了一下从官网上找到了相关的解释为这是一个go tool chain。详情请移步这里 。一般情况下由于raceenabled为false其不会执行里面的函数。
写屏障 在channel分别出现了2个不同的写屏障方法分别是typeBitsBulkBarrier()和bulkBarrierPreWrite()。 typeBitsBulkBarrier()：出现在recvDirect()和sendDirect()中，是对每个指针复制到另外一个指针执行写屏障(这方面理解不太清楚，正确理解请自行Google)。个人理解是当接受/发送数据时，当前goroutine正在运行，而GC已经把所用到的对象已经关联起来，如果这时不执行写屏障，那么在下一次GC中所接收/发送的数据就会被清除。 bulkBarrierPreWrite()：出现在closechan()中，对在一定范围的内存中的每个指针执行写屏障。个人理解为当在关闭channel时，如果在channel中还有数据，那么就把它复制到另一块内存中。这也是当关闭管道之后，还能读取数据。
总结 总的来说channel内部结构是一个环形链表和一个接收者/发送者队列所组成。当对其发送或者接收数据时，都触发写屏障将数据复制到对应的goroutine中或者将数据写进channel中。而当channel中的buffer满时或者没有数据给goroutine接收时，这时候会将g抽象成sudog类存放在revc/send队列中，这里面会涉及到goroutine的暂停和唤醒操作。</content></entry><entry><title>go sync.Map</title><url>/post/go-sync.map/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> sync.Map的主要结构 Map type Map struct { mu Mutex //锁 read atomic.Value // readOnly 只读数据 dirty map[interface{}]*entry //存储数据，允许读写，但要用锁 //未命中次数，主要记录read读取不到数据，加锁去读取dirty中数据的次数 //到达一定次数时，dirty中的数据就会复制到read中，并且会重新再创建一个dirty misses int } readOnly // readOnly是用于存储，通过原子操作在map中的read字段 type readOnly struct { m map[interface{}]*entry // 如果为true则表示some key在dirty存在，而readOnly这没有 amended bool } entry type entry struct { // 如果p==nil，表示已经被删除，并且m.dirty=nil // 如果p==expunged，表示已经被删除，但是该键已经不在dirty map中 // 其他情况，表示在m.read和m.dirty都存有数据 p unsafe.Pointer // \*interface{} } Map结构主要函数 Load() 该函数首先会从read map 中取出数据，如果read中没有所需要的数据就去dirty map 中读取数据
func (m *Map) Load(key interface{}) (value interface{}, ok bool) { read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended { m.mu.Lock() //防止第一次从read取数据时碰上数据正从dirty复制到read中 //因此这里还需再读取一次 read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended { e, ok = m.dirty[key] //未命中次数+1 m.missLocked() } m.mu.Unlock() } if !ok { return nil, false } return e.load()//调取entry类所定义的load方法 } store() 该函数主要用于存储数据。具体流程是： 1、先从read map中看看能找到key/value，如果找到通过e.tryStore函数更新数据 2、当从read map中没有找到数据，上锁，对dirty map进行操作。首先先check一下read map状态，如果在read map中找到，但仍是expunged状态，就通过e.unexpungeLocked()将expunged标记为nil，最后再把数据存储到entry中。 3、如果在read map 和dirty map 都没有找到数据，那么就判断read.amended是否为false，如果为false那么就说明在dirty map中没有该数据，因此就通过m.dirtyLocked去新建dirty map。
func (m *Map) Store(key, value interface{}) { read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) { return } m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok { if e.unexpungeLocked() { m.dirty[key] = e } e.storeLocked(&amp;value) } else if e, ok := m.dirty[key]; ok { e.storeLocked(&amp;value) } else { if !read.amended { m.dirtyLocked() m.read.Store(readOnly{m: read.m, amended: true}) } m.dirty[key] = newEntry(value) } m.mu.Unlock() } 另外在源码中，有一处地方需要我们关注一下，请看上面加粗代码，通过观察我们可以发现从read.m[key]取出来的e，居然赋值给了m.dirty[key]，由此可以推测在底层中read map 和dirty map使用的是不同的指针，但是它们指向的都是同一个值。
Delete() 这个函数的代码流程就十分清晰了，首先它是先check一下read map中是否含有该数据如果没有而且read.amended为true，那么就上锁去dirty map中删除该数据，如果在read map中找到就直接删除即可。
func (m *Map) Delete(key interface{}) { read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended { m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended { delete(m.dirty, key) } m.mu.Unlock() } if ok { e.delete() } } Range() 这个主要用于遍历，通过检查read.amended是否为true，然后去dirty map或者read map遍历。
总结 sync.map原理也不难，主要思想是空间换时间的思路，使用两个map来进行操作实现对锁的操作，一个read map主要用于并发中的读取和已经存在的写(请看store函数中加粗的部分)，一个dirty map主要用于读写。但有几个关键的状态还需注意一下分别是read.amended和expunded，其中read.amended状态量决定了是对哪个map。</content></entry><entry><title>Context in Go</title><url>/post/go-context/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> ![](/images/Context in Go/20200609041455723.png)
Context Interface type Context interface { //超过时间，该context便死亡 Deadline() (deadline time.Time, ok bool) //当context结束时，返回一个channel Done() &lt;-chan struct{} //context错误 Err() error //返回上下文所关联的key/value值 Value(key interface{}) interface{} } emptyCtx 这个emptyCtx是一个int类型，它实现了Context Interface里面的所有方法，但是都返回空值，只有一个String()方法来判断其类型来进行相应输出。
type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() &lt;-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } func (e *emptyCtx) String() string { switch e { case background: return "context.Background" case todo: return "context.TODO" } return "unknown empty Context" } 同时background和todo也是基于这个emptyCtx类new出来的，了解过background的都知道，在使用context时都是先context.Background()一个context来进行后继操作。
var ( background = new(emptyCtx) todo = new(emptyCtx) ) 而background和todo的区别在于，todo是用于你不清楚要使用哪个上下文的情况下。而background通常是由main函数使用，说白了background就是所有context的root，不能被cancel。
CancelFunc 这个类定义了一个可以返回的cancel函数，它可以cancel当前的context和它的子context，并且把自己从父类的context删除。它提供三个可以cancel的方法。
WithCancel 这个方法主要是当上下文操作完成时立即调用结束该context链。当调用父类的cancel时，父类和其子节点就会一起Done()。
func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { //创建一个新context节点 c := newCancelCtx(parent) //创建context的链路，其主要是创建一个map来存储context的子节点 propagateCancel(parent, &amp;c) return &amp;c, func() { c.cancel(true, Canceled) } } 但是不同的context子类是分布在不同的goroutine，那么它是如何实现当父类Done()，子节点接着Done()。通常首先想到的方法时子类监听一个channel传过来的关闭信号，然后子类收到这个关闭信号之后就结束自身。但具体如何实现的，请看下图。 ![](/images/Context in Go/20200609041008165.png)
WithDeadline 这个方法主要是当超过一定时间后，context就自行结束自己和其子节点。同时它也返回一个可以主动cancel的方法
func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { ........ c := &amp;timerCtx{ cancelCtx: newCancelCtx(parent), deadline: d, } //创建context的链路，其主要是创建一个map来存储context的子节点 propagateCancel(parent, c) dur := time.Until(d) if dur &lt;= 0 { c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() { c.cancel(false, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { c.timer = time.AfterFunc(dur, func() { c.cancel(true, DeadlineExceeded) }) } return c, func() { c.cancel(true, Canceled) } } WithTimeout 这个方法也是一个超时控制，内部其实是引用了WithDeadline方法
func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } canceler canceler是一个具有类似于取消器的接口（A canceler is a context type that can be canceled directly），它定义了二个方法。
type canceler interface { cancel(removeFromParent bool, err error) Done() &lt;-chan struct{} } cancelCtx 首先cancelCtx定义了一个结构体，其继承context接口，同时重点注意加粗部分。
type cancelCtx struct { Context mu sync.Mutex // protects following fields done chan struct{} // created lazily, closed by first cancel call children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call } 重点关注cancel()函数和加粗部分，具体代码解释就不说了，源码写的很清楚。
func (c *cancelCtx) cancel(removeFromParent bool, err error) { if err == nil { panic("context: internal error: missing cancel error") } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // already canceled } c.err = err if c.done == nil { c.done = closedchan } else { close(c.done) } for child := range c.children { // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err) } c.children = nil c.mu.Unlock() if removeFromParent { removeChild(c.Context, c) } } timerCtx 首先timerCtx定义了一个结构体，其继承cancelCtx结构
type timerCtx struct { cancelCtx timer *time.Timer // 计时器，A Timer must be created with NewTimer or AfterFunc. deadline time.Time //结束时间 } 同时其定义了三个方法分别为Deadline()、String()、cancel()。重点关注cancel()函数
func (c *timerCtx) cancel(removeFromParent bool, err error) { c.cancelCtx.cancel(false, err) //cancel当前context和其子节点 if removeFromParent { // Remove this timerCtx from its parent cancelCtx's children. removeChild(c.cancelCtx.Context, c) } c.mu.Lock() //将计时器重置 if c.timer != nil { c.timer.Stop() c.timer = nil } c.mu.Unlock() } valueCtx 首先valueCtx定义了一个结构体，其继承context接口
type valueCtx struct { Context key, val interface{} } 其定义了二个方法分别为String()、Value()。它们主要的作用是输出打印和输出key所对应的value。重点关注WithValue()函数。
func WithValue(parent Context, key, val interface{}) Context { if key == nil { panic("nil key") } if !reflectlite.TypeOf(key).Comparable() { panic("key is not comparable") } return &amp;valueCtx{parent, key, val} } 总结 context包在平常运用的很多，尤其是在http方面居多，因此熟悉和了解其内部实现是很重要的事情。最后文章虽然写完了，但是总觉得还是有点懵，主要是关于父类和其子节点的Done()操作还有些细枝末节仍未摸清。</content></entry><entry><title>链表反转</title><url>/post/%E9%93%BE%E8%A1%A8%E7%9A%84%E5%8F%8D%E8%BD%AC/</url><categories><category>LeetCode</category></categories><tags><tag>LeetCode</tag></tags><content type="html"> 链表的反转的类型，就我目前做题的经历和情况来看，目前我一共遇到2种类型的链表反转，第一类是最简单的把整条链表反转，第二类就是把链表中的某一段进行反转，但每次遇到这第二类题总感觉堵在那，因此很有必要自己画个流程图手推一波。
第一类：将整条链表反转 这一类很简单，便不再讲述，请看图
//伪代码 循环体 { mid := cur.Next cur.Next = prev prev = cur cur = mid } 第二类：将链表中的某一部分反转
这一类你要先找到要反转的区间，然后用head和tail标记，接着就按照第一类反转链表的方法反转这一段区间的链表，最后你会得到三段链表：原链表的未反转首部，已经被反转的链表，原链表的未反转尾部，你只要把这三段连接起来即可。</content></entry><entry><title>Go中的defer</title><url>/post/go%E4%B8%AD%E7%9A%84defer/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 例子1：
func increaseA() int { var i int defer func() { i++ }() return i //注意这是个匿名返回值 } func increaseB() (r int) { defer func() { r++ }() return r } 例子2：
func f1() (r int) { defer func() { r++ }() return 0 } func f2() (r int) { t := 5 defer func() { t = t + 5 }() return t } func f3() (r int) { defer func(r int) { r = r + 5 }(r) return 1 } 例子3：
func main() { defer func() { fmt.Print(recover()) }() defer func() { defer func() { fmt.Print(recover()) }() panic(1) }() defer recover() panic(2) } 跳坑关键：return ×× 这句话在编译过后会变为一下三条指令 1、返回值 = ×× 2、调用defer function 3、空的return
因此例子2的结果显而易见，分别为：1、5、1， 而例子3的结果为12，例子1的结果为0、1</content></entry><entry><title>Go goroutine Stack</title><url>/post/goroutine-stack/</url><categories><category>Go</category></categories><tags><tag>Go</tag></tags><content type="html"> 前言 goroutine需要自己的栈才能够运行。假如每个goroutine分配的栈固定，那么这个栈太小则会导致溢出，太大又会浪费空间，因此如果这样的gorouting有成千上万个，那么系统是无法容纳众多的goroutine的。 因此在当前Go版本中， goroutine就已经在使用连续栈的形式来为其分配内存空间，该形式可以让goroutine尽可能的把内存空间最大化利用同时，也能正常运行。
Goroutine Stack 的结构体定义 首先我们来看一下在Go的源码中对 Goroutine Stack 的结构体定义：
type stack struct { lo uintptr 栈空间的低地址 hi uintptr 栈空间的高地址 } // uintptr is an integer type that is large enough to hold the bit pattern of // any pointer. 每当我们起一个 goroutine 时，Go会给这个 goroutine 分配一个内存空间，其大小基于所使用的平台，在源码的定义如下：
// Number of orders that get caching. Order 0 is FixedStack // and each successive order is twice as large. // We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks // will be allocated directly. // Since FixedStack is different on different systems, we // must vary NumStackOrders to keep the same maximum cached size. // OS FixedStack NumStackOrders // -----------------+------------+--------------- // linux/darwin/bsd 2KB 4 // windows/32 4KB 3 // windows/64 8KB 2 // plan9 4KB 3 栈的初始化 首先是对栈的初始化：
func stackinit() { if _StackCacheSize&amp;_PageMask != 0 { throw("cache size must be a multiple of page size") } //从栈池中取出一个栈来init()相当对mSpanList初始化 for i := range stackpool { stackpool[i].init() } for i := range stackLarge.free { stackLarge.free[i].init() } } 其中对于stackpool栈池，它的定义是一个 [_NumStackOrders]mSpanList ,而 mSpanList 涉及到Go的内存管理，在这里先不展开。而stackLarge（// Global pool of large stack spans.）个人理解是是最大栈空间全局池。spans可以理解为一块内存。
栈的增大 接下来我们看看当 Goroutine 在运行时栈空间需要增加是如何操作的。
func newstack() { //获取当前的goroutine thisg := getg() // TODO: double check all gp. shouldn't be getg(). ...........中间代码省略................ // Allocate a bigger segment and move the stack. oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 if newsize > maxstacksize { print("runtime: goroutine stack exceeds ", maxstacksize, "-byte limit\\n") throw("stack overflow") } //修改该协程的状态 由_Grunning 变成_Gcopystack casgstatus(gp, _Grunning, _Gcopystack) // The concurrent GC will not scan the stack while we are doing the copy since 将当前栈的数据复制到新的栈中 // the gp is in a Gcopystack status. copystack(gp, newsize, true) if stackDebug >= 1 { print("stack grow done\\n") } //修改该协程的状态 由_Gcopystack变成 _Grunning casgstatus(gp, _Gcopystack, _Grunning) gogo(&amp;gp.sched) } 从源码来看，一个 Goroutine 栈空间增大的过程为，先获取当前的goroutine，更新syscallsp和syscallpc，应该是防止在增大栈空间时系统又回调了这个 Goroutine ，然后中间的代码（看的我一脸懵）和注解，个人的见解为，在 Goroutine 增大栈的过程中，会令 Goroutine 的状态发生改变，这时候GC的什么写屏障和 Goroutine 抢占的问题就会出现，因此这不详细说明。接着就计算新栈的空间，其等于旧栈空间的一倍（newsize := oldsize * 2）然后把该协程g的状态由 _Grunning 变 _Gcopystack，复制数据到新栈，把 该协程g 恢复原状，进而完成栈增大操作。
栈的复制 接着上述我们看看copystack()是如何操作的。
func copystack(gp *g, newsize uintptr, sync bool) { ....... old := gp.stack ....... used := old.hi - gp.sched.sp // 分配新堆栈 new := stackalloc(uint32(newsize)) .......... // Compute adjustment. 这一步应该是调整旧栈指针 var adjinfo adjustinfo adjinfo.old = old adjinfo.delta = new.hi - old.hi .......... // 将旧栈复制到新位置 memmove(unsafe.Pointer(new.hi-ncopy), unsafe.Pointer(old.hi-ncopy), ncopy) // Adjust remaining structures that have pointers into stacks. // We have to do most of these before we traceback the new // stack because gentraceback uses them. adjustctxt(gp, &amp;adjinfo) adjustdefers(gp, &amp;adjinfo) adjustpanics(gp, &amp;adjinfo) if adjinfo.sghi != 0 { adjinfo.sghi += adjinfo.delta } // Swap out old stack for new one gp.stack = new gp.stackguard0 = new.lo + _StackGuard // NOTE: might clobber a preempt request gp.sched.sp = new.hi - used gp.stktopsp += adjinfo.delta // 调整新栈中的指针 gentraceback(^uintptr(0), ^uintptr(0), 0, gp, 0, nil, 0x7fffffff, adjustframe, noescape(unsafe.Pointer(&amp;adjinfo)), 0) // 释放旧栈 if stackPoisonCopy != 0 { fillstack(old, 0xfc) } stackfree(old) } 从源码来看，一个 Goroutine 栈的复制的过程大致为，先计算调整旧栈的指针，然后再把旧栈复制到新栈中，最后调整新栈的指针(gentraceback)和释放旧栈。注意复制栈的过程需要的条件为：当前栈的状态一定是 _Gcopystack 。
栈的缩小 既然有 Goroutine的增栈，那么必然有 Goroutine 的缩栈，下面我们来看一下缩栈的操作。
// Maybe shrink the stack being used by gp. // Called at garbage collection time. // gp must be stopped, but the world need not be. //由上面的注释可以知道,缩栈是在垃圾回收时间做的 func shrinkstack(gp *g) { //获取当前g的状态 gstatus := readgstatus(gp) if gp.stack.lo == 0 { throw("missing stack in shrinkstack") } if gstatus&amp;_Gscan == 0 { throw("bad status in shrinkstack") } ............ //计算新栈的newsize,从这里可以看出Go缩栈是缩小到原来的一半 oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize / 2 // Don't shrink the allocation below the minimum-sized stack // allocation. if newsize &lt; _FixedStack { return } // Compute how much of the stack is currently in use and only // shrink the stack if gp is using less than a quarter of its // current stack. The currently used stack includes everything // down to the SP plus the stack guard space that ensures // there's room for nosplit functions. //当已使用的栈占不到总栈的1/4 进行缩容 avail := gp.stack.hi - gp.stack.lo if used := gp.stack.hi - gp.sched.sp + _StackLimit; used >= avail/4 { return } // We can't copy the stack if we're in a syscall. // The syscall might have pointers into the stack. //判断当前栈是否在被系统调用,如果正在被调用那么syscall可以有指针在当前栈里面 if gp.syscallsp != 0 { return } if sys.GoosWindows != 0 &amp;&amp; gp.m != nil &amp;&amp; gp.m.libcallsp != 0 { return } if stackDebug > 0 { print("shrinking stack ", oldsize, "->", newsize, "\\n") } //最后把数据复制到新栈中 copystack(gp, newsize, false) } 从 Goroutine 缩栈的源码来看，缩栈的大致过程为，先获取当前goroutine的状态进来判断，然后计算newsize，另外如果gp使用少于其四分之一的空间，则缩小堆栈 。检查当前g是否正在被系统调用， 如果当前g正在被系统调用就直接return回去然后等待系统调用结束，最后把数据复制到新栈中。
Goroutine Stack Frame 建议先了解 Stack Frame
FP，SP，PC ，LR PC： 程序计数器 SP： 保存栈顶的地址 FP ： 保存栈底的地址 LR ： 连接寄存器的程序计数器
Stack frame layout 从Go的源码可以得知Stack frame 的设计如下：
// (x86) // +------------------+ // args from caller //调用者的args // +------------------+ &lt;- frame->argp // return address // +------------------+ // caller's BP (\*) (\*) if framepointer\_enabled &amp;&amp; varp &lt; sp // +------------------+ &lt;- frame->varp // locals // +------------------+ // args to callee //被调用者的args // +------------------+ &lt;- frame->sp // // (arm) // +------------------+ // args from caller // +------------------+ &lt;- frame->argp // caller's retaddr // +------------------+ &lt;- frame->varp // locals // +------------------+ // args to callee // +------------------+ // return address // +------------------+ &lt;- frame->sp 同时还要关注一下这个 结构体 stack traces（栈的跟踪）
type stkframe struct { fn funcInfo // function being run pc uintptr // program counter within fn continpc uintptr // program counter where execution can continue, or 0 if not lr uintptr // program counter at caller aka link register sp uintptr // stack pointer at pc fp uintptr // stack pointer at caller aka frame pointer varp uintptr // top of local variables argp uintptr // pointer to function arguments arglen uintptr // number of bytes at argp argmap \*bitvector // force use of this argmap } 关于调整新旧栈的指针 从函数gentraceback()开始，由于 gentraceback() 代码量过于庞大，这里我们只关注 PC，SP，FP，LR 和与stkframe 相关 的代码部分。
...... var frame stkframe frame.pc = pc0 frame.sp = sp0 ...... //findfunc->findmoduledatap ,他返回时一个moduledata对象，其记录有关可执行文件布局的信息 f := findfunc(frame.pc) ...... frame.fn = f for n &lt; max { ...... f = frame.fn if f.pcsp == 0 { // No frame information, must be external function, like race support. // See golang.org/issue/13568. break } ...... if frame.fp == 0 { sp := frame.sp ...... //计算FP frame.fp = sp + uintptr(funcspdelta(f, frame.pc, &amp;cache)) if !usesLR { // On x86, call instruction pushes return PC before entering new function. frame.fp += sys.RegSize } } var flr funcInfo if topofstack(f, gp.m != nil &amp;&amp; gp == gp.m.g0) { ...... } else if usesLR &amp;&amp; f.funcID == funcID\_jmpdefer { ...... } else { var lrPtr uintptr if usesLR { ...... } else { if frame.lr == 0 { lrPtr = frame.fp - sys.RegSize frame.lr = uintptr(\*(\*sys.Uintreg)(unsafe.Pointer(lrPtr))) } } flr = findfunc(frame.lr) ...... } //top of local variables frame.varp = frame.fp if !usesLR { // On x86, call instruction pushes return PC before entering new function. frame.varp -= sys.RegSize } ...... if framepointer\_enabled &amp;&amp; GOARCH == "amd64" &amp;&amp; frame.varp > frame.sp { frame.varp -= sys.RegSize } ...... if callback != nil printing { frame.argp = frame.fp + sys.MinFrameSize ...... } ...... // Unwind to next frame. 退至下一帧 frame.fn = flr frame.pc = frame.lr frame.lr = 0 frame.sp = frame.fp frame.fp = 0 frame.argmap = nil ...... } ...... 到这 goroutine Stack 也说的差不多了
总结 从 Goroutine 栈的实现方式上可以知道，不管栈扩大还是缩小，都会重新申请一块新栈（新的一块内存），然后把旧栈的数据复制到新栈，相当于完整替换了Goroutine使用的物理内存，再借助指针重新指向新栈，便完成了栈大小改变过程。</content></entry><entry><title>关于我</title><url>/about.html</url><categories/><tags/><content type="html"> Hugo是用Go编写的一个开放源代码静态站点生成器，可在Apache许可证2.0 下使用。 Hugo支持TOML, YAML和JSON数据文件类型，Markdown和HTML内容文件，并使用短代码添加丰富的内容。其他值得注意的功能包括分类法、多语言模式、图像处理、自定义输出格式、HTML/CSS/JS缩小和对Sass SCSS工作流的支持。
Hugo使用了多种开源项目，包括:
https://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo是博客、企业网站、创意作品集、在线杂志、单页应用程序甚至是数千页的网站的理想选择。
Hugo适合那些想要手工编写自己的网站代码，而不用担心设置复杂的运行时、依赖关系和数据库的人。
使用Hugo建立的网站非常快速、安全，可以部署在任何地方，包括AWS、GitHub Pages、Heroku、Netlify和任何其他托管提供商。
更多信息请访问GitHub .</content></entry></search>